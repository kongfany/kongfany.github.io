<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Knowledge Distillation on 大胖狗来了</title>
    <link>https://kongfany.github.io/tags/knowledge-distillation/</link>
    <description>Recent content in Knowledge Distillation on 大胖狗来了</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 21 Mar 2022 21:48:26 +0800</lastBuildDate><atom:link href="https://kongfany.github.io/tags/knowledge-distillation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>联邦知识蒸馏</title>
      <link>https://kongfany.github.io/post/fl_fd/</link>
      <pubDate>Mon, 21 Mar 2022 21:48:26 +0800</pubDate>
      
      <guid>https://kongfany.github.io/post/fl_fd/</guid>
      <description>联邦知识蒸馏概述与思考 续 随着深度学习与大数据的进一步发展，效果好的模型往往有着较大的规模和复杂的结构，往往计算效率与资源使用方面开销很大，无</description>
    </item>
    
  </channel>
</rss>
