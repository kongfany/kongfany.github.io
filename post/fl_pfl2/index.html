<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>个性化联邦 - 乐观积极的...</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="kong" /><meta name="description" content="视频 PFLlib_code fedbn ditto fedala fedb PFLLIB 数据集处理 dataset等三个文件夹mark directory as sources root&amp;ndash;import下列的文件时才可以识别到 system ​ flscor" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.87.0 with theme even" />


<link rel="canonical" href="https://kongfany.github.io/post/fl_pfl2/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="个性化联邦" />
<meta property="og:description" content="视频 PFLlib_code fedbn ditto fedala fedb PFLLIB 数据集处理 dataset等三个文件夹mark directory as sources root&ndash;import下列的文件时才可以识别到 system ​ flscor" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kongfany.github.io/post/fl_pfl2/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-11-28T15:25:56+08:00" />
<meta property="article:modified_time" content="2023-11-28T15:25:56+08:00" />

<meta itemprop="name" content="个性化联邦">
<meta itemprop="description" content="视频 PFLlib_code fedbn ditto fedala fedb PFLLIB 数据集处理 dataset等三个文件夹mark directory as sources root&ndash;import下列的文件时才可以识别到 system ​ flscor"><meta itemprop="datePublished" content="2023-11-28T15:25:56+08:00" />
<meta itemprop="dateModified" content="2023-11-28T15:25:56+08:00" />
<meta itemprop="wordCount" content="41986">
<meta itemprop="keywords" content="Federated Learning," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="个性化联邦"/>
<meta name="twitter:description" content="视频 PFLlib_code fedbn ditto fedala fedb PFLLIB 数据集处理 dataset等三个文件夹mark directory as sources root&ndash;import下列的文件时才可以识别到 system ​ flscor"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">乐观积极的...</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">乐观积极的...</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">个性化联邦</h1>

      <div class="post-meta">
        <span class="post-time"> 2023-11-28 </span>
        
          <span class="more-meta"> 41986 words </span>
          <span class="more-meta"> 84 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#pfllib">PFLLIB</a>
          <ul>
            <li><a href="#数据集处理">数据集处理</a></li>
            <li><a href="#client处理">client处理</a></li>
            <li><a href="#server处理">server处理</a></li>
          </ul>
        </li>
        <li><a href="#fedala--aaai2023">FedALA -AAAI2023</a>
          <ul>
            <li><a href="#自适应本地聚合ala过程">自适应本地聚合（ALA）过程</a></li>
            <li><a href="#实验">实验：</a></li>
            <li><a href="#code">code</a></li>
          </ul>
        </li>
        <li><a href="#fedcp--kdd2023">FedCP -KDD2023</a></li>
        <li><a href="#fedsoup">fedsoup</a>
          <ul>
            <li><a href="#paper">paper</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <blockquote>
<p><a href="https://www.bilibili.com/video/BV1bu411K7bV/?spm_id_from=333.999.0.0&amp;vd_source=ad42090d7d6fcdfc144126ae0e2884ac">视频</a></p>
<p><a href="https://github.com/TsingZ0/PFL-Non-IID">PFLlib_code</a></p>
<p>fedbn</p>
<p>ditto</p>
<p>fedala</p>
<p>fedb</p>
</blockquote>
<h2 id="pfllib">PFLLIB</h2>
<h3 id="数据集处理">数据集处理</h3>
<p>dataset等三个文件夹mark directory as sources root&ndash;import下列的文件时才可以识别到</p>
<p>system</p>
<p>​	flscore：</p>
<p>​		client</p>
<p>​		server</p>
<p>​		optimizers</p>
<p>​	until：</p>
<p>generate_mnist.py：生成数据集mnist在dirpath下</p>
<p>rawdata：原始数据集60000trian10000test</p>
<p>test、train：20个npz文件</p>
<p>jason串格式化<code>ctrl+alt+l</code>(所有代码格式化都可用，和网易云冲突但是)</p>
<hr>
<blockquote>
<p>For the <em><strong>label skew</strong></em> scenario ：标签偏移</p>
<p>In <strong>non-IID</strong> scenario, 2 situations exist. The first one is the <strong>pathological non-IID</strong> scenario, the second one is <strong>practical non-IID</strong> scenario.</p>
<p>第一种是病理性非 IID 场景，第二种是实际的非 IID 场景。例如，在病理性非 IID 场景中，每个客户机上的数据只包含特定数量的标签(可能只有2个标签) ，尽管所有客户机上的数据包含10个标签，比如 MNIST 数据集。在实际的非内部 ID 场景中，使用了狄利克雷分布。</p>
</blockquote>
<blockquote>
<p>划分数据的不同场景</p>
<p>1、python generate_mnist.py iid balance - # for iid and balanced scenario</p>
<p>IID，平衡&ndash;每个client包含所有类，且每个client分到的当前类的样本数相同</p>
<p>2、python generate_mnist.py iid - - # for iid and unbalanced scenario</p>
<p>IID,不平衡场景&ndash;每个client包含所有类，但数据量当前类对应的</p>
<p>3、python generate_mnist.py noniid - pat # for pathological noniid and unbalanced scenario</p>
<p>4、python generate_mnist.py noniid - dir # for practical noniid and unbalanced scenario</p>
</blockquote>
<p>“balance” 通常表示在训练过程中平衡各个参与方（如客户端）的贡献或资源分配。这可以包括确保每个参与方有足够的训练样本、计算资源或者对模型更新的贡献，以实现公平性和效率性。</p>
<p>联邦学习的不平衡场景可能包括以下情况： 1. 数据分布不均：参与方（如客户端）拥有的数据量差异很大，有些参与方拥有的样本数量非常少，而有些参与方拥有的样本数量非常多。 2. 计算资源不均：参与方的计算能力不同，有些参与方的设备性能较低，而有些参与方的设备性能较高。 3. 数据标签不均：在分类任务中，不同类别的样本数量差异很大，导致一些类别的数据在训练过程中被较少地考虑。 这些不平衡的场景可能会对联邦学习的模型训练和整体性能产生影响，需要采取相应的策略来解决。</p>
<p>1、iid and balanced scenario：将每个类(10)划分到所有客户端，将当前类对应的数据平均分给每个client(20)</p>
<p>2、iid and unbalanced scenario：</p>
<hr>
<p>下载数据，将数据的训练集测试集揉在一起，然后划分到每个client，然后分为训练集和测试集。然后将划分的参数和每个client的数据情况保存到json串中。</p>
<p>pfl每个client都有测试集，揉和后划分之后再区分&ndash;保证客户端的训练集和测试集的分布一致，有利于pfl的准确率</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># idxs:7000个数据对应的下标数组</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">idxs</span>
<span class="n">array</span><span class="p">([</span>    <span class="mi">0</span><span class="p">,</span>     <span class="mi">1</span><span class="p">,</span>     <span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69997</span><span class="p">,</span> <span class="mi">69998</span><span class="p">,</span> <span class="mi">69999</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>
<span class="mi">70000</span>

<span class="c1"># idx_for_each_class 对应了数据的每个类别的下标</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">idx_for_each_class</span>
<span class="p">[</span><span class="n">array</span><span class="p">([</span>    <span class="mi">1</span><span class="p">,</span>    <span class="mi">21</span><span class="p">,</span>    <span class="mi">34</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69964</span><span class="p">,</span> <span class="mi">69983</span><span class="p">,</span> <span class="mi">69993</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>    <span class="mi">3</span><span class="p">,</span>     <span class="mi">6</span><span class="p">,</span>     <span class="mi">8</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69978</span><span class="p">,</span> <span class="mi">69984</span><span class="p">,</span> <span class="mi">69994</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>    <span class="mi">5</span><span class="p">,</span>    <span class="mi">16</span><span class="p">,</span>    <span class="mi">25</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69980</span><span class="p">,</span> <span class="mi">69985</span><span class="p">,</span> <span class="mi">69995</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>    <span class="mi">7</span><span class="p">,</span>
  <span class="mi">10</span><span class="p">,</span>    <span class="mi">12</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69975</span><span class="p">,</span> <span class="mi">69986</span><span class="p">,</span> <span class="mi">69996</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>    <span class="mi">2</span><span class="p">,</span>     <span class="mi">9</span><span class="p">,</span>    <span class="mi">20</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69977</span><span class="p">,</span> <span class="mi">69987</span><span class="p">,</span> <span class="mi">69997</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>    <span class="mi">0</span><span class="p">,</span>    <span class="mi">11</span><span class="p">,</span>    <span class="mi">35</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69982</span><span class="p">,</span> <span class="mi">69988</span><span class="p">,</span> <span class="mi">69998</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>   <span class="mi">13</span><span class="p">,</span>    <span class="mi">18</span><span class="p">,</span>    <span class="mi">32</span><span class="p">,</span> <span class="o">...</span>
<span class="p">,</span> <span class="mi">69981</span><span class="p">,</span> <span class="mi">69989</span><span class="p">,</span> <span class="mi">69999</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>   <span class="mi">15</span><span class="p">,</span>    <span class="mi">29</span><span class="p">,</span>    <span class="mi">38</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69968</span><span class="p">,</span> <span class="mi">69979</span><span class="p">,</span> <span class="mi">69990</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>   <span class="mi">17</span><span class="p">,</span>    <span class="mi">31</span><span class="p">,</span>    <span class="mi">41</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69959</span><span class="p">,</span> <span class="mi">69967</span><span class="p">,</span> <span class="mi">69991</span><span class="p">]),</span> <span class="n">array</span><span class="p">([</span>    <span class="mi">4</span><span class="p">,</span>    <span class="mi">19</span><span class="p">,</span>    <span class="mi">22</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69945</span><span class="p">,</span> <span class="mi">69973</span><span class="p">,</span>
<span class="mi">69992</span><span class="p">])]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx_for_each_class</span><span class="p">)</span>
<span class="mi">10</span>
<span class="c1"># 每个client的种类数量--平衡都是10，不平衡默认2</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">class_num_per_client</span>
<span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="c1"># 对于每一个class类别i，选择应该划分该类别的client</span>
<span class="c1"># 选取client的比例＝客户端种类的采样比例，不平衡len=2，选两个客户端</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">selected_clients</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">]</span>
<span class="c1"># 类别i的图片数量</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_all_samples</span>
<span class="mi">6903</span>
<span class="c1"># 类别i被划分的客户端数目</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_selected_clients</span>
<span class="mi">20</span>
<span class="c1"># 每个类别被分到的类别i对应的数量</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_per</span>
<span class="mf">345.15</span>
<span class="c1"># 每个客户端对应类别i的图片数量，前n-1个取整num_per,剩余的图片全放在第n个client</span>
<span class="c1"># 不平衡的话，以num_per为中间值高斯分布</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">345</span><span class="p">,</span> <span class="mi">348</span><span class="p">]</span>

<span class="c1"># 对于当前class，每个client分到的数据下标</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">dataidx_map</span>
<span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span>   <span class="mi">1</span><span class="p">,</span>   <span class="mi">21</span><span class="p">,</span>   <span class="mi">34</span><span class="p">,</span>   <span class="mi">37</span><span class="p">,</span>   <span class="mi">51</span><span class="p">,</span>   <span class="mi">56</span><span class="p">,</span>   <span class="mi">63</span><span class="p">,</span>   <span class="mi">68</span><span class="p">,</span>   <span class="mi">69</span><span class="p">,</span>   <span class="mi">75</span><span class="p">,</span>   <span class="mi">81</span><span class="p">,</span>
         <span class="mi">88</span><span class="p">,</span>   <span class="mi">95</span><span class="p">,</span>  <span class="mi">108</span><span class="p">,</span>  <span class="mi">114</span><span class="p">,</span>  <span class="mi">118</span><span class="p">,</span>  <span class="mi">119</span><span class="p">,</span>  <span class="mi">121</span><span class="p">,</span>  <span class="mi">156</span><span class="p">,</span>  <span class="mi">169</span><span class="p">,</span>  <span class="mi">192</span><span class="p">,</span>  <span class="mi">206</span><span class="p">,</span>
        <span class="mi">209</span><span class="p">,</span>  <span class="mi">210</span><span class="p">,</span>  <span class="mi">216</span><span class="p">,</span>  <span class="mi">229</span><span class="p">,</span>  <span class="mi">232</span><span class="p">,</span>  <span class="mi">234</span><span class="p">,</span>  <span class="mi">246</span><span class="p">,</span>  <span class="mi">249</span><span class="p">,</span>  <span class="mi">260</span><span class="p">,</span>  <span class="mi">283</span><span class="p">,</span>  <span class="mi">293</span><span class="p">,</span>
        <span class="mi">296</span><span class="p">,</span>  <span class="mi">303</span><span class="p">,</span>  <span class="mi">320</span><span class="p">,</span>  <span class="mi">326</span><span class="p">,</span>  <span class="mi">359</span><span class="p">,</span>  <span class="mi">399</span><span class="p">,</span>  <span class="mi">427</span><span class="p">,</span>  <span class="mi">429</span><span class="p">,</span>  <span class="mi">435</span><span class="p">,</span>  <span class="mi">440</span><span class="p">,</span>  <span class="mi">451</span><span class="p">,</span>
        <span class="mi">453</span><span class="p">,</span>  <span class="mi">458</span><span class="p">,</span>  <span class="mi">462</span><span class="p">,</span>  <span class="mi">464</span><span class="p">,</span>  <span class="mi">473</span><span class="p">,</span>  <span class="mi">489</span><span class="p">,</span>  <span class="mi">519</span><span class="p">,</span>  <span class="mi">524</span><span class="p">,</span>  <span class="mi">526</span><span class="p">,</span>  <span class="mi">527</span><span class="p">,</span>  <span class="mi">542</span><span class="p">,</span>
        <span class="mi">577</span><span class="p">,</span>  <span class="mi">582</span><span class="p">,</span>  <span class="mi">596</span><span class="p">,</span>  <span class="mi">603</span><span class="p">,</span>  <span class="mi">612</span><span class="p">,</span>  <span class="mi">633</span><span class="p">,</span>  <span class="mi">639</span><span class="p">,</span>  <span class="mi">656</span><span class="p">,</span>  <span class="mi">662</span><span class="p">,</span>  <span class="mi">666</span><span class="p">,</span>  <span class="mi">667</span><span class="p">,</span>
        <span class="mi">668</span><span class="p">,</span>  <span class="mi">669</span><span class="p">,</span>  <span class="mi">689</span><span class="p">,</span>  <span class="mi">702</span><span class="p">,</span>  <span class="mi">709</span><span class="p">,</span>  <span class="mi">712</span><span class="p">,</span>  <span class="mi">733</span><span class="p">,</span>  <span class="mi">743</span><span class="p">,</span>  <span class="mi">745</span><span class="p">,</span>  <span class="mi">776</span><span class="p">,</span>  <span class="mi">781</span><span class="p">,</span>
        <span class="mi">787</span><span class="p">,</span>  <span class="mi">790</span><span class="p">,</span>  <span class="mi">818</span><span class="p">,</span>  <span class="mi">825</span><span class="p">,</span>  <span class="mi">849</span><span class="p">,</span>  <span class="mi">859</span><span class="p">,</span>  <span class="mi">860</span><span class="p">,</span>  <span class="mi">869</span><span class="p">,</span>  <span class="mi">872</span><span class="p">,</span>  <span class="mi">889</span><span class="p">,</span>  <span class="mi">903</span><span class="p">,</span>
        <span class="mi">927</span><span class="p">,</span>  <span class="mi">943</span><span class="p">,</span>  <span class="mi">949</span><span class="p">,</span>  <span class="mi">952</span><span class="p">,</span>  <span class="mi">957</span><span class="p">,</span>  <span class="mi">965</span><span class="p">,</span>  <span class="mi">979</span><span class="p">,</span>  <span class="mi">984</span><span class="p">,</span>  <span class="mi">997</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1015</span><span class="p">,</span>
       <span class="mi">1018</span><span class="p">,</span> <span class="mi">1028</span><span class="p">,</span> <span class="mi">1029</span><span class="p">,</span> <span class="mi">1040</span><span class="p">,</span> <span class="mi">1046</span><span class="p">,</span> <span class="mi">1049</span><span class="p">,</span> <span class="mi">1076</span><span class="p">,</span> <span class="mi">1078</span><span class="p">,</span> <span class="mi">1090</span><span class="p">,</span> <span class="mi">1093</span><span class="p">,</span> <span class="mi">1102</span><span class="p">,</span>
       <span class="mi">1107</span><span class="p">,</span> <span class="mi">1128</span><span class="p">,</span> <span class="mi">1137</span><span class="p">,</span> <span class="mi">1152</span><span class="p">,</span> <span class="mi">1168</span><span class="p">,</span> <span class="mi">1179</span><span class="p">,</span> <span class="mi">1195</span><span class="p">,</span> <span class="mi">1209</span><span class="p">,</span> <span class="mi">1268</span><span class="p">,</span> <span class="mi">1304</span><span class="p">,</span> <span class="mi">1310</span><span class="p">,</span>
       <span class="mi">1346</span><span class="p">,</span> <span class="mi">1349</span><span class="p">,</span> <span class="mi">1359</span><span class="p">,</span> <span class="mi">1363</span><span class="p">,</span> <span class="mi">1367</span><span class="p">,</span> <span class="mi">1368</span><span class="p">,</span> <span class="mi">1371</span><span class="p">,</span> <span class="mi">1372</span><span class="p">,</span> <span class="mi">1377</span><span class="p">,</span> <span class="mi">1386</span><span class="p">,</span> <span class="mi">1387</span><span class="p">,</span>
       <span class="mi">1403</span><span class="p">,</span> <span class="mi">1423</span><span class="p">,</span> <span class="mi">1443</span><span class="p">,</span> <span class="mi">1454</span><span class="p">,</span> <span class="mi">1471</span><span class="p">,</span> <span class="mi">1479</span><span class="p">,</span> <span class="mi">1489</span><span class="p">,</span> <span class="mi">1495</span><span class="p">,</span> <span class="mi">1501</span><span class="p">,</span> <span class="mi">1502</span><span class="p">,</span> <span class="mi">1512</span><span class="p">,</span>
       <span class="mi">1517</span><span class="p">,</span> <span class="mi">1530</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1571</span><span class="p">,</span> <span class="mi">1578</span><span class="p">,</span> <span class="mi">1590</span><span class="p">,</span> <span class="mi">1596</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">1605</span><span class="p">,</span> <span class="mi">1606</span><span class="p">,</span> <span class="mi">1625</span><span class="p">,</span>
       <span class="mi">1626</span><span class="p">,</span> <span class="mi">1645</span><span class="p">,</span> <span class="mi">1664</span><span class="p">,</span> <span class="mi">1678</span><span class="p">,</span> <span class="mi">1682</span><span class="p">,</span> <span class="mi">1701</span><span class="p">,</span> <span class="mi">1709</span><span class="p">,</span> <span class="mi">1712</span><span class="p">,</span> <span class="mi">1723</span><span class="p">,</span> <span class="mi">1725</span><span class="p">,</span> <span class="mi">1729</span><span class="p">,</span>
       <span class="mi">1742</span><span class="p">,</span> <span class="mi">1769</span><span class="p">,</span> <span class="mi">1771</span><span class="p">,</span> <span class="mi">1775</span><span class="p">,</span> <span class="mi">1796</span><span class="p">,</span> <span class="mi">1797</span><span class="p">,</span> <span class="mi">1798</span><span class="p">,</span> <span class="mi">1819</span><span class="p">,</span> <span class="mi">1828</span><span class="p">,</span> <span class="mi">1837</span><span class="p">,</span> <span class="mi">1857</span><span class="p">,</span>
       <span class="mi">1868</span><span class="p">,</span> <span class="mi">1876</span><span class="p">,</span> <span class="mi">1877</span><span class="p">,</span> <span class="mi">1897</span><span class="p">,</span> <span class="mi">1904</span><span class="p">,</span> <span class="mi">1907</span><span class="p">,</span> <span class="mi">1916</span><span class="p">,</span> <span class="mi">1926</span><span class="p">,</span> <span class="mi">1927</span><span class="p">,</span> <span class="mi">1930</span><span class="p">,</span> <span class="mi">1956</span><span class="p">,</span>
       <span class="mi">1963</span><span class="p">,</span> <span class="mi">1969</span><span class="p">,</span> <span class="mi">1995</span><span class="p">,</span> <span class="mi">1999</span><span class="p">,</span> <span class="mi">2009</span><span class="p">,</span> <span class="mi">2051</span><span class="p">,</span> <span class="mi">2058</span><span class="p">,</span> <span class="mi">2066</span><span class="p">,</span> <span class="mi">2079</span><span class="p">,</span> <span class="mi">2081</span><span class="p">,</span> <span class="mi">2082</span><span class="p">,</span>
       <span class="mi">2084</span><span class="p">,</span> <span class="mi">2100</span><span class="p">,</span> <span class="mi">2101</span><span class="p">,</span> <span class="mi">2112</span><span class="p">,</span> <span class="mi">2121</span><span class="p">,</span> <span class="mi">2144</span><span class="p">,</span> <span class="mi">2147</span><span class="p">,</span> <span class="mi">2160</span><span class="p">,</span> <span class="mi">2191</span><span class="p">,</span> <span class="mi">2192</span><span class="p">,</span> <span class="mi">2195</span><span class="p">,</span>
       <span class="mi">2218</span><span class="p">,</span> <span class="mi">2245</span><span class="p">,</span> <span class="mi">2253</span><span class="p">,</span> <span class="mi">2257</span><span class="p">,</span> <span class="mi">2269</span><span class="p">,</span> <span class="mi">2278</span><span class="p">,</span> <span class="mi">2298</span><span class="p">,</span> <span class="mi">2310</span><span class="p">,</span> <span class="mi">2325</span><span class="p">,</span> <span class="mi">2327</span><span class="p">,</span> <span class="mi">2333</span><span class="p">,</span>
       <span class="mi">2340</span><span class="p">,</span> <span class="mi">2352</span><span class="p">,</span> <span class="mi">2353</span><span class="p">,</span> <span class="mi">2373</span><span class="p">,</span> <span class="mi">2396</span><span class="p">,</span> <span class="mi">2403</span><span class="p">,</span> <span class="mi">2404</span><span class="p">,</span> <span class="mi">2411</span><span class="p">,</span> <span class="mi">2435</span><span class="p">,</span> <span class="mi">2436</span><span class="p">,</span> <span class="mi">2440</span><span class="p">,</span>
       <span class="mi">2473</span><span class="p">,</span> <span class="mi">2483</span><span class="p">,</span> <span class="mi">2493</span><span class="p">,</span> <span class="mi">2500</span><span class="p">,</span> <span class="mi">2525</span><span class="p">,</span> <span class="mi">2526</span><span class="p">,</span> <span class="mi">2528</span><span class="p">,</span> <span class="mi">2532</span><span class="p">,</span> <span class="mi">2538</span><span class="p">,</span> <span class="mi">2539</span><span class="p">,</span> <span class="mi">2557</span><span class="p">,</span>
       <span class="mi">2561</span><span class="p">,</span> <span class="mi">2581</span><span class="p">,</span> <span class="mi">2582</span><span class="p">,</span> <span class="mi">2584</span><span class="p">,</span> <span class="mi">2586</span><span class="p">,</span> <span class="mi">2597</span><span class="p">,</span> <span class="mi">2606</span><span class="p">,</span> <span class="mi">2615</span><span class="p">,</span> <span class="mi">2617</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2624</span><span class="p">,</span>
       <span class="mi">2629</span><span class="p">,</span> <span class="mi">2642</span><span class="p">,</span> <span class="mi">2709</span><span class="p">,</span> <span class="mi">2718</span><span class="p">,</span> <span class="mi">2729</span><span class="p">,</span> <span class="mi">2736</span><span class="p">,</span> <span class="mi">2745</span><span class="p">,</span> <span class="mi">2746</span><span class="p">,</span> <span class="mi">2765</span><span class="p">,</span> <span class="mi">2770</span><span class="p">,</span> <span class="mi">2782</span><span class="p">,</span>
       <span class="mi">2806</span><span class="p">,</span> <span class="mi">2817</span><span class="p">,</span> <span class="mi">2826</span><span class="p">,</span> <span class="mi">2839</span><span class="p">,</span> <span class="mi">2854</span><span class="p">,</span> <span class="mi">2864</span><span class="p">,</span> <span class="mi">2873</span><span class="p">,</span> <span class="mi">2882</span><span class="p">,</span> <span class="mi">2890</span><span class="p">,</span> <span class="mi">2897</span><span class="p">,</span> <span class="mi">2899</span><span class="p">,</span>
       <span class="mi">2904</span><span class="p">,</span> <span class="mi">2914</span><span class="p">,</span> <span class="mi">2919</span><span class="p">,</span> <span class="mi">2935</span><span class="p">,</span> <span class="mi">2944</span><span class="p">,</span> <span class="mi">2952</span><span class="p">,</span> <span class="mi">2955</span><span class="p">,</span> <span class="mi">2974</span><span class="p">,</span> <span class="mi">2975</span><span class="p">,</span> <span class="mi">2996</span><span class="p">,</span> <span class="mi">3001</span><span class="p">,</span>
       <span class="mi">3012</span><span class="p">,</span> <span class="mi">3015</span><span class="p">,</span> <span class="mi">3016</span><span class="p">,</span> <span class="mi">3021</span><span class="p">,</span> <span class="mi">3024</span><span class="p">,</span> <span class="mi">3035</span><span class="p">,</span> <span class="mi">3049</span><span class="p">,</span> <span class="mi">3067</span><span class="p">,</span> <span class="mi">3106</span><span class="p">,</span> <span class="mi">3107</span><span class="p">,</span> <span class="mi">3128</span><span class="p">,</span>
       <span class="mi">3131</span><span class="p">,</span> <span class="mi">3135</span><span class="p">,</span> <span class="mi">3143</span><span class="p">,</span> <span class="mi">3160</span><span class="p">,</span> <span class="mi">3175</span><span class="p">,</span> <span class="mi">3195</span><span class="p">,</span> <span class="mi">3198</span><span class="p">,</span> <span class="mi">3213</span><span class="p">,</span> <span class="mi">3231</span><span class="p">,</span> <span class="mi">3241</span><span class="p">,</span> <span class="mi">3247</span><span class="p">,</span>
       <span class="mi">3248</span><span class="p">,</span> <span class="mi">3259</span><span class="p">,</span> <span class="mi">3262</span><span class="p">,</span> <span class="mi">3269</span><span class="p">,</span> <span class="mi">3286</span><span class="p">,</span> <span class="mi">3309</span><span class="p">,</span> <span class="mi">3328</span><span class="p">,</span> <span class="mi">3337</span><span class="p">,</span> <span class="mi">3367</span><span class="p">,</span> <span class="mi">3369</span><span class="p">,</span> <span class="mi">3376</span><span class="p">,</span>
       <span class="mi">3377</span><span class="p">,</span> <span class="mi">3391</span><span class="p">,</span> <span class="mi">3396</span><span class="p">,</span> <span class="mi">3409</span><span class="p">,</span> <span class="mi">3410</span><span class="p">,</span> <span class="mi">3429</span><span class="p">,</span> <span class="mi">3434</span><span class="p">,</span> <span class="mi">3441</span><span class="p">,</span> <span class="mi">3443</span><span class="p">,</span> <span class="mi">3461</span><span class="p">,</span> <span class="mi">3479</span><span class="p">,</span>
       <span class="mi">3490</span><span class="p">,</span> <span class="mi">3492</span><span class="p">,</span> <span class="mi">3514</span><span class="p">,</span> <span class="mi">3516</span><span class="p">,</span> <span class="mi">3529</span><span class="p">,</span> <span class="mi">3534</span><span class="p">,</span> <span class="mi">3541</span><span class="p">,</span> <span class="mi">3562</span><span class="p">,</span> <span class="mi">3565</span><span class="p">,</span> <span class="mi">3568</span><span class="p">,</span> <span class="mi">3585</span><span class="p">,</span>
       <span class="mi">3590</span><span class="p">,</span> <span class="mi">3603</span><span class="p">,</span> <span class="mi">3610</span><span class="p">,</span> <span class="mi">3612</span><span class="p">])}</span>
<span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span>   <span class="mi">1</span><span class="p">,</span>   <span class="mi">21</span><span class="p">,</span>   <span class="mi">34</span><span class="p">,</span>   <span class="mi">37</span><span class="p">,</span>   <span class="mi">51</span><span class="p">,</span>   <span class="mi">56</span><span class="p">,</span>   <span class="mi">63</span><span class="p">,</span>   <span class="mi">68</span><span class="p">,</span>   <span class="mi">69</span><span class="p">,</span>   <span class="mi">75</span><span class="p">,</span>   <span class="mi">81</span><span class="p">,</span>
         <span class="mi">88</span><span class="p">,</span>   <span class="mi">95</span><span class="p">,</span>  <span class="mi">108</span><span class="p">,</span>  <span class="mi">114</span><span class="p">,</span>  <span class="mi">118</span><span class="p">,</span>  <span class="mi">119</span><span class="p">,</span>  <span class="mi">121</span><span class="p">,</span>  <span class="mi">156</span><span class="p">,</span>  <span class="mi">169</span><span class="p">,</span>  <span class="mi">192</span><span class="p">,</span>  <span class="mi">206</span><span class="p">,</span>
        <span class="mi">209</span><span class="p">,</span>  <span class="mi">210</span><span class="p">,</span>  <span class="mi">216</span><span class="p">,</span>  <span class="mi">229</span><span class="p">,</span>  <span class="mi">232</span><span class="p">,</span>  <span class="mi">234</span><span class="p">,</span>  <span class="mi">246</span><span class="p">,</span>  <span class="mi">249</span><span class="p">,</span>  <span class="mi">260</span><span class="p">,</span>  <span class="mi">283</span><span class="p">,</span>  <span class="mi">293</span><span class="p">,</span>
        <span class="mi">296</span><span class="p">,</span>  <span class="mi">303</span><span class="p">,</span>  <span class="mi">320</span><span class="p">,</span>  <span class="mi">326</span><span class="p">,</span>  <span class="mi">359</span><span class="p">,</span>  <span class="mi">399</span><span class="p">,</span>  <span class="mi">427</span><span class="p">,</span>  <span class="mi">429</span><span class="p">,</span>  <span class="mi">435</span><span class="p">,</span>  <span class="mi">440</span><span class="p">,</span>  <span class="mi">451</span><span class="p">,</span>
        <span class="mi">453</span><span class="p">,</span>  <span class="mi">458</span><span class="p">,</span>  <span class="mi">462</span><span class="p">,</span>  <span class="mi">464</span><span class="p">,</span>  <span class="mi">473</span><span class="p">,</span>  <span class="mi">489</span><span class="p">,</span>  <span class="mi">519</span><span class="p">,</span>  <span class="mi">524</span><span class="p">,</span>  <span class="mi">526</span><span class="p">,</span>  <span class="mi">527</span><span class="p">,</span>  <span class="mi">542</span><span class="p">,</span>
        <span class="mi">577</span><span class="p">,</span>  <span class="mi">582</span><span class="p">,</span>  <span class="mi">596</span><span class="p">,</span>  <span class="mi">603</span><span class="p">,</span>  <span class="mi">612</span><span class="p">,</span>  <span class="mi">633</span><span class="p">,</span>  <span class="mi">639</span><span class="p">,</span>  <span class="mi">656</span><span class="p">,</span>  <span class="mi">662</span><span class="p">,</span>  <span class="mi">666</span><span class="p">,</span>  <span class="mi">667</span><span class="p">,</span>
        <span class="mi">668</span><span class="p">,</span>  <span class="mi">669</span><span class="p">,</span>  <span class="mi">689</span><span class="p">,</span>  <span class="mi">702</span><span class="p">,</span>  <span class="mi">709</span><span class="p">,</span>  <span class="mi">712</span><span class="p">,</span>  <span class="mi">733</span><span class="p">,</span>  <span class="mi">743</span><span class="p">,</span>  <span class="mi">745</span><span class="p">,</span>  <span class="mi">776</span><span class="p">,</span>  <span class="mi">781</span><span class="p">,</span>
        <span class="mi">787</span><span class="p">,</span>  <span class="mi">790</span><span class="p">,</span>  <span class="mi">818</span><span class="p">,</span>  <span class="mi">825</span><span class="p">,</span>  <span class="mi">849</span><span class="p">,</span>  <span class="mi">859</span><span class="p">,</span>  <span class="mi">860</span><span class="p">,</span>  <span class="mi">869</span><span class="p">,</span>  <span class="mi">872</span><span class="p">,</span>  <span class="mi">889</span><span class="p">,</span>  <span class="mi">903</span><span class="p">,</span>
        <span class="mi">927</span><span class="p">,</span>  <span class="mi">943</span><span class="p">,</span>  <span class="mi">949</span><span class="p">,</span>  <span class="mi">952</span><span class="p">,</span>  <span class="mi">957</span><span class="p">,</span>  <span class="mi">965</span><span class="p">,</span>  <span class="mi">979</span><span class="p">,</span>  <span class="mi">984</span><span class="p">,</span>  <span class="mi">997</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">1015</span><span class="p">,</span>
       <span class="mi">1018</span><span class="p">,</span> <span class="mi">1028</span><span class="p">,</span> <span class="mi">1029</span><span class="p">,</span> <span class="mi">1040</span><span class="p">,</span> <span class="mi">1046</span><span class="p">,</span> <span class="mi">1049</span><span class="p">,</span> <span class="mi">1076</span><span class="p">,</span> <span class="mi">1078</span><span class="p">,</span> <span class="mi">1090</span><span class="p">,</span> <span class="mi">1093</span><span class="p">,</span> <span class="mi">1102</span><span class="p">,</span>
       <span class="mi">1107</span><span class="p">,</span> <span class="mi">1128</span><span class="p">,</span> <span class="mi">1137</span><span class="p">,</span> <span class="mi">1152</span><span class="p">,</span> <span class="mi">1168</span><span class="p">,</span> <span class="mi">1179</span><span class="p">,</span> <span class="mi">1195</span><span class="p">,</span> <span class="mi">1209</span><span class="p">,</span> <span class="mi">1268</span><span class="p">,</span> <span class="mi">1304</span><span class="p">,</span> <span class="mi">1310</span><span class="p">,</span>
       <span class="mi">1346</span><span class="p">,</span> <span class="mi">1349</span><span class="p">,</span> <span class="mi">1359</span><span class="p">,</span> <span class="mi">1363</span><span class="p">,</span> <span class="mi">1367</span><span class="p">,</span> <span class="mi">1368</span><span class="p">,</span> <span class="mi">1371</span><span class="p">,</span> <span class="mi">1372</span><span class="p">,</span> <span class="mi">1377</span><span class="p">,</span> <span class="mi">1386</span><span class="p">,</span> <span class="mi">1387</span><span class="p">,</span>
       <span class="mi">1403</span><span class="p">,</span> <span class="mi">1423</span><span class="p">,</span> <span class="mi">1443</span><span class="p">,</span> <span class="mi">1454</span><span class="p">,</span> <span class="mi">1471</span><span class="p">,</span> <span class="mi">1479</span><span class="p">,</span> <span class="mi">1489</span><span class="p">,</span> <span class="mi">1495</span><span class="p">,</span> <span class="mi">1501</span><span class="p">,</span> <span class="mi">1502</span><span class="p">,</span> <span class="mi">1512</span><span class="p">,</span>
       <span class="mi">1517</span><span class="p">,</span> <span class="mi">1530</span><span class="p">,</span> <span class="mi">1532</span><span class="p">,</span> <span class="mi">1571</span><span class="p">,</span> <span class="mi">1578</span><span class="p">,</span> <span class="mi">1590</span><span class="p">,</span> <span class="mi">1596</span><span class="p">,</span> <span class="mi">1600</span><span class="p">,</span> <span class="mi">1605</span><span class="p">,</span> <span class="mi">1606</span><span class="p">,</span> <span class="mi">1625</span><span class="p">,</span>
       <span class="mi">1626</span><span class="p">,</span> <span class="mi">1645</span><span class="p">,</span> <span class="mi">1664</span><span class="p">,</span> <span class="mi">1678</span><span class="p">,</span> <span class="mi">1682</span><span class="p">,</span> <span class="mi">1701</span><span class="p">,</span> <span class="mi">1709</span><span class="p">,</span> <span class="mi">1712</span><span class="p">,</span> <span class="mi">1723</span><span class="p">,</span> <span class="mi">1725</span><span class="p">,</span> <span class="mi">1729</span><span class="p">,</span>
       <span class="mi">1742</span><span class="p">,</span> <span class="mi">1769</span><span class="p">,</span> <span class="mi">1771</span><span class="p">,</span> <span class="mi">1775</span><span class="p">,</span> <span class="mi">1796</span><span class="p">,</span> <span class="mi">1797</span><span class="p">,</span> <span class="mi">1798</span><span class="p">,</span> <span class="mi">1819</span><span class="p">,</span> <span class="mi">1828</span><span class="p">,</span> <span class="mi">1837</span><span class="p">,</span> <span class="mi">1857</span><span class="p">,</span>
       <span class="mi">1868</span><span class="p">,</span> <span class="mi">1876</span><span class="p">,</span> <span class="mi">1877</span><span class="p">,</span> <span class="mi">1897</span><span class="p">,</span> <span class="mi">1904</span><span class="p">,</span> <span class="mi">1907</span><span class="p">,</span> <span class="mi">1916</span><span class="p">,</span> <span class="mi">1926</span><span class="p">,</span> <span class="mi">1927</span><span class="p">,</span> <span class="mi">1930</span><span class="p">,</span> <span class="mi">1956</span><span class="p">,</span>
       <span class="mi">1963</span><span class="p">,</span> <span class="mi">1969</span><span class="p">,</span> <span class="mi">1995</span><span class="p">,</span> <span class="mi">1999</span><span class="p">,</span> <span class="mi">2009</span><span class="p">,</span> <span class="mi">2051</span><span class="p">,</span> <span class="mi">2058</span><span class="p">,</span> <span class="mi">2066</span><span class="p">,</span> <span class="mi">2079</span><span class="p">,</span> <span class="mi">2081</span><span class="p">,</span> <span class="mi">2082</span><span class="p">,</span>
       <span class="mi">2084</span><span class="p">,</span> <span class="mi">2100</span><span class="p">,</span> <span class="mi">2101</span><span class="p">,</span> <span class="mi">2112</span><span class="p">,</span> <span class="mi">2121</span><span class="p">,</span> <span class="mi">2144</span><span class="p">,</span> <span class="mi">2147</span><span class="p">,</span> <span class="mi">2160</span><span class="p">,</span> <span class="mi">2191</span><span class="p">,</span> <span class="mi">2192</span><span class="p">,</span> <span class="mi">2195</span><span class="p">,</span>
       <span class="mi">2218</span><span class="p">,</span> <span class="mi">2245</span><span class="p">,</span> <span class="mi">2253</span><span class="p">,</span> <span class="mi">2257</span><span class="p">,</span> <span class="mi">2269</span><span class="p">,</span> <span class="mi">2278</span><span class="p">,</span> <span class="mi">2298</span><span class="p">,</span> <span class="mi">2310</span><span class="p">,</span> <span class="mi">2325</span><span class="p">,</span> <span class="mi">2327</span><span class="p">,</span> <span class="mi">2333</span><span class="p">,</span>
       <span class="mi">2340</span><span class="p">,</span> <span class="mi">2352</span><span class="p">,</span> <span class="mi">2353</span><span class="p">,</span> <span class="mi">2373</span><span class="p">,</span> <span class="mi">2396</span><span class="p">,</span> <span class="mi">2403</span><span class="p">,</span> <span class="mi">2404</span><span class="p">,</span> <span class="mi">2411</span><span class="p">,</span> <span class="mi">2435</span><span class="p">,</span> <span class="mi">2436</span><span class="p">,</span> <span class="mi">2440</span><span class="p">,</span>
       <span class="mi">2473</span><span class="p">,</span> <span class="mi">2483</span><span class="p">,</span> <span class="mi">2493</span><span class="p">,</span> <span class="mi">2500</span><span class="p">,</span> <span class="mi">2525</span><span class="p">,</span> <span class="mi">2526</span><span class="p">,</span> <span class="mi">2528</span><span class="p">,</span> <span class="mi">2532</span><span class="p">,</span> <span class="mi">2538</span><span class="p">,</span> <span class="mi">2539</span><span class="p">,</span> <span class="mi">2557</span><span class="p">,</span>
       <span class="mi">2561</span><span class="p">,</span> <span class="mi">2581</span><span class="p">,</span> <span class="mi">2582</span><span class="p">,</span> <span class="mi">2584</span><span class="p">,</span> <span class="mi">2586</span><span class="p">,</span> <span class="mi">2597</span><span class="p">,</span> <span class="mi">2606</span><span class="p">,</span> <span class="mi">2615</span><span class="p">,</span> <span class="mi">2617</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2624</span><span class="p">,</span>
       <span class="mi">2629</span><span class="p">,</span> <span class="mi">2642</span><span class="p">,</span> <span class="mi">2709</span><span class="p">,</span> <span class="mi">2718</span><span class="p">,</span> <span class="mi">2729</span><span class="p">,</span> <span class="mi">2736</span><span class="p">,</span> <span class="mi">2745</span><span class="p">,</span> <span class="mi">2746</span><span class="p">,</span> <span class="mi">2765</span><span class="p">,</span> <span class="mi">2770</span><span class="p">,</span> <span class="mi">2782</span><span class="p">,</span>
       <span class="mi">2806</span><span class="p">,</span> <span class="mi">2817</span><span class="p">,</span> <span class="mi">2826</span><span class="p">,</span> <span class="mi">2839</span><span class="p">,</span> <span class="mi">2854</span><span class="p">,</span> <span class="mi">2864</span><span class="p">,</span> <span class="mi">2873</span><span class="p">,</span> <span class="mi">2882</span><span class="p">,</span> <span class="mi">2890</span><span class="p">,</span> <span class="mi">2897</span><span class="p">,</span> <span class="mi">2899</span><span class="p">,</span>
       <span class="mi">2904</span><span class="p">,</span> <span class="mi">2914</span><span class="p">,</span> <span class="mi">2919</span><span class="p">,</span> <span class="mi">2935</span><span class="p">,</span> <span class="mi">2944</span><span class="p">,</span> <span class="mi">2952</span><span class="p">,</span> <span class="mi">2955</span><span class="p">,</span> <span class="mi">2974</span><span class="p">,</span> <span class="mi">2975</span><span class="p">,</span> <span class="mi">2996</span><span class="p">,</span> <span class="mi">3001</span><span class="p">,</span>
       <span class="mi">3012</span><span class="p">,</span> <span class="mi">3015</span><span class="p">,</span> <span class="mi">3016</span><span class="p">,</span> <span class="mi">3021</span><span class="p">,</span> <span class="mi">3024</span><span class="p">,</span> <span class="mi">3035</span><span class="p">,</span> <span class="mi">3049</span><span class="p">,</span> <span class="mi">3067</span><span class="p">,</span> <span class="mi">3106</span><span class="p">,</span> <span class="mi">3107</span><span class="p">,</span> <span class="mi">3128</span><span class="p">,</span>
       <span class="mi">3131</span><span class="p">,</span> <span class="mi">3135</span><span class="p">,</span> <span class="mi">3143</span><span class="p">,</span> <span class="mi">3160</span><span class="p">,</span> <span class="mi">3175</span><span class="p">,</span> <span class="mi">3195</span><span class="p">,</span> <span class="mi">3198</span><span class="p">,</span> <span class="mi">3213</span><span class="p">,</span> <span class="mi">3231</span><span class="p">,</span> <span class="mi">3241</span><span class="p">,</span> <span class="mi">3247</span><span class="p">,</span>
       <span class="mi">3248</span><span class="p">,</span> <span class="mi">3259</span><span class="p">,</span> <span class="mi">3262</span><span class="p">,</span> <span class="mi">3269</span><span class="p">,</span> <span class="mi">3286</span><span class="p">,</span> <span class="mi">3309</span><span class="p">,</span> <span class="mi">3328</span><span class="p">,</span> <span class="mi">3337</span><span class="p">,</span> <span class="mi">3367</span><span class="p">,</span> <span class="mi">3369</span><span class="p">,</span> <span class="mi">3376</span><span class="p">,</span>
       <span class="mi">3377</span><span class="p">,</span> <span class="mi">3391</span><span class="p">,</span> <span class="mi">3396</span><span class="p">,</span> <span class="mi">3409</span><span class="p">,</span> <span class="mi">3410</span><span class="p">,</span> <span class="mi">3429</span><span class="p">,</span> <span class="mi">3434</span><span class="p">,</span> <span class="mi">3441</span><span class="p">,</span> <span class="mi">3443</span><span class="p">,</span> <span class="mi">3461</span><span class="p">,</span> <span class="mi">3479</span><span class="p">,</span>
       <span class="mi">3490</span><span class="p">,</span> <span class="mi">3492</span><span class="p">,</span> <span class="mi">3514</span><span class="p">,</span> <span class="mi">3516</span><span class="p">,</span> <span class="mi">3529</span><span class="p">,</span> <span class="mi">3534</span><span class="p">,</span> <span class="mi">3541</span><span class="p">,</span> <span class="mi">3562</span><span class="p">,</span> <span class="mi">3565</span><span class="p">,</span> <span class="mi">3568</span><span class="p">,</span> <span class="mi">3585</span><span class="p">,</span>
       <span class="mi">3590</span><span class="p">,</span> <span class="mi">3603</span><span class="p">,</span> <span class="mi">3610</span><span class="p">,</span> <span class="mi">3612</span><span class="p">]),</span> <span class="mi">1</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">3622</span><span class="p">,</span> <span class="mi">3632</span><span class="p">,</span> <span class="mi">3661</span><span class="p">,</span> <span class="mi">3667</span><span class="p">,</span> <span class="mi">3673</span><span class="p">,</span> <span class="mi">3677</span><span class="p">,</span> <span class="mi">3691</span><span class="p">,</span> <span class="mi">3693</span><span class="p">,</span> <span class="mi">3698</span><span class="p">,</span> <span class="mi">3702</span><span class="p">,</span> <span class="mi">3709</span><span class="p">,</span>
       <span class="mi">3715</span><span class="p">,</span> <span class="mi">3720</span><span class="p">,</span> <span class="mi">3734</span><span class="p">,</span> <span class="mi">3743</span><span class="p">,</span> <span class="mi">3776</span><span class="p">,</span> <span class="mi">3777</span><span class="p">,</span> <span class="mi">3778</span><span class="p">,</span> <span class="mi">3806</span><span class="p">,</span> <span class="mi">3809</span><span class="p">,</span> <span class="mi">3829</span><span class="p">,</span> <span class="mi">3849</span><span class="p">,</span>
       <span class="mi">3876</span><span class="p">,</span> <span class="mi">3881</span><span class="p">,</span> <span class="mi">3882</span><span class="p">,</span> <span class="mi">3884</span><span class="p">,</span> <span class="mi">3888</span><span class="p">,</span> <span class="mi">3896</span><span class="p">,</span> <span class="mi">3906</span><span class="p">,</span> <span class="mi">3929</span><span class="p">,</span> <span class="mi">3937</span><span class="p">,</span> <span class="mi">3943</span><span class="p">,</span> <span class="mi">3953</span><span class="p">,</span>
       <span class="mi">3964</span><span class="p">,</span> <span class="mi">4002</span><span class="p">,</span> <span class="mi">4008</span><span class="p">,</span> <span class="mi">4009</span><span class="p">,</span> <span class="mi">4023</span><span class="p">,</span> <span class="mi">4027</span><span class="p">,</span> <span class="mi">4044</span><span class="p">,</span> <span class="mi">4047</span><span class="p">,</span> <span class="mi">4067</span><span class="p">,</span> <span class="mi">4082</span><span class="p">,</span> <span class="mi">4106</span><span class="p">,</span>
       <span class="mi">4108</span><span class="p">,</span> <span class="mi">4115</span><span class="p">,</span> <span class="mi">4123</span><span class="p">,</span> <span class="mi">4142</span><span class="p">,</span> <span class="mi">4145</span><span class="p">,</span> <span class="mi">4159</span><span class="p">,</span> <span class="mi">4181</span><span class="p">,</span> <span class="mi">4201</span><span class="p">,</span> <span class="mi">4203</span><span class="p">,</span> <span class="mi">4216</span><span class="p">,</span> <span class="mi">4218</span><span class="p">,</span>
       <span class="mi">4220</span><span class="p">,</span> <span class="mi">4239</span><span class="p">,</span> <span class="mi">4245</span><span class="p">,</span> <span class="mi">4265</span><span class="p">,</span> <span class="mi">4270</span><span class="p">,</span> <span class="mi">4279</span><span class="p">,</span> <span class="mi">4284</span><span class="p">,</span> <span class="mi">4288</span><span class="p">,</span> <span class="mi">4289</span><span class="p">,</span> <span class="mi">4310</span><span class="p">,</span> <span class="mi">4316</span><span class="p">,</span>
       <span class="mi">4321</span><span class="p">,</span> <span class="mi">4325</span><span class="p">,</span> <span class="mi">4340</span><span class="p">,</span> <span class="mi">4343</span><span class="p">,</span> <span class="mi">4344</span><span class="p">,</span> <span class="mi">4347</span><span class="p">,</span> <span class="mi">4356</span><span class="p">,</span> <span class="mi">4374</span><span class="p">,</span> <span class="mi">4389</span><span class="p">,</span> <span class="mi">4415</span><span class="p">,</span> <span class="mi">4444</span><span class="p">,</span>
       <span class="mi">4453</span><span class="p">,</span> <span class="mi">4455</span><span class="p">,</span> <span class="mi">4465</span><span class="p">,</span> <span class="mi">4485</span><span class="p">,</span> <span class="mi">4488</span><span class="p">,</span> <span class="mi">4496</span><span class="p">,</span> <span class="mi">4500</span><span class="p">,</span> <span class="mi">4505</span><span class="p">,</span> <span class="mi">4527</span><span class="p">,</span> <span class="mi">4532</span><span class="p">,</span> <span class="mi">4535</span><span class="p">,</span>
       <span class="mi">4539</span><span class="p">,</span> <span class="mi">4556</span><span class="p">,</span> <span class="mi">4563</span><span class="p">,</span> <span class="mi">4565</span><span class="p">,</span> <span class="mi">4588</span><span class="p">,</span> <span class="mi">4597</span><span class="p">,</span> <span class="mi">4607</span><span class="p">,</span> <span class="mi">4608</span><span class="p">,</span> <span class="mi">4624</span><span class="p">,</span> <span class="mi">4629</span><span class="p">,</span> <span class="mi">4642</span><span class="p">,</span>
       <span class="mi">4653</span><span class="p">,</span> <span class="mi">4655</span><span class="p">,</span> <span class="mi">4656</span><span class="p">,</span> <span class="mi">4682</span><span class="p">,</span> <span class="mi">4686</span><span class="p">,</span> <span class="mi">4688</span><span class="p">,</span> <span class="mi">4713</span><span class="p">,</span> <span class="mi">4718</span><span class="p">,</span> <span class="mi">4729</span><span class="p">,</span> <span class="mi">4738</span><span class="p">,</span> <span class="mi">4744</span><span class="p">,</span>
       <span class="mi">4748</span><span class="p">,</span> <span class="mi">4749</span><span class="p">,</span> <span class="mi">4756</span><span class="p">,</span> <span class="mi">4773</span><span class="p">,</span> <span class="mi">4776</span><span class="p">,</span> <span class="mi">4793</span><span class="p">,</span> <span class="mi">4804</span><span class="p">,</span> <span class="mi">4849</span><span class="p">,</span> <span class="mi">4852</span><span class="p">,</span> <span class="mi">4854</span><span class="p">,</span> <span class="mi">4855</span><span class="p">,</span>
       <span class="mi">4870</span><span class="p">,</span> <span class="mi">4889</span><span class="p">,</span> <span class="mi">4892</span><span class="p">,</span> <span class="mi">4906</span><span class="p">,</span> <span class="mi">4911</span><span class="p">,</span> <span class="mi">4918</span><span class="p">,</span> <span class="mi">4926</span><span class="p">,</span> <span class="mi">4931</span><span class="p">,</span> <span class="mi">4951</span><span class="p">,</span> <span class="mi">4962</span><span class="p">,</span> <span class="mi">4981</span><span class="p">,</span>
       <span class="mi">4985</span><span class="p">,</span> <span class="mi">4994</span><span class="p">,</span> <span class="mi">5007</span><span class="p">,</span> <span class="mi">5010</span><span class="p">,</span> <span class="mi">5015</span><span class="p">,</span> <span class="mi">5019</span><span class="p">,</span> <span class="mi">5048</span><span class="p">,</span> <span class="mi">5052</span><span class="p">,</span> <span class="mi">5053</span><span class="p">,</span> <span class="mi">5072</span><span class="p">,</span> <span class="mi">5074</span><span class="p">,</span>
       <span class="mi">5082</span><span class="p">,</span> <span class="mi">5083</span><span class="p">,</span> <span class="mi">5096</span><span class="p">,</span> <span class="mi">5108</span><span class="p">,</span> <span class="mi">5115</span><span class="p">,</span> <span class="mi">5120</span><span class="p">,</span> <span class="mi">5131</span><span class="p">,</span> <span class="mi">5133</span><span class="p">,</span> <span class="mi">5144</span><span class="p">,</span> <span class="mi">5147</span><span class="p">,</span> <span class="mi">5154</span><span class="p">,</span>
       <span class="mi">5167</span><span class="p">,</span> <span class="mi">5187</span><span class="p">,</span> <span class="mi">5192</span><span class="p">,</span> <span class="mi">5194</span><span class="p">,</span> <span class="mi">5196</span><span class="p">,</span> <span class="mi">5202</span><span class="p">,</span> <span class="mi">5203</span><span class="p">,</span> <span class="mi">5228</span><span class="p">,</span> <span class="mi">5244</span><span class="p">,</span> <span class="mi">5246</span><span class="p">,</span> <span class="mi">5249</span><span class="p">,</span>
       <span class="mi">5266</span><span class="p">,</span> <span class="mi">5268</span><span class="p">,</span> <span class="mi">5272</span><span class="p">,</span> <span class="mi">5275</span><span class="p">,</span> <span class="mi">5285</span><span class="p">,</span> <span class="mi">5286</span><span class="p">,</span> <span class="mi">5288</span><span class="p">,</span> <span class="mi">5298</span><span class="p">,</span> <span class="mi">5317</span><span class="p">,</span> <span class="mi">5319</span><span class="p">,</span> <span class="mi">5321</span><span class="p">,</span>
       <span class="mi">5327</span><span class="p">,</span> <span class="mi">5330</span><span class="p">,</span> <span class="mi">5340</span><span class="p">,</span> <span class="mi">5342</span><span class="p">,</span> <span class="mi">5344</span><span class="p">,</span> <span class="mi">5353</span><span class="p">,</span> <span class="mi">5368</span><span class="p">,</span> <span class="mi">5373</span><span class="p">,</span> <span class="mi">5393</span><span class="p">,</span> <span class="mi">5398</span><span class="p">,</span> <span class="mi">5461</span><span class="p">,</span>
       <span class="mi">5462</span><span class="p">,</span> <span class="mi">5464</span><span class="p">,</span> <span class="mi">5468</span><span class="p">,</span> <span class="mi">5469</span><span class="p">,</span> <span class="mi">5470</span><span class="p">,</span> <span class="mi">5475</span><span class="p">,</span> <span class="mi">5487</span><span class="p">,</span> <span class="mi">5488</span><span class="p">,</span> <span class="mi">5498</span><span class="p">,</span> <span class="mi">5502</span><span class="p">,</span> <span class="mi">5505</span><span class="p">,</span>
       <span class="mi">5512</span><span class="p">,</span> <span class="mi">5514</span><span class="p">,</span> <span class="mi">5525</span><span class="p">,</span> <span class="mi">5541</span><span class="p">,</span> <span class="mi">5560</span><span class="p">,</span> <span class="mi">5563</span><span class="p">,</span> <span class="mi">5583</span><span class="p">,</span> <span class="mi">5585</span><span class="p">,</span> <span class="mi">5604</span><span class="p">,</span> <span class="mi">5608</span><span class="p">,</span> <span class="mi">5612</span><span class="p">,</span>
       <span class="mi">5615</span><span class="p">,</span> <span class="mi">5622</span><span class="p">,</span> <span class="mi">5627</span><span class="p">,</span> <span class="mi">5641</span><span class="p">,</span> <span class="mi">5646</span><span class="p">,</span> <span class="mi">5662</span><span class="p">,</span> <span class="mi">5665</span><span class="p">,</span> <span class="mi">5674</span><span class="p">,</span> <span class="mi">5690</span><span class="p">,</span> <span class="mi">5692</span><span class="p">,</span> <span class="mi">5694</span><span class="p">,</span>
       <span class="mi">5697</span><span class="p">,</span> <span class="mi">5698</span><span class="p">,</span> <span class="mi">5711</span><span class="p">,</span> <span class="mi">5729</span><span class="p">,</span> <span class="mi">5730</span><span class="p">,</span> <span class="mi">5747</span><span class="p">,</span> <span class="mi">5767</span><span class="p">,</span> <span class="mi">5773</span><span class="p">,</span> <span class="mi">5801</span><span class="p">,</span> <span class="mi">5808</span><span class="p">,</span> <span class="mi">5809</span><span class="p">,</span>
       <span class="mi">5830</span><span class="p">,</span> <span class="mi">5844</span><span class="p">,</span> <span class="mi">5881</span><span class="p">,</span> <span class="mi">5883</span><span class="p">,</span> <span class="mi">5884</span><span class="p">,</span> <span class="mi">5886</span><span class="p">,</span> <span class="mi">5888</span><span class="p">,</span> <span class="mi">5905</span><span class="p">,</span> <span class="mi">5921</span><span class="p">,</span> <span class="mi">5927</span><span class="p">,</span> <span class="mi">5929</span><span class="p">,</span>
       <span class="mi">5932</span><span class="p">,</span> <span class="mi">5938</span><span class="p">,</span> <span class="mi">5961</span><span class="p">,</span> <span class="mi">5978</span><span class="p">,</span> <span class="mi">5981</span><span class="p">,</span> <span class="mi">6001</span><span class="p">,</span> <span class="mi">6008</span><span class="p">,</span> <span class="mi">6023</span><span class="p">,</span> <span class="mi">6034</span><span class="p">,</span> <span class="mi">6045</span><span class="p">,</span> <span class="mi">6065</span><span class="p">,</span>
       <span class="mi">6076</span><span class="p">,</span> <span class="mi">6094</span><span class="p">,</span> <span class="mi">6097</span><span class="p">,</span> <span class="mi">6103</span><span class="p">,</span> <span class="mi">6113</span><span class="p">,</span> <span class="mi">6117</span><span class="p">,</span> <span class="mi">6133</span><span class="p">,</span> <span class="mi">6137</span><span class="p">,</span> <span class="mi">6146</span><span class="p">,</span> <span class="mi">6179</span><span class="p">,</span> <span class="mi">6207</span><span class="p">,</span>
       <span class="mi">6227</span><span class="p">,</span> <span class="mi">6228</span><span class="p">,</span> <span class="mi">6245</span><span class="p">,</span> <span class="mi">6279</span><span class="p">,</span> <span class="mi">6292</span><span class="p">,</span> <span class="mi">6296</span><span class="p">,</span> <span class="mi">6302</span><span class="p">,</span> <span class="mi">6316</span><span class="p">,</span> <span class="mi">6320</span><span class="p">,</span> <span class="mi">6325</span><span class="p">,</span> <span class="mi">6326</span><span class="p">,</span>
       <span class="mi">6330</span><span class="p">,</span> <span class="mi">6331</span><span class="p">,</span> <span class="mi">6332</span><span class="p">,</span> <span class="mi">6337</span><span class="p">,</span> <span class="mi">6341</span><span class="p">,</span> <span class="mi">6357</span><span class="p">,</span> <span class="mi">6367</span><span class="p">,</span> <span class="mi">6394</span><span class="p">,</span> <span class="mi">6400</span><span class="p">,</span> <span class="mi">6406</span><span class="p">,</span> <span class="mi">6409</span><span class="p">,</span>
       <span class="mi">6421</span><span class="p">,</span> <span class="mi">6422</span><span class="p">,</span> <span class="mi">6441</span><span class="p">,</span> <span class="mi">6444</span><span class="p">,</span> <span class="mi">6446</span><span class="p">,</span> <span class="mi">6461</span><span class="p">,</span> <span class="mi">6468</span><span class="p">,</span> <span class="mi">6496</span><span class="p">,</span> <span class="mi">6502</span><span class="p">,</span> <span class="mi">6538</span><span class="p">,</span> <span class="mi">6591</span><span class="p">,</span>
       <span class="mi">6593</span><span class="p">,</span> <span class="mi">6603</span><span class="p">,</span> <span class="mi">6607</span><span class="p">,</span> <span class="mi">6611</span><span class="p">,</span> <span class="mi">6614</span><span class="p">,</span> <span class="mi">6615</span><span class="p">,</span> <span class="mi">6619</span><span class="p">,</span> <span class="mi">6629</span><span class="p">,</span> <span class="mi">6630</span><span class="p">,</span> <span class="mi">6645</span><span class="p">,</span> <span class="mi">6648</span><span class="p">,</span>
       <span class="mi">6663</span><span class="p">,</span> <span class="mi">6676</span><span class="p">,</span> <span class="mi">6681</span><span class="p">,</span> <span class="mi">6684</span><span class="p">,</span> <span class="mi">6700</span><span class="p">,</span> <span class="mi">6704</span><span class="p">,</span> <span class="mi">6708</span><span class="p">,</span> <span class="mi">6723</span><span class="p">,</span> <span class="mi">6730</span><span class="p">,</span> <span class="mi">6737</span><span class="p">,</span> <span class="mi">6738</span><span class="p">,</span>
       <span class="mi">6757</span><span class="p">,</span> <span class="mi">6758</span><span class="p">,</span> <span class="mi">6763</span><span class="p">,</span> <span class="mi">6768</span><span class="p">,</span> <span class="mi">6791</span><span class="p">,</span> <span class="mi">6802</span><span class="p">,</span> <span class="mi">6819</span><span class="p">,</span> <span class="mi">6823</span><span class="p">,</span> <span class="mi">6832</span><span class="p">,</span> <span class="mi">6847</span><span class="p">,</span> <span class="mi">6857</span><span class="p">,</span>
       <span class="mi">6875</span><span class="p">,</span> <span class="mi">6883</span><span class="p">,</span> <span class="mi">6917</span><span class="p">,</span> <span class="mi">6923</span><span class="p">,</span> <span class="mi">6928</span><span class="p">,</span> <span class="mi">6935</span><span class="p">,</span> <span class="mi">6958</span><span class="p">,</span> <span class="mi">6960</span><span class="p">,</span> <span class="mi">6967</span><span class="p">,</span> <span class="mi">6968</span><span class="p">,</span> <span class="mi">6970</span><span class="p">,</span>
       <span class="mi">6972</span><span class="p">,</span> <span class="mi">6973</span><span class="p">,</span> <span class="mi">6979</span><span class="p">,</span> <span class="mi">6987</span><span class="p">])}</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">dataidx_map</span>
<span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span>   <span class="mi">1</span><span class="p">,</span>   <span class="mi">21</span><span class="p">,</span>   <span class="mi">34</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">3510</span><span class="p">,</span> <span class="mi">3511</span><span class="p">,</span> <span class="mi">3519</span><span class="p">]),</span> <span class="mi">1</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">3622</span><span class="p">,</span> <span class="mi">3632</span><span class="p">,</span> <span class="mi">3661</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">6964</span><span class="p">,</span> <span class="mi">6969</span><span class="p">,</span> <span class="mi">6993</span><span class="p">]),</span> <span class="mi">2</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span> <span class="mi">6991</span><span class="p">,</span>  <span class="mi">7003</span><span class="p">,</span>  <span class="mi">7004</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">10576</span><span class="p">,</span> <span class="mi">10577</span><span class="p">,</span> <span class="mi">10588</span><span class="p">]),</span> <span class="mi">3</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">10276</span><span class="p">,</span> <span class="mi">1</span>
<span class="mi">0283</span><span class="p">,</span> <span class="mi">10323</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">13897</span><span class="p">,</span> <span class="mi">13898</span><span class="p">,</span> <span class="mi">13904</span><span class="p">]),</span> <span class="mi">4</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">13799</span><span class="p">,</span> <span class="mi">13807</span><span class="p">,</span> <span class="mi">13808</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">17317</span><span class="p">,</span> <span class="mi">17319</span><span class="p">,</span> <span class="mi">17323</span><span class="p">]),</span> <span class="mi">5</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">17360</span><span class="p">,</span> <span class="mi">17365</span><span class="p">,</span> <span class="mi">17376</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">20554</span><span class="p">,</span> <span class="mi">20562</span><span class="p">,</span> <span class="mi">20567</span><span class="p">]),</span> <span class="mi">6</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">20816</span><span class="p">,</span> <span class="mi">20827</span><span class="p">,</span> <span class="mi">2</span>
<span class="mi">0840</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">24185</span><span class="p">,</span> <span class="mi">24193</span><span class="p">,</span> <span class="mi">24194</span><span class="p">]),</span> <span class="mi">7</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">24419</span><span class="p">,</span> <span class="mi">24429</span><span class="p">,</span> <span class="mi">24440</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">27815</span><span class="p">,</span> <span class="mi">27817</span><span class="p">,</span> <span class="mi">27839</span><span class="p">]),</span> <span class="mi">8</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">28102</span><span class="p">,</span> <span class="mi">28105</span><span class="p">,</span> <span class="mi">28108</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">31115</span><span class="p">,</span> <span class="mi">31120</span><span class="p">,</span> <span class="mi">31128</span><span class="p">]),</span> <span class="mi">9</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">31425</span><span class="p">,</span> <span class="mi">31430</span><span class="p">,</span> <span class="mi">31445</span><span class="p">,</span> <span class="o">.</span>
<span class="o">..</span><span class="p">,</span> <span class="mi">34489</span><span class="p">,</span> <span class="mi">34491</span><span class="p">,</span> <span class="mi">34496</span><span class="p">]),</span> <span class="mi">10</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">35192</span><span class="p">,</span> <span class="mi">35209</span><span class="p">,</span> <span class="mi">35217</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">37867</span><span class="p">,</span> <span class="mi">37873</span><span class="p">,</span> <span class="mi">37877</span><span class="p">]),</span> <span class="mi">11</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">38729</span><span class="p">,</span> <span class="mi">38744</span><span class="p">,</span> <span class="mi">38746</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">41502</span><span class="p">,</span> <span class="mi">41503</span><span class="p">,</span> <span class="mi">41510</span><span class="p">]),</span> <span class="mi">12</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">41987</span><span class="p">,</span> <span class="mi">41989</span><span class="p">,</span> <span class="mi">41999</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span>
<span class="mi">44882</span><span class="p">,</span> <span class="mi">44884</span><span class="p">,</span> <span class="mi">44890</span><span class="p">]),</span> <span class="mi">13</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">45544</span><span class="p">,</span> <span class="mi">45557</span><span class="p">,</span> <span class="mi">45577</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">48616</span><span class="p">,</span> <span class="mi">48618</span><span class="p">,</span> <span class="mi">48633</span><span class="p">]),</span> <span class="mi">14</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">49065</span><span class="p">,</span> <span class="mi">49075</span><span class="p">,</span> <span class="mi">49085</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">52232</span><span class="p">,</span> <span class="mi">52236</span><span class="p">,</span> <span class="mi">52239</span><span class="p">]),</span> <span class="mi">15</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">52393</span><span class="p">,</span> <span class="mi">52396</span><span class="p">,</span> <span class="mi">52397</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">5585</span>
<span class="mi">9</span><span class="p">,</span> <span class="mi">55884</span><span class="p">,</span> <span class="mi">55889</span><span class="p">]),</span> <span class="mi">16</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">55890</span><span class="p">,</span> <span class="mi">55892</span><span class="p">,</span> <span class="mi">55907</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">59458</span><span class="p">,</span> <span class="mi">59464</span><span class="p">,</span> <span class="mi">59486</span><span class="p">]),</span> <span class="mi">17</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">59418</span><span class="p">,</span> <span class="mi">59420</span><span class="p">,</span> <span class="mi">59421</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">62983</span><span class="p">,</span> <span class="mi">63001</span><span class="p">,</span> <span class="mi">63005</span><span class="p">]),</span> <span class="mi">18</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">63210</span><span class="p">,</span> <span class="mi">63215</span><span class="p">,</span> <span class="mi">63219</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">66214</span><span class="p">,</span> <span class="mi">6</span>
<span class="mi">6221</span><span class="p">,</span> <span class="mi">66228</span><span class="p">]),</span> <span class="mi">19</span><span class="p">:</span> <span class="n">array</span><span class="p">([</span><span class="mi">66633</span><span class="p">,</span> <span class="mi">66643</span><span class="p">,</span> <span class="mi">66651</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">69945</span><span class="p">,</span> <span class="mi">69973</span><span class="p">,</span> <span class="mi">69992</span><span class="p">])}</span>
<span class="c1"># 划分好之后</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">dataidx_map</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span>   <span class="mi">1</span><span class="p">,</span>   <span class="mi">21</span><span class="p">,</span>   <span class="mi">34</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">3510</span><span class="p">,</span> <span class="mi">3511</span><span class="p">,</span> <span class="mi">3519</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataidx_map</span><span class="p">[</span><span class="n">client</span><span class="p">])</span>
<span class="mi">3495</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y</span><span class="p">[</span><span class="n">client</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int64</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y</span><span class="p">[</span><span class="n">client</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">3495</span><span class="p">,)</span>

<span class="c1"># 从给定的数组中提取出唯一元素，对提取的唯一元素进行排序</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">client</span><span class="p">])</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int64</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">statistic</span><span class="p">[</span><span class="n">client</span><span class="p">]</span>
<span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">)]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">statistic</span>
<span class="p">[[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">)],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">statistic</span><span class="p">[</span><span class="n">client</span><span class="p">]</span>
<span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">348</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">410</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">359</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">358</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">328</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">359</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">377</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">346</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">365</span><span class="p">)]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">statistic</span><span class="p">[</span><span class="n">client</span><span class="p">])</span>
<span class="mi">10</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">statistic</span><span class="p">)</span>
<span class="mi">20</span>

<span class="c1"># 划分训练集和测试集</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="mi">20</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="mi">3495</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="mi">2621</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="mi">874</span>
<span class="c1"># 划分之后 train_data[0]是一个字典，包含两个键值对</span>
<span class="c1"># &#39;x&#39;为client0对应的数据，&#39;y&#39;为标签</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="mi">20</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="mi">2</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">type</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">dict</span><span class="s1">&#39;&gt;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">2621</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int64</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">2621</span><span class="p">,)</span>

<span class="c1"># 划分后每个客户端训练集和测试集的数量</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2696</span><span class="p">],</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span>
<span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">899</span><span class="p">]}</span>

</code></pre></td></tr></table>
</div>
</div><p><code>运行</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">rm ../dataset/mnist/config.json
cd ../dataset/
nohup python -u generate_mnist.py noniid - dir &gt; mnist_dataset.out 2&gt;&amp;1
</code></pre></td></tr></table>
</div>
</div><p>这是一个后台运行Python脚本的命令，使用了**<code>nohup</code>**命令来让命令在后台持续运行，同时将输出重定向到mnist_dataset.out文件中。**<code>-u</code>**选项在Python命令行中表示使用无缓冲的输出。它的作用是强制Python在标准输出和标准错误流中不进行缓冲，而是立即输出到终端。这对于实时查看脚本的输出或日志很有用，尤其是在脚本需要长时间运行或需要实时监控输出时。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">python</span> <span class="n">generate_mnist</span><span class="o">.</span><span class="n">py</span> <span class="n">iid</span> <span class="n">balance</span> <span class="o">-</span> <span class="c1"># for iid and balanced scenario</span>

<span class="n">Client</span> <span class="mi">0</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">1</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">2</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">3</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">4</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">5</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">6</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">7</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">8</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">9</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">10</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">11</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">12</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">13</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">14</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">15</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">16</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">17</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">18</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3495</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">393</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">349</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">357</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">315</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">343</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">364</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">341</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">347</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">19</span>	 <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">3595</span>	 <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
		 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">348</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">410</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">359</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">358</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">345</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">328</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">359</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">377</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">346</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">365</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>

<span class="n">Total</span> <span class="n">number</span> <span class="n">of</span> <span class="n">samples</span><span class="p">:</span> <span class="mi">70000</span>
<span class="n">The</span> <span class="n">number</span> <span class="n">of</span> <span class="n">train</span> <span class="n">samples</span><span class="p">:</span> <span class="p">[</span><span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2621</span><span class="p">,</span> <span class="mi">2696</span><span class="p">]</span>
<span class="n">The</span> <span class="n">number</span> <span class="n">of</span> <span class="n">test</span> <span class="n">samples</span><span class="p">:</span> <span class="p">[</span><span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">874</span><span class="p">,</span> <span class="mi">899</span><span class="p">]</span>

<span class="n">Saving</span> <span class="n">to</span> <span class="n">disk</span><span class="o">.</span>

<span class="n">Finish</span> <span class="n">generating</span> <span class="n">dataset</span><span class="o">.</span>

<span class="c1"># IID 不平衡python generate_mnist.py iid - -</span>
<span class="c1"># 对于每个i，每个client分到的样本数会重新划分</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">71</span><span class="p">,</span> <span class="mi">269</span><span class="p">,</span> <span class="mi">106</span><span class="p">,</span> <span class="mi">289</span><span class="p">,</span> <span class="mi">237</span><span class="p">,</span> <span class="mi">167</span><span class="p">,</span> <span class="mi">178</span><span class="p">,</span> <span class="mi">163</span><span class="p">,</span> <span class="mi">105</span><span class="p">,</span> <span class="mi">271</span><span class="p">,</span> <span class="mi">315</span><span class="p">,</span> <span class="mi">212</span><span class="p">,</span> <span class="mi">310</span><span class="p">,</span> <span class="mi">288</span><span class="p">,</span> <span class="mi">286</span><span class="p">,</span> <span class="mi">190</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">249</span><span class="p">,</span> <span class="mi">3011</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">280</span><span class="p">,</span> <span class="mi">391</span><span class="p">,</span> <span class="mi">125</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">358</span><span class="p">,</span> <span class="mi">356</span><span class="p">,</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">352</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">355</span><span class="p">,</span> <span class="mi">248</span><span class="p">,</span> <span class="mi">303</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">154</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">3854</span><span class="p">]</span>

<span class="n">Client</span> <span class="mi">0</span>         <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">1882</span>      <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
                 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">71</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">280</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">83</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">300</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">336</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">67</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">137</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">253</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">111</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">244</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">1</span>         <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">2922</span>      <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
                 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">269</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">391</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">347</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">230</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">250</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">294</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">300</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">267</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">253</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">321</span><span class="p">)]</span>
<span class="o">--------------------------------------------------</span>
<span class="n">Client</span> <span class="mi">2</span>         <span class="n">Size</span> <span class="n">of</span> <span class="n">data</span><span class="p">:</span> <span class="mi">1323</span>      <span class="n">Labels</span><span class="p">:</span>  <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span><span class="p">]</span>
                 <span class="n">Samples</span> <span class="n">of</span> <span class="n">labels</span><span class="p">:</span>  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">106</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">125</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">37</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">161</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">185</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">88</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">216</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">132</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">211</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">62</span><span class="p">)]</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="client处理">client处理</h3>
<blockquote>
<p>learning_rate_decay：收敛性分析的时候要求学习率必须衰减，不衰减的话就没办法收敛，理论上是这样，实际上不衰减也能收敛，能达到一个比较高的准确率，北大的 On the Convergence of FedAvg on Non-IID Data，<a href="https://zhuanlan.zhihu.com/p/640517716">blog</a></p>
<p>local epochs：每次聚合的时候本地训练次数</p>
<p>每个本地 epoch 内执行多通常情况下，在每个本地 epoch 中，模型会根据本地训练数据进行一次参数更新，然后进行一次模型评估。然而，当采用多个更新步骤时，模型将在同一本地 epoch 内执行多次参数更新，而不是仅执行一次。这种方法可能会提高模型的收敛速度，特别是在使用大型模型和数据集进行训练时。然而，需要注意的是，多次更新步骤可能会增加过拟合的风险，因此需要仔细权衡。个参数更新步骤。</p>
<p>join_ratio：客户端每轮参加的比例，用于衡量client dift的程度</p>
<p>eval_gap：几轮评估test一次模型</p>
<p><strong><code>copy.deepcopy</code></strong> 是用于创建对象的深度拷贝，这意味着它会递归地复制对象及其包含的所有对象，而不仅仅是复制对象本身。这样做可以确保 <strong><code>self.model</code></strong> 是 <strong><code>args.model</code></strong> 的完整和独立的副本，而不是对原始对象的引用。</p>
<p><strong><code>nn.BatchNorm2d</code></strong> 批标准化被用于加速神经网络的训练，并且有助于处理梯度消失/爆炸问题。用于在卷积神经网络的卷积层后应用批标准化。它通过规范化每个输入通道的输出，然后应用缩放和偏移，以使模型更容易训练并提高泛化能力。在使用深度学习框架构建卷积神经网络时，批标准化通常被认为是一种标准的正则化技术，并且已被证明在许多情况下能够提高模型的性能和训练速度。</p>
<hr>
<p>准确率和<a href="https://zhuanlan.zhihu.com/p/569006692">AUC</a>是两个不同的指标，用于评估分类模型的性能，它们分别从整体准确性和类别排序能力的角度来衡量模型的表现。ROC 曲线下面积（ROC AUC）是一种用于衡量分类模型性能的指标，它表示分类模型在不同阈值下真正例率（True Positive Rate）与假正例率（False Positive Rate）之间的权衡。ROC 曲线是以真正例率为纵轴，假正例率为横轴所绘制的曲线，ROC AUC 则是 ROC 曲线下方的面积，取值范围在 0 到 1 之间。ROC AUC 值越接近 1，表示模型性能越好；越接近 0.5，表示模型性能越一般；小于 0.5，则表示模型性能不如随机猜测。ROC 曲线和 ROC AUC 可以帮助我们评估分类模型在不同阈值下的整体性能和稳定性。</p>
<p>AUC（Area Under the Curve）和准确率是两种不同的性能评估指标：</p>
<ol>
<li>AUC（Area Under the Curve）：用于衡量分类模型在不同阈值下真正例率（True Positive Rate）与假正例率（False Positive Rate）之间的权衡。ROC 曲线下面积（ROC AUC）是对整个 ROC 曲线的一个总体性能指标，表示分类模型对正负样本的排序能力，范围在 0 到 1 之间。</li>
<li>准确率（Accuracy）：表示模型在所有预测样本中正确分类的比例，是最常见的分类模型性能指标之一。但准确率不能很好地处理样本不均衡的情况，当正负样本比例严重失衡时，准确率并不是一个很好的评价指标。</li>
</ol>
<p>总的来说，AUC 是评估分类模型排序能力的指标，而准确率是评估模型分类能力的指标。 AUC 更适用于样本不均衡的情况，而准确率则更适用于样本均衡的情况。</p>
<ul>
<li><strong>Epoch（周期）</strong> 是指整个训练数据集被送入神经网络进行了一次正向传播和反向传播的过程。一个 epoch 表示神经网络已经学习完整个训练数据集的过程。</li>
<li><strong>Batch（批次）</strong> 是指将整个训练数据集分成若干批次，每个批次包含若干个样本。在每个批次中，模型根据批次内的样本进行一次正向传播和反向传播，然后更新参数。</li>
<li>在使用时，可以将整个训练数据集划分为多个批次，每个批次包含固定数量的样本。在每个 epoch 中，神经网络会依次处理每个批次的数据，并进行参数更新。这样可以加速训练过程，减少内存占用，并且有助于模型的收敛。</li>
</ul>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># ClientAVG继承了client类</span>
<span class="c1"># 定义了clientAVG，它是 `Client` 类的子类。</span>
<span class="c1"># 该类具有一个构造函数 `__init__`，它接受 `args`、`id`、`train_samples`、`test_samples` 等参数</span>
<span class="c1"># 并通过调用 `super()` 函数来调用父类 `Client` 的构造函数，并将参数传递给父类构造函数。</span>
<span class="kn">from</span> <span class="nn">flcore.clients.clientbase</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="k">class</span> <span class="nc">clientAVG</span><span class="p">(</span><span class="n">Client</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">train_samples</span><span class="p">,</span> <span class="n">test_samples</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 调用父类的构造函数 </span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">train_samples</span><span class="p">,</span> <span class="n">test_samples</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">argparse</span>  
  
<span class="c1"># 创建 ArgumentParser 对象  </span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Process some integers.&#39;</span><span class="p">)</span>  
  
<span class="c1"># 添加命令行参数  </span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;integers&#39;</span><span class="p">,</span> <span class="n">metavar</span><span class="o">=</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">nargs</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;an integer for the accumulator&#39;</span><span class="p">)</span>  
  
<span class="c1"># 解析命令行参数  </span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>  
  
<span class="c1"># 访问解析后的参数  </span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">integers</span><span class="p">)</span>  

<span class="c1"># 添加参数直接add，允许用户在命令行中指定程序的行为，而不需要修改代码。</span>
<span class="c1"># 不用在函数中添加参数，再修改调用的代码</span>
<span class="c1"># 管理超参数的一种方式</span>
</code></pre></td></tr></table>
</div>
</div><hr>
<p>client端先通过DataLoader分好batch，用batch的每个样本做梯度下降</p>
<h4 id="clientavgpy">clientavg.py</h4>
<p>一个epoch把所有训练集跑一遍，一个batch是一个batchsize的数据</p>
<p>client.train()</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># client0 训练</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="mi">141</span>
<span class="c1"># 本地训练的epoch</span>
<span class="n">max_local_epochs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_epochs</span><span class="o">=</span><span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span><span class="c1">#按batch进行</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">loss</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">2.3381</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">NllLossBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_time_cost</span><span class="c1"># 每个client独立计算</span>
<span class="p">{</span><span class="s1">&#39;num_rounds&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;total_cost&#39;</span><span class="p">:</span> <span class="mf">657.0169236660004</span><span class="p">}</span>
<span class="c1"># 每个client单独记录，不会被清空</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">selected_clients</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">train_time_cost</span>
<span class="p">{</span><span class="s1">&#39;num_rounds&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;total_cost&#39;</span><span class="p">:</span> <span class="mf">657.0169236660004</span><span class="p">}</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">selected_clients</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">train_time_cost</span>
<span class="p">{</span><span class="s1">&#39;num_rounds&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;total_cost&#39;</span><span class="p">:</span> <span class="mf">26.226897716522217</span><span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="server处理">server处理</h3>
<h4 id="mainpy">main.py</h4>
<p>初始化超参数，生成模型和选择算法，使用 FedAvg 类来执行 FedAvg 算法，设置client(数据集和实例化client)，server.train()服务器端训练，计算测试结果</p>
<p>for i in range(args.prev, args.times):# (0,1)&ndash;实验运行的次数，打印信息用的，并不是模型聚合的次数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">	<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&#34;FedAvg&#34;</span><span class="p">:</span>
        	<span class="c1"># 备份模型的全连接层 (fc) </span>
            <span class="n">args</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="p">)</span>
            <span class="c1"># 将模型的全连接层 (fc) 替换为 nn.Identity()，</span>
            <span class="c1"># 这样做的目的是为了在聚合时只考虑模型的特征提取部分，而不包括全连接层的参数</span>
            
            <span class="c1"># 用于创建一个恒等映射(identity mapping)的模块。</span>
            <span class="c1">#在神经网络中，它通常被用作一个占位符，不对输入进行任何变换，直接将输入作为输出返回。</span>
            <span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span> 
        
            <span class="c1"># 将模型和备份的全连接层重新组合，这可能是为了在聚合后重新还原模型结构。</span>
            <span class="n">args</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">BaseHeadSplit</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">head</span><span class="p">)</span>
            
            <span class="c1"># FedAvg 类来执行 FedAvg 算法</span>
            <span class="c1"># 传入参数 args 和 i，进行模型参数的聚合和更新。</span>
            <span class="n">server</span> <span class="o">=</span> <span class="n">FedAvg</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
            
<span class="c1"># split an original model into a base and a head</span>
<span class="c1"># 可以灵活地将原始模型拆分为基础部分和头部部分，</span>
<span class="c1"># 以便在需要的时候进行个性化定制或模型结构的重新组合。</span>
<span class="k">class</span> <span class="nc">BaseHeadSplit</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">head</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BaseHeadSplit</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="o">=</span> <span class="n">base</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">head</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>         
<span class="c1"># 备份全连接层的作用是为了在聚合过程中保留原始模型的全连接层参数。通过备份全连接层，可以确保在模型参数聚合后，还能够恢复原始的全连接层结构和参数，以便保持模型的完整性和性能。</span>

<span class="c1"># 在代码中，备份全连接层的操作使得在模型参数聚合过程中，只聚合模型的特征提取部分，而不包括全连接层的参数。这有助于保持模型结构的一致性，并且在分布式学习中更容易实现模型参数的共享和聚合。</span>

<span class="c1"># 在联邦学习中，是否需要聚合全连接层取决于具体的联邦学习任务和模型架构。一般来说，如果在联邦学习中全连接层的参数在不同设备上是相同的，那么可能不需要聚合全连接层；如果全连接层的参数需要在不同设备上进行调整，那么可能需要聚合全连接层。</span>

<span class="c1"># 全连接层（Fully Connected Layer）通常位于神经网络的末尾，负责将前面的特征表示映射到最终的输出。在分类任务中，全连接层通常将特征映射到不同类别的得分或概率；在其他任务中，全连接层也扮演着类似的作用，将中间特征映射到最终输出。</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">args</span><span class="o">.</span><span class="n">head</span>
<span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span>
<span class="n">FedAvgCNN</span><span class="p">(</span>
  <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
<span class="p">)</span>
           
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span>
<span class="n">BaseHeadSplit</span><span class="p">(</span>
  <span class="p">(</span><span class="n">base</span><span class="p">):</span> <span class="n">FedAvgCNN</span><span class="p">(</span>
    <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Identity</span><span class="p">()</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">head</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 使用 FedAvg 类来执行 FedAvg 算法</span>
<span class="n">server</span> <span class="o">=</span> <span class="n">FedAvg</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
<span class="c1"># 设置client</span>
<span class="bp">self</span><span class="o">.</span><span class="n">set_clients</span><span class="p">(</span><span class="n">clientAVG</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_clients</span>
<span class="mi">2</span>

<span class="c1"># 读取数据 mnist，0(client_id),train</span>
<span class="k">def</span> <span class="nf">read_client_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_file</span>
<span class="s1">&#39;../dataset</span><span class="se">\\</span><span class="s1">mnist</span><span class="se">\\</span><span class="s1">train/0.npz&#39;</span>
<span class="c1"># train_data读取数据</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">1411</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">type</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="s1">&#39;&gt;</span>
<span class="c1"># 转为tensor张量</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1411</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">type</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="s1">&#39;&gt;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1411</span><span class="p">])</span>

<span class="c1"># 打包为元组的列表</span>
<span class="c1"># 1411个二元组组成的列表</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">type</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">list</span><span class="s1">&#39;&gt;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="mi">1411</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">type</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">tuple</span><span class="s1">&#39;&gt;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span>  <span class="n">p</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="n">tensor</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># 训练集和测试集 client0 1882</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="mi">1411</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="mi">471</span>
<span class="c1"># client1 2922</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="mi">2191</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="mi">731</span>

<span class="c1"># 实例化client类</span>
<span class="bp">self</span><span class="o">.</span><span class="n">set_clients</span><span class="p">(</span><span class="n">clientAVG</span><span class="p">)</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">clientObj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> 
                            <span class="nb">id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> 
                            <span class="n">train_samples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> 
                            <span class="n">test_samples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">),</span> 
                            <span class="n">train_slow</span><span class="o">=</span><span class="n">train_slow</span><span class="p">,</span> 
                            <span class="n">send_slow</span><span class="o">=</span><span class="n">send_slow</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">client</span>
<span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientavg</span><span class="o">.</span><span class="n">clientAVG</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x0000022DCD9EAC08</span><span class="o">&gt;</span>
</code></pre></td></tr></table>
</div>
</div><h4 id="serveravgpy">serveravg.py</h4>
<p>server.train()：</p>
<p>for i in range(self.global_rounds+1)#2000+1&ndash;模型聚合的次数2000次</p>
<p>下发全局模型(server-&gt;client)，评估全局模型(acc,auc,loss)，每个client进行训练(client.train()),接收client模型，聚合模型，保存结果，保存全局模型</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">selected_clients</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientavg</span><span class="o">.</span><span class="n">clientAVG</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x0000022DC7683948</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientavg</span><span class="o">.</span><span class="n">clientAVG</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x0000022DCD9EAC08</span><span class="o">&gt;</span><span class="p">]</span>

<span class="bp">self</span><span class="o">.</span><span class="n">global_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># server-&gt;client下发模型</span>
<span class="bp">self</span><span class="o">.</span><span class="n">send_models</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">global_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">client</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_model</span><span class="p">)</span><span class="c1"># 全局模型set到cliet模型上</span>
<span class="c1"># new_param 是 model 的参数。在 PyTorch 中，模型的参数通常是指模型中的权重和偏置等可学习的参数。这些参数以张量（tensor）的形式存在，用于表示神经网络中不同层之间的连接权重和偏置。new_param 是一个模型的参数张量，其格式可以是任意形状的张量，取决于具体模型的结构和层的数量。</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">new_param</span>
<span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[[[</span><span class="o">-</span><span class="mf">1.4974e-03</span><span class="p">,</span>  <span class="mf">1.0729e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6461e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4719e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.7031e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">5.3631e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9626e-03</span><span class="p">,</span>  <span class="mf">1.5858e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7749e-02</span><span class="p">,</span>  <span class="mf">5.2923e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">6.0443e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9313e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9107e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3246e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.2445e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">7.4087e-03</span><span class="p">,</span>  <span class="mf">7.9067e-02</span><span class="p">,</span>  <span class="mf">1.2000e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3559e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.7093e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">7.2643e-02</span><span class="p">,</span>  <span class="mf">1.6608e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1160e-02</span><span class="p">,</span>  <span class="mf">1.4966e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2237e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">2.1163e-02</span><span class="p">,</span>  <span class="mf">1.8110e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8553e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2591e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0633e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">7.7960e-02</span><span class="p">,</span>  <span class="mf">1.7280e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2964e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.2067e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3973e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8731e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1675e-01</span><span class="p">,</span>  <span class="mf">1.7192e-01</span><span class="p">,</span>  <span class="mf">8.9244e-02</span><span class="p">,</span>  <span class="mf">9.6935e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.0518e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0254e-01</span><span class="p">,</span>  <span class="mf">3.3837e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8674e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4451e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.0311e-01</span><span class="p">,</span>  <span class="mf">1.2619e-01</span><span class="p">,</span>  <span class="mf">1.1726e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.8699e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.2165e-03</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.2791e-01</span><span class="p">,</span>  <span class="mf">1.9883e-01</span><span class="p">,</span>  <span class="mf">7.9376e-02</span><span class="p">,</span>  <span class="mf">2.7019e-02</span><span class="p">,</span>  <span class="mf">1.3410e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1776e-01</span><span class="p">,</span>  <span class="mf">3.7269e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5506e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3862e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0332e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">9.0495e-02</span><span class="p">,</span>  <span class="mf">8.0432e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1847e-01</span><span class="p">,</span>  <span class="mf">6.0421e-02</span><span class="p">,</span>  <span class="mf">1.0979e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">2.5243e-02</span><span class="p">,</span>  <span class="mf">7.6363e-03</span><span class="p">,</span>  <span class="mf">4.6341e-02</span><span class="p">,</span>  <span class="mf">1.2408e-01</span><span class="p">,</span>  <span class="mf">1.9204e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.5412e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.3294e-02</span><span class="p">,</span>  <span class="mf">7.8602e-02</span><span class="p">,</span>  <span class="mf">1.6571e-01</span><span class="p">,</span>  <span class="mf">1.7404e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.7647e-01</span><span class="p">,</span>  <span class="mf">3.9803e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7392e-01</span><span class="p">,</span>  <span class="mf">1.8398e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2512e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8639e-01</span><span class="p">,</span>  <span class="mf">1.7770e-01</span><span class="p">,</span>  <span class="mf">1.5207e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9951e-01</span><span class="p">,</span>  <span class="mf">3.7434e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">3.3692e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2912e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.1551e-02</span><span class="p">,</span>  <span class="mf">7.6911e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1846e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">7.3318e-02</span><span class="p">,</span>  <span class="mf">1.0114e-01</span><span class="p">,</span>  <span class="mf">1.4317e-01</span><span class="p">,</span>  <span class="mf">7.4782e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9795e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2974e-01</span><span class="p">,</span>  <span class="mf">9.9863e-02</span><span class="p">,</span>  <span class="mf">4.1860e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5602e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1516e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.8815e-01</span><span class="p">,</span>  <span class="mf">1.3476e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.7205e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0337e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9052e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">3.5948e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5061e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5427e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1020e-02</span><span class="p">,</span>  <span class="mf">3.0029e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">8.1906e-02</span><span class="p">,</span>  <span class="mf">1.1868e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2171e-01</span><span class="p">,</span>  <span class="mf">1.8147e-01</span><span class="p">,</span>  <span class="mf">1.3706e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.6866e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.9777e-02</span><span class="p">,</span>  <span class="mf">9.0245e-03</span><span class="p">,</span>  <span class="mf">2.9180e-02</span><span class="p">,</span>  <span class="mf">4.7435e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">7.8486e-02</span><span class="p">,</span>  <span class="mf">1.1980e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7586e-02</span><span class="p">,</span>  <span class="mf">9.4638e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9185e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.1854e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0066e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7423e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9967e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6392e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">4.2543e-02</span><span class="p">,</span>  <span class="mf">4.2751e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3029e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0264e-02</span><span class="p">,</span>  <span class="mf">1.4317e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">2.0560e-02</span><span class="p">,</span>  <span class="mf">5.5584e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7254e-02</span><span class="p">,</span>  <span class="mf">4.0476e-02</span><span class="p">,</span>  <span class="mf">1.2717e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8945e-01</span><span class="p">,</span>  <span class="mf">1.2701e-01</span><span class="p">,</span>  <span class="mf">1.8988e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4464e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7966e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">9.4815e-02</span><span class="p">,</span>  <span class="mf">1.3618e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2965e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.9409e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5326e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.8717e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6880e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0567e-02</span><span class="p">,</span>  <span class="mf">1.0968e-01</span><span class="p">,</span>  <span class="mf">1.0813e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.9289e-01</span><span class="p">,</span>  <span class="mf">1.2476e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5650e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2282e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.1095e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">3.8523e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9269e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7947e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7269e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1296e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.5864e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.0855e-02</span><span class="p">,</span>  <span class="mf">7.5340e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8001e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3497e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7588e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.1578e-02</span><span class="p">,</span>  <span class="mf">1.8060e-01</span><span class="p">,</span>  <span class="mf">7.2431e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8049e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.2654e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3079e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.9281e-02</span><span class="p">,</span>  <span class="mf">1.5993e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6162e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.1461e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1874e-02</span><span class="p">,</span>  <span class="mf">1.4282e-01</span><span class="p">,</span>  <span class="mf">5.5829e-02</span><span class="p">,</span>  <span class="mf">9.6101e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">7.0632e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8095e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2061e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6482e-01</span><span class="p">,</span>  <span class="mf">1.0837e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.5880e-01</span><span class="p">,</span>  <span class="mf">1.3684e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4108e-01</span><span class="p">,</span>  <span class="mf">8.9200e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4099e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1010e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1654e-01</span><span class="p">,</span>  <span class="mf">6.8349e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1918e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.3634e-03</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">8.4136e-03</span><span class="p">,</span>  <span class="mf">1.2892e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5118e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3730e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1613e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.3999e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1893e-02</span><span class="p">,</span>  <span class="mf">1.6870e-01</span><span class="p">,</span>  <span class="mf">7.2322e-02</span><span class="p">,</span>  <span class="mf">2.5325e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.4888e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.9536e-02</span><span class="p">,</span>  <span class="mf">2.5093e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5669e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4053e-03</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.5519e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5644e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0483e-01</span><span class="p">,</span>  <span class="mf">1.6150e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6231e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.4361e-02</span><span class="p">,</span>  <span class="mf">1.9785e-01</span><span class="p">,</span>  <span class="mf">7.2247e-02</span><span class="p">,</span>  <span class="mf">5.6626e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7332e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">9.9076e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4246e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6773e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7103e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9617e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.1877e-03</span><span class="p">,</span>  <span class="mf">1.6496e-01</span><span class="p">,</span>  <span class="mf">2.4968e-02</span><span class="p">,</span>  <span class="mf">1.7914e-01</span><span class="p">,</span>  <span class="mf">1.2234e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2644e-01</span><span class="p">,</span>  <span class="mf">8.9701e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4138e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.4765e-02</span><span class="p">,</span>  <span class="mf">5.8825e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">6.6038e-02</span><span class="p">,</span>  <span class="mf">1.5005e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.4383e-02</span><span class="p">,</span>  <span class="mf">3.2017e-04</span><span class="p">,</span>  <span class="mf">1.0296e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.9342e-01</span><span class="p">,</span>  <span class="mf">1.4460e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6538e-01</span><span class="p">,</span>  <span class="mf">2.7565e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.4003e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.0533e-01</span><span class="p">,</span>  <span class="mf">2.6434e-02</span><span class="p">,</span>  <span class="mf">1.6538e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.8464e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1874e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">7.3968e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9823e-01</span><span class="p">,</span>  <span class="mf">9.0279e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.6053e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3347e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1522e-01</span><span class="p">,</span>  <span class="mf">1.1499e-01</span><span class="p">,</span>  <span class="mf">1.0591e-01</span><span class="p">,</span>  <span class="mf">1.5350e-01</span><span class="p">,</span>  <span class="mf">7.2545e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">6.6792e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.5890e-02</span><span class="p">,</span>  <span class="mf">5.9086e-02</span><span class="p">,</span>  <span class="mf">1.6441e-01</span><span class="p">,</span>  <span class="mf">5.4378e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">9.4629e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.4015e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8909e-01</span><span class="p">,</span>  <span class="mf">4.3210e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1224e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.7832e-01</span><span class="p">,</span>  <span class="mf">1.7538e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2988e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2752e-02</span><span class="p">,</span>  <span class="mf">5.7299e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">6.3717e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3458e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6166e-01</span><span class="p">,</span>  <span class="mf">1.5942e-01</span><span class="p">,</span>  <span class="mf">3.2568e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.6592e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7048e-02</span><span class="p">,</span>  <span class="mf">5.8911e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5735e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.8942e-03</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2181e-01</span><span class="p">,</span>  <span class="mf">6.7640e-02</span><span class="p">,</span>  <span class="mf">6.3233e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.1257e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.4980e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2329e-01</span><span class="p">,</span>  <span class="mf">1.3831e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4888e-01</span><span class="p">,</span>  <span class="mf">8.1933e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7251e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">9.6494e-02</span><span class="p">,</span>  <span class="mf">3.5929e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0389e-01</span><span class="p">,</span>  <span class="mf">4.6081e-02</span><span class="p">,</span>  <span class="mf">3.9276e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.4850e-01</span><span class="p">,</span>  <span class="mf">3.3300e-02</span><span class="p">,</span>  <span class="mf">8.5186e-02</span><span class="p">,</span>  <span class="mf">7.9166e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5176e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.6396e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.0831e-02</span><span class="p">,</span>  <span class="mf">6.9462e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.2975e-02</span><span class="p">,</span>  <span class="mf">7.5916e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.3319e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0443e-01</span><span class="p">,</span>  <span class="mf">1.9724e-03</span><span class="p">,</span>  <span class="mf">8.2700e-02</span><span class="p">,</span>  <span class="mf">1.5676e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.6706e-02</span><span class="p">,</span>  <span class="mf">2.4973e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5723e-01</span><span class="p">,</span>  <span class="mf">1.5718e-02</span><span class="p">,</span>  <span class="mf">1.3849e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.8022e-01</span><span class="p">,</span>  <span class="mf">1.1755e-01</span><span class="p">,</span>  <span class="mf">2.6801e-02</span><span class="p">,</span>  <span class="mf">9.3403e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7296e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.6574e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7198e-01</span><span class="p">,</span>  <span class="mf">1.9952e-01</span><span class="p">,</span>  <span class="mf">1.2696e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3825e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">7.8249e-02</span><span class="p">,</span>  <span class="mf">1.5103e-01</span><span class="p">,</span>  <span class="mf">1.9992e-01</span><span class="p">,</span>  <span class="mf">1.7488e-01</span><span class="p">,</span>  <span class="mf">1.5495e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">4.5852e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.0188e-02</span><span class="p">,</span>  <span class="mf">1.6421e-01</span><span class="p">,</span>  <span class="mf">1.1208e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2036e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7980e-01</span><span class="p">,</span>  <span class="mf">9.6632e-02</span><span class="p">,</span>  <span class="mf">1.0903e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2536e-01</span><span class="p">,</span>  <span class="mf">5.7380e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">7.0112e-02</span><span class="p">,</span>  <span class="mf">1.5627e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5988e-02</span><span class="p">,</span>  <span class="mf">7.7863e-02</span><span class="p">,</span>  <span class="mf">3.5524e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">8.5094e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7966e-02</span><span class="p">,</span>  <span class="mf">9.7514e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3969e-01</span><span class="p">,</span>  <span class="mf">4.5171e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.3532e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9731e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6061e-01</span><span class="p">,</span>  <span class="mf">1.5789e-01</span><span class="p">,</span>  <span class="mf">1.0821e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8764e-01</span><span class="p">,</span>  <span class="mf">1.6022e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7861e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3649e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2317e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.2989e-01</span><span class="p">,</span>  <span class="mf">1.3888e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5120e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7580e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9322e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.1355e-01</span><span class="p">,</span>  <span class="mf">1.6450e-01</span><span class="p">,</span>  <span class="mf">1.6375e-01</span><span class="p">,</span>  <span class="mf">1.4317e-01</span><span class="p">,</span>  <span class="mf">1.5443e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7784e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.1213e-02</span><span class="p">,</span>  <span class="mf">8.8000e-02</span><span class="p">,</span>  <span class="mf">1.7819e-01</span><span class="p">,</span>  <span class="mf">6.6164e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.9994e-01</span><span class="p">,</span>  <span class="mf">1.0373e-01</span><span class="p">,</span>  <span class="mf">1.2433e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.9998e-02</span><span class="p">,</span>  <span class="mf">9.5966e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">2.2981e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.7763e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1274e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1222e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5390e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.3427e-01</span><span class="p">,</span>  <span class="mf">1.4219e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2763e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1574e-01</span><span class="p">,</span>  <span class="mf">1.5458e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.2790e-01</span><span class="p">,</span>  <span class="mf">1.4867e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.4427e-02</span><span class="p">,</span>  <span class="mf">1.8381e-01</span><span class="p">,</span>  <span class="mf">8.1788e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.5183e-01</span><span class="p">,</span>  <span class="mf">1.9142e-01</span><span class="p">,</span>  <span class="mf">1.5187e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.2898e-02</span><span class="p">,</span>  <span class="mf">1.1243e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1364e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1343e-02</span><span class="p">,</span>  <span class="mf">1.6982e-01</span><span class="p">,</span>  <span class="mf">8.2638e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4144e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">6.6847e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4289e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.8586e-02</span><span class="p">,</span>  <span class="mf">1.9140e-02</span><span class="p">,</span>  <span class="mf">1.8497e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.0708e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2349e-01</span><span class="p">,</span>  <span class="mf">1.0251e-02</span><span class="p">,</span>  <span class="mf">9.5897e-02</span><span class="p">,</span>  <span class="mf">9.9208e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.8279e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5788e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4863e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.5337e-02</span><span class="p">,</span>  <span class="mf">7.2059e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.4203e-01</span><span class="p">,</span>  <span class="mf">7.4345e-02</span><span class="p">,</span>  <span class="mf">1.6976e-01</span><span class="p">,</span>  <span class="mf">1.3118e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3330e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">7.1657e-02</span><span class="p">,</span>  <span class="mf">4.3673e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5246e-01</span><span class="p">,</span>  <span class="mf">9.9362e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8157e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.9226e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9433e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0573e-02</span><span class="p">,</span>  <span class="mf">1.3449e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8930e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.6624e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0005e-02</span><span class="p">,</span>  <span class="mf">5.8577e-02</span><span class="p">,</span>  <span class="mf">9.1206e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8034e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.6587e-01</span><span class="p">,</span>  <span class="mf">1.0769e-01</span><span class="p">,</span>  <span class="mf">1.9880e-01</span><span class="p">,</span>  <span class="mf">1.0104e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3201e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.6692e-01</span><span class="p">,</span>  <span class="mf">1.0749e-02</span><span class="p">,</span>  <span class="mf">9.4843e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6037e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.7525e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.9638e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.7899e-02</span><span class="p">,</span>  <span class="mf">4.3146e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5703e-01</span><span class="p">,</span>  <span class="mf">6.3753e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.0736e-01</span><span class="p">,</span>  <span class="mf">2.7862e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3382e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5506e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.1703e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">8.7792e-02</span><span class="p">,</span>  <span class="mf">1.9728e-01</span><span class="p">,</span>  <span class="mf">1.1501e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2522e-02</span><span class="p">,</span>  <span class="mf">7.0123e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.9621e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7082e-01</span><span class="p">,</span>  <span class="mf">9.3322e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1328e-01</span><span class="p">,</span>  <span class="mf">9.6219e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.4119e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.9062e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6474e-01</span><span class="p">,</span>  <span class="mf">1.0437e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0379e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.5392e-01</span><span class="p">,</span>  <span class="mf">1.2377e-01</span><span class="p">,</span>  <span class="mf">1.1067e-01</span><span class="p">,</span>  <span class="mf">6.4312e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.1836e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">4.3486e-02</span><span class="p">,</span>  <span class="mf">2.6580e-02</span><span class="p">,</span>  <span class="mf">9.9142e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4012e-01</span><span class="p">,</span>  <span class="mf">1.6786e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">2.1746e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6759e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0821e-01</span><span class="p">,</span>  <span class="mf">1.7696e-01</span><span class="p">,</span>  <span class="mf">1.8291e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.8526e-01</span><span class="p">,</span>  <span class="mf">1.4106e-01</span><span class="p">,</span>  <span class="mf">1.0023e-01</span><span class="p">,</span>  <span class="mf">1.1838e-01</span><span class="p">,</span>  <span class="mf">1.6931e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.0779e-01</span><span class="p">,</span>  <span class="mf">6.3155e-02</span><span class="p">,</span>  <span class="mf">8.1847e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.9099e-02</span><span class="p">,</span>  <span class="mf">6.6931e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">5.7543e-02</span><span class="p">,</span>  <span class="mf">1.2365e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.5491e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.4559e-02</span><span class="p">,</span>  <span class="mf">5.0350e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">7.0939e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.7714e-02</span><span class="p">,</span>  <span class="mf">1.7680e-02</span><span class="p">,</span>  <span class="mf">1.1591e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9899e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">6.0867e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8237e-02</span><span class="p">,</span>  <span class="mf">7.0100e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4488e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1761e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.0152e-01</span><span class="p">,</span>  <span class="mf">1.8380e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.3817e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4607e-04</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.6899e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.9966e-01</span><span class="p">,</span>  <span class="mf">1.9533e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5084e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6213e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5160e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">9.6455e-04</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0981e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3091e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.1734e-02</span><span class="p">,</span>  <span class="mf">3.7786e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.0450e-01</span><span class="p">,</span>  <span class="mf">4.4317e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5863e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.6913e-02</span><span class="p">,</span>  <span class="mf">2.7477e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.6445e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3522e-01</span><span class="p">,</span>  <span class="mf">9.2869e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.3754e-02</span><span class="p">,</span>  <span class="mf">1.9626e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.8975e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9173e-01</span><span class="p">,</span>  <span class="mf">1.9708e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2654e-01</span><span class="p">,</span>  <span class="mf">3.8345e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.7261e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2131e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.4670e-02</span><span class="p">,</span>  <span class="mf">1.2709e-01</span><span class="p">,</span>  <span class="mf">9.5594e-03</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.9472e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1807e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.8180e-02</span><span class="p">,</span>  <span class="mf">1.0064e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2943e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8858e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.4546e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5912e-02</span><span class="p">,</span>  <span class="mf">1.5671e-01</span><span class="p">,</span>  <span class="mf">1.0052e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.6962e-01</span><span class="p">,</span>  <span class="mf">1.1569e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.0671e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3269e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4881e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.6553e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.7124e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8547e-01</span><span class="p">,</span>  <span class="mf">8.1986e-02</span><span class="p">,</span>  <span class="mf">1.9469e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">5.6937e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6560e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8141e-01</span><span class="p">,</span>  <span class="mf">5.0120e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5144e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.0100e-01</span><span class="p">,</span>  <span class="mf">4.0426e-02</span><span class="p">,</span>  <span class="mf">7.5952e-02</span><span class="p">,</span>  <span class="mf">1.5906e-01</span><span class="p">,</span>  <span class="mf">1.5528e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">2.9937e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7635e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8072e-01</span><span class="p">,</span>  <span class="mf">1.8674e-01</span><span class="p">,</span>  <span class="mf">8.8411e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">8.7181e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7304e-01</span><span class="p">,</span>  <span class="mf">1.8520e-01</span><span class="p">,</span>  <span class="mf">1.8947e-01</span><span class="p">,</span>  <span class="mf">1.8057e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.6872e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.5465e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3755e-01</span><span class="p">,</span>  <span class="mf">1.8939e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.5936e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">9.1311e-02</span><span class="p">,</span>  <span class="mf">1.0478e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.2518e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.8504e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7494e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.9224e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5580e-01</span><span class="p">,</span>  <span class="mf">1.6673e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.8224e-02</span><span class="p">,</span>  <span class="mf">7.0943e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7396e-01</span><span class="p">,</span>  <span class="mf">1.0086e-01</span><span class="p">,</span>  <span class="mf">2.8316e-02</span><span class="p">,</span>  <span class="mf">1.7017e-01</span><span class="p">,</span>  <span class="mf">2.6885e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">9.2531e-02</span><span class="p">,</span>  <span class="mf">1.8920e-01</span><span class="p">,</span>  <span class="mf">4.7336e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9514e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6938e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.3623e-01</span><span class="p">,</span>  <span class="mf">1.7535e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.3029e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8230e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2573e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.2560e-01</span><span class="p">,</span>  <span class="mf">5.1960e-02</span><span class="p">,</span>  <span class="mf">6.3241e-02</span><span class="p">,</span>  <span class="mf">1.8575e-02</span><span class="p">,</span>  <span class="mf">7.4564e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">4.8730e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.9560e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8694e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5067e-01</span><span class="p">,</span>  <span class="mf">8.6681e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.1842e-01</span><span class="p">,</span>  <span class="mf">2.8693e-02</span><span class="p">,</span>  <span class="mf">6.3838e-02</span><span class="p">,</span>  <span class="mf">1.4161e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2967e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.9127e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1630e-01</span><span class="p">,</span>  <span class="mf">1.6450e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5910e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.8110e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.0879e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.1716e-02</span><span class="p">,</span>  <span class="mf">1.6801e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3763e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6795e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">9.0184e-02</span><span class="p">,</span>  <span class="mf">3.2330e-02</span><span class="p">,</span>  <span class="mf">1.8415e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.5480e-02</span><span class="p">,</span>  <span class="mf">7.1526e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">5.0141e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.3372e-02</span><span class="p">,</span>  <span class="mf">1.4706e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5500e-01</span><span class="p">,</span>  <span class="mf">2.1243e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.8808e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7482e-02</span><span class="p">,</span>  <span class="mf">1.5529e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.1591e-02</span><span class="p">,</span>  <span class="mf">1.6099e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.9347e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8825e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5129e-02</span><span class="p">,</span>  <span class="mf">6.4817e-02</span><span class="p">,</span>  <span class="mf">7.8465e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">1.5356e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9797e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.9310e-03</span><span class="p">,</span>  <span class="mf">1.3696e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.4115e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7533e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3316e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1642e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0742e-02</span><span class="p">,</span>  <span class="mf">8.9242e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.3674e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1701e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6570e-01</span><span class="p">,</span>  <span class="mf">9.9098e-02</span><span class="p">,</span>  <span class="mf">5.9826e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">8.0340e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2337e-01</span><span class="p">,</span>  <span class="mf">1.2872e-01</span><span class="p">,</span>  <span class="mf">1.8943e-01</span><span class="p">,</span>  <span class="mf">1.7354e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.8681e-01</span><span class="p">,</span>  <span class="mf">1.4040e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4829e-01</span><span class="p">,</span>  <span class="mf">4.5974e-02</span><span class="p">,</span>  <span class="mf">2.9064e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">9.3607e-02</span><span class="p">,</span>  <span class="mf">6.9639e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7889e-01</span><span class="p">,</span>  <span class="mf">4.5552e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2679e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">2.1627e-02</span><span class="p">,</span>  <span class="mf">2.5726e-02</span><span class="p">,</span>  <span class="mf">1.7039e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.5424e-02</span><span class="p">,</span>  <span class="mf">1.2813e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">2.5407e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.4986e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7416e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8350e-01</span><span class="p">,</span>  <span class="mf">1.9532e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">4.9880e-02</span><span class="p">,</span>  <span class="mf">9.9898e-03</span><span class="p">,</span>  <span class="mf">5.4222e-02</span><span class="p">,</span>  <span class="mf">1.3595e-01</span><span class="p">,</span>  <span class="mf">1.7069e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.6220e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4818e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2043e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1835e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1427e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">4.7441e-02</span><span class="p">,</span>  <span class="mf">1.8773e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6022e-01</span><span class="p">,</span>  <span class="mf">1.2104e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0369e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">3.8955e-02</span><span class="p">,</span>  <span class="mf">1.5875e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5234e-02</span><span class="p">,</span>  <span class="mf">1.8211e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3981e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7025e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5854e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4628e-01</span><span class="p">,</span>  <span class="mf">5.8562e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4220e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.5870e-01</span><span class="p">,</span>  <span class="mf">1.2184e-02</span><span class="p">,</span>  <span class="mf">1.5857e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.6597e-02</span><span class="p">,</span>  <span class="mf">9.4133e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">1.7186e-01</span><span class="p">,</span>  <span class="mf">1.3265e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0491e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1924e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.2937e-02</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.6082e-01</span><span class="p">,</span>  <span class="mf">8.2642e-05</span><span class="p">,</span>  <span class="mf">1.5049e-01</span><span class="p">,</span>  <span class="mf">1.6850e-01</span><span class="p">,</span>  <span class="mf">1.8660e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">4.5425e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.6584e-02</span><span class="p">,</span>  <span class="mf">1.5097e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.3222e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3893e-01</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">3.0799e-02</span><span class="p">,</span>  <span class="mf">1.1988e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8032e-01</span><span class="p">,</span>  <span class="mf">1.8079e-01</span><span class="p">,</span>  <span class="mf">7.1946e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.4012e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.3081e-02</span><span class="p">,</span>  <span class="mf">1.7352e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5345e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.8455e-02</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">6.5596e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7522e-01</span><span class="p">,</span>  <span class="mf">1.0964e-01</span><span class="p">,</span>  <span class="mf">1.0411e-01</span><span class="p">,</span>  <span class="mf">1.2404e-01</span><span class="p">]]],</span>


        <span class="p">[[[</span><span class="o">-</span><span class="mf">1.2751e-01</span><span class="p">,</span>  <span class="mf">1.9920e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1855e-01</span><span class="p">,</span>  <span class="mf">1.9967e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9194e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.7819e-01</span><span class="p">,</span>  <span class="mf">1.2284e-01</span><span class="p">,</span>  <span class="mf">2.0903e-02</span><span class="p">,</span>  <span class="mf">1.1538e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1075e-01</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">8.3895e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.8477e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9483e-01</span><span class="p">,</span>  <span class="mf">1.0394e-02</span><span class="p">,</span>  <span class="mf">3.5372e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">1.6932e-04</span><span class="p">,</span>  <span class="mf">6.4586e-02</span><span class="p">,</span>  <span class="mf">1.8977e-01</span><span class="p">,</span>  <span class="mf">5.3177e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.3214e-02</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">8.2309e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.2796e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3864e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2210e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5373e-02</span><span class="p">]]]],</span>
       <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 模型发送时间 每个客户端单独计算发送时间</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">client</span><span class="o">.</span><span class="n">send_time_cost</span>
<span class="p">{</span><span class="s1">&#39;num_rounds&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;total_cost&#39;</span><span class="p">:</span> <span class="mf">1461.3775424957275</span><span class="p">}</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 每一轮评估一次模型</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_gap</span>
<span class="mi">1</span>
<span class="c1"># rounds = 0: 初始状态评估，全局模型下发到client之后</span>
<span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">0</span><span class="o">-------------</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="c1"># 测试指标 中心方没有test数据集 中心方得到准确率的方式是从客户端这里拿，然后加权</span>
<span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">()</span>

<span class="c1"># client的测试方法client0,acc和auc</span>
<span class="n">c</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">()</span>
<span class="c1"># 一个batch一个batch的操作 DataLoader 第一个batchsize</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">testloaderfull</span><span class="err">：</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc</span>
<span class="mi">0</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_num</span>
<span class="mi">10</span>
<span class="c1"># 真实值y对应的独热编码</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">lb</span>
<span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># 遍历完所有client0 测试集</span>
<span class="c1"># 48个batchsize后，重新按第一个维度进行拼接</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
<span class="mi">48</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="c1"># 每个batchsize,(batchsize,numclasses)</span>
<span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="c1"># 每个数据</span>
<span class="c1"># 按第一个维度拼接之后，所有数据拼接，去掉了batchsize</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
<span class="mi">471</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="c1">#同上</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">auc</span>
<span class="mf">0.38275912327597994</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc</span>
<span class="mi">16</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_num</span>
<span class="mi">471</span>

<span class="c1"># 对于server而言</span>
<span class="c1"># client 0 </span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_correct</span> <span class="c1"># 正确的样本数</span>
<span class="p">[</span><span class="mf">16.0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_auc</span> <span class="c1"># auc 乘 总样本数</span>
<span class="p">[</span><span class="mf">180.27954706298655</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">471</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_correct</span>
<span class="p">[</span><span class="mf">16.0</span><span class="p">,</span> <span class="mf">55.0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_auc</span>
<span class="p">[</span><span class="mf">180.27954706298655</span><span class="p">,</span> <span class="mf">331.7627298981608</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">471</span><span class="p">,</span> <span class="mi">731</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">ids</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># client测试方法，loss，client 0 </span>
<span class="n">c</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">()</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span><span class="c1"># 141个batchsize</span>
<span class="mi">141</span>
<span class="c1"># 第一个batchsize</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">loss</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">2.3223</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_num</span>
<span class="mi">10</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">losses</span> <span class="c1"># loss*样本数</span>
<span class="mf">23.22331666946411</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">losses</span>
<span class="mf">3269.2880749702454</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_num</span> <span class="c1"># 少一个数据？</span>
<span class="mi">1410</span>
           
<span class="c1"># 对于server而言</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">1410</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">losses</span>
<span class="p">[</span><span class="mf">3269.2880749702454</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">1410</span><span class="p">,</span> <span class="mi">2190</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">losses</span>
<span class="p">[</span><span class="mf">3269.2880749702454</span><span class="p">,</span> <span class="mf">5063.167586326599</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">ids</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># 计算acc，auc，loss</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">stats</span>
<span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">471</span><span class="p">,</span> <span class="mi">731</span><span class="p">],</span> <span class="p">[</span><span class="mf">16.0</span><span class="p">,</span> <span class="mf">55.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">180.27954706298655</span><span class="p">,</span> <span class="mf">331.7627298981608</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">stats_train</span>
<span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1410</span><span class="p">,</span> <span class="mi">2190</span><span class="p">],</span> <span class="p">[</span><span class="mf">3269.2880749702454</span><span class="p">,</span> <span class="mf">5063.167586326599</span><span class="p">])</span>
<span class="c1"># acc：每个client样本正确数分别除所有client样本数 想加</span>
<span class="c1"># 相当于，每个客户端的正确率*每个客户端数据的比重 相加          </span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span><span class="o">*</span><span class="mf">1.0</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># &lt;=&gt;stats[2][0]/stats[1][0])*(stats[1][0]/sum[stats[1]]+...</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc</span>
<span class="mf">0.05906821963394343</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_auc</span>
<span class="mf">0.4259919109493738</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">train_loss</span>
<span class="mf">2.314571017026901</span>
<span class="c1"># 每个客户端的acc和auc</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">accs</span>
<span class="p">[</span><span class="mf">0.03397027600849257</span><span class="p">,</span> <span class="mf">0.07523939808481532</span><span class="p">]</span> 
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">aucs</span>
<span class="p">[</span><span class="mf">0.38275912327597994</span><span class="p">,</span> <span class="mf">0.4538477837184142</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="c1"># 标准差</span>
<span class="mf">0.020634561038161376</span>   
           
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.3146</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0591</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.4260</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0206</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0355</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># client -&gt;server</span>
<span class="bp">self</span><span class="o">.</span><span class="n">receive_models</span><span class="p">()</span>
<span class="c1"># 选择client的时候是随机选择的，所有会导致选择的客户端的id的顺序会变化</span>
<span class="c1"># `random.choice` 函数在随机抽取多个元素时会打乱它们的顺序</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">clients</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientavg</span><span class="o">.</span><span class="n">clientAVG</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x000001BC6BCD4F88</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientavg</span><span class="o">.</span><span class="n">clientAVG</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x000001BC3A056D88</span><span class="o">&gt;</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">selected_clients</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientavg</span><span class="o">.</span><span class="n">clientAVG</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x000001BC3A056D88</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientavg</span><span class="o">.</span><span class="n">clientAVG</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x000001BC6BCD4F88</span><span class="o">&gt;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">client</span> <span class="ow">in</span> <span class="n">active_clients</span><span class="p">:</span>
<span class="c1"># client 1</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">client_time_cost</span><span class="c1"># 训练时间＋发送时间</span>
<span class="mf">1.2847285270690918</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_threthold</span> <span class="c1"># 超时就不参与聚合(s)(2.78h)</span>
<span class="mi">10000</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_samples</span> <span class="c1"># 样本数</span>
<span class="mi">2191</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_ids</span> <span class="c1"># client id</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_weights</span> <span class="c1"># 权重，数据量</span>
<span class="p">[</span><span class="mi">2191</span><span class="p">]</span>
<span class="bp">self</span><span class="o">.</span><span class="n">uploaded_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">model</span><span class="p">)</span><span class="c1"># client模型</span>
<span class="c1"># 所有client</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_samples</span>
<span class="mi">3602</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_ids</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_weights</span>
<span class="p">[</span><span class="mi">2191</span><span class="p">,</span> <span class="mi">1411</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_weights</span><span class="c1"># 权重归一化</span>
<span class="p">[</span><span class="mf">0.6082731815657968</span><span class="p">,</span> <span class="mf">0.3917268184342032</span><span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 聚合模型</span>
<span class="bp">self</span><span class="o">.</span><span class="n">aggregate_parameters</span><span class="p">()</span>
<span class="c1"># 初始化全局模型，复制client 0的模型结构且参数置为0 </span>
<span class="bp">self</span><span class="o">.</span><span class="n">global_model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">uploaded_models</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_weights</span>
<span class="p">[</span><span class="mf">0.6082731815657968</span><span class="p">,</span> <span class="mf">0.3917268184342032</span><span class="p">]</span>
<span class="c1"># 加权累加</span>
<span class="bp">self</span><span class="o">.</span><span class="n">add_parameters</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">client_model</span><span class="p">)</span>
<span class="n">server_param</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">client_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">*</span> <span class="n">w</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span><span class="lnt">90
</span><span class="lnt">91
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># python main.py -gr 20</span>
<span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">20</span><span class="o">-------------</span>

<span class="n">Evaluate</span> <span class="k">global</span> <span class="n">model</span>
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0773</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.9584</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9933</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0025</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0011</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs_test_acc</span>
<span class="p">[</span><span class="mf">0.05906821963394343</span><span class="p">,</span> <span class="mf">0.3011647254575707</span><span class="p">,</span> <span class="mf">0.6755407653910149</span><span class="p">,</span> <span class="mf">0.8236272878535774</span><span class="p">,</span> <span class="mf">0.8569051580698835</span><span class="p">,</span> <span class="mf">0.889351081530782</span><span class="p">,</span> <span class="mf">0.9026622296173045</span><span class="p">,</span> <span class="mf">0.913477537437604</span><span class="p">,</span> <span class="mf">0.9234608985024958</span><span class="p">,</span> <span class="mf">0.93</span>
<span class="mi">51081530782029</span><span class="p">,</span> <span class="mf">0.9334442595673876</span><span class="p">,</span> <span class="mf">0.937603993344426</span><span class="p">,</span> <span class="mf">0.9484193011647255</span><span class="p">,</span> <span class="mf">0.9450915141430949</span><span class="p">,</span> <span class="mf">0.9492512479201332</span><span class="p">,</span> <span class="mf">0.9559068219633944</span><span class="p">,</span> <span class="mf">0.9534109816971714</span><span class="p">,</span> <span class="mf">0.9492512479201332</span><span class="p">,</span> <span class="mf">0.9550748</span>
<span class="mi">752079867</span><span class="p">,</span> <span class="mf">0.9534109816971714</span><span class="p">,</span> <span class="mf">0.9584026622296173</span><span class="p">]</span>

<span class="n">Best</span> <span class="n">accuracy</span><span class="o">.</span>
<span class="mf">0.9584026622296173</span>
<span class="n">Average</span> <span class="n">time</span> <span class="n">cost</span> <span class="n">per</span> <span class="nb">round</span><span class="o">.</span><span class="c1"># 时间开销</span>
<span class="mf">1.283463180065155</span>

<span class="c1"># 保存结果 </span>
<span class="bp">self</span><span class="o">.</span><span class="n">save_results</span><span class="p">()</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">algo</span>
<span class="s1">&#39;mnist_FedAvg&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">result_path</span>
<span class="s1">&#39;../results/&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">algo</span>
<span class="s1">&#39;mnist_FedAvg_test_0&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">file_path</span>
<span class="s1">&#39;../results/mnist_FedAvg_test_0.h5&#39;</span>
<span class="c1"># 向文件中写入三个数据集，分别是 ‘rs_test_acc’、‘rs_test_auc’ 和 ‘rs_train_loss’</span>
<span class="c1"># evaluate中并没有保存rs_test_auc</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs_test_acc</span>
<span class="p">[</span><span class="mf">0.05906821963394343</span><span class="p">,</span> <span class="mf">0.3011647254575707</span><span class="p">,</span> <span class="mf">0.6755407653910149</span><span class="p">,</span> <span class="mf">0.8236272878535774</span><span class="p">,</span> <span class="mf">0.8569051580698835</span><span class="p">,</span> <span class="mf">0.889351081530782</span><span class="p">,</span> <span class="mf">0.9026622296173045</span><span class="p">,</span> <span class="mf">0.913477537437604</span><span class="p">,</span> <span class="mf">0.9234608985024958</span><span class="p">,</span> <span class="mf">0.93</span>
<span class="mi">51081530782029</span><span class="p">,</span> <span class="mf">0.9334442595673876</span><span class="p">,</span> <span class="mf">0.937603993344426</span><span class="p">,</span> <span class="mf">0.9484193011647255</span><span class="p">,</span> <span class="mf">0.9450915141430949</span><span class="p">,</span> <span class="mf">0.9492512479201332</span><span class="p">,</span> <span class="mf">0.9559068219633944</span><span class="p">,</span> <span class="mf">0.9534109816971714</span><span class="p">,</span> <span class="mf">0.9492512479201332</span><span class="p">,</span> <span class="mf">0.9550748</span>
<span class="mi">752079867</span><span class="p">,</span> <span class="mf">0.9534109816971714</span><span class="p">,</span> <span class="mf">0.9584026622296173</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs_test_auc</span>
<span class="p">[]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs_train_loss</span>
<span class="p">[</span><span class="mf">2.314571017026901</span><span class="p">,</span> <span class="mf">2.086947536468506</span><span class="p">,</span> <span class="mf">1.3195898769630325</span><span class="p">,</span> <span class="mf">0.6253579562943843</span><span class="p">,</span> <span class="mf">0.43360667809223136</span><span class="p">,</span> <span class="mf">0.32859760250689257</span><span class="p">,</span> <span class="mf">0.28546613085911504</span><span class="p">,</span> <span class="mf">0.2471489678292225</span><span class="p">,</span> <span class="mf">0.21599847520701587</span><span class="p">,</span> <span class="mi">0</span>
<span class="mf">.1882220826137604</span><span class="p">,</span> <span class="mf">0.1742167657499926</span><span class="p">,</span> <span class="mf">0.1539727338169339</span><span class="p">,</span> <span class="mf">0.14151522711229822</span><span class="p">,</span> <span class="mf">0.13180814096962826</span><span class="p">,</span> <span class="mf">0.126626226349294</span><span class="p">,</span> <span class="mf">0.10841290605668392</span><span class="p">,</span> <span class="mf">0.10612137640515963</span><span class="p">,</span> <span class="mf">0.10253492878497733</span><span class="p">,</span> <span class="mi">0</span>
<span class="mf">.08905219737142842</span><span class="p">,</span> <span class="mf">0.0842796761914441</span><span class="p">,</span> <span class="mf">0.07731760282153523</span><span class="p">]</span>

<span class="c1"># 重新修改 self.evaluate() 后</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">rs_test_auc</span>
<span class="p">[</span><span class="mf">0.4259919109493738</span><span class="p">,</span> <span class="mf">0.7447302648864543</span><span class="p">,</span> <span class="mf">0.9229162438448569</span><span class="p">,</span> <span class="mf">0.9672684372148672</span><span class="p">,</span> <span class="mf">0.9744737834460371</span><span class="p">,</span> <span class="mf">0.9766528826588904</span><span class="p">,</span> <span class="mf">0.9814767508333857</span><span class="p">,</span> <span class="mf">0.9850432755687425</span><span class="p">,</span> <span class="mf">0.985603523913785</span><span class="p">,</span> <span class="mf">0.98</span>
<span class="mi">7561845229809</span><span class="p">,</span> <span class="mf">0.9881369731008514</span><span class="p">,</span> <span class="mf">0.989969924724848</span><span class="p">,</span> <span class="mf">0.989925098209637</span><span class="p">,</span> <span class="mf">0.9911689493678782</span><span class="p">,</span> <span class="mf">0.9914534505980559</span><span class="p">,</span> <span class="mf">0.9919368626935388</span><span class="p">,</span> <span class="mf">0.9926894980663109</span><span class="p">,</span> <span class="mf">0.9917173501511063</span><span class="p">,</span> <span class="mf">0.992863217</span>
<span class="mi">0223384</span><span class="p">,</span> <span class="mf">0.9926117283057737</span><span class="p">,</span> <span class="mf">0.9932817668849713</span><span class="p">]</span>

<span class="c1"># 读取h5文件并按原格式打印</span>
<span class="kn">import</span> <span class="nn">h5py</span>
<span class="k">with</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="s1">&#39;D:/Python/PycharmProjects/PFL-Non-IID-master/PFL-Non-IID-master/results/mnist_FedAvg_test_0.h5&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="c1"># 遍历文件中的所有数据集并打印</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">file</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="s2">&#34;:&#34;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">file</span><span class="p">[</span><span class="n">key</span><span class="p">][:])</span>
        
<span class="c1"># 保存全局模型</span>
<span class="bp">self</span><span class="o">.</span><span class="n">save_global_model</span><span class="p">()</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">model_path</span>
<span class="s1">&#39;models</span><span class="se">\\</span><span class="s1">mnist&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">model_path</span>
<span class="s1">&#39;models</span><span class="se">\\</span><span class="s1">mnist</span><span class="se">\\</span><span class="s1">FedAvg_server.pt&#39;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_model</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>

<span class="c1"># 全局平均，多次实验中(args.times)</span>
<span class="c1"># 从文件中读取多次实验的acc，得到每次实验的最大acc，计算标准差和均值</span>
<span class="n">average_data</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">algorithm</span>
<span class="s1">&#39;FedAvg&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">dataset</span>
<span class="s1">&#39;mnist&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">goal</span>
<span class="s1">&#39;test&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">times</span>
<span class="mi">1</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">max_accurancy</span>
<span class="p">[</span><span class="mf">0.9584026622296173</span><span class="p">]</span>
<span class="c1"># 多次实验结果的标准差和均值</span>
<span class="n">std</span> <span class="k">for</span> <span class="n">best</span> <span class="n">accurancy</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="n">mean</span> <span class="k">for</span> <span class="n">best</span> <span class="n">accurancy</span><span class="p">:</span> <span class="mf">0.9584026622296173</span>

<span class="c1"># 输出内存使用情况</span>
<span class="n">reporter</span><span class="o">.</span><span class="n">report</span><span class="p">()</span>

<span class="n">Storage</span> <span class="n">on</span> <span class="n">cuda</span><span class="p">:</span><span class="mi">0</span>
<span class="o">-------------------------------------------------------------------------------</span>
<span class="n">Total</span> <span class="n">Tensors</span><span class="p">:</span> <span class="mi">4656208</span>  <span class="n">Used</span> <span class="n">Memory</span><span class="p">:</span> <span class="mf">13.33</span><span class="n">M</span>
<span class="n">The</span> <span class="n">allocated</span> <span class="n">memory</span> <span class="n">on</span> <span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">:</span> <span class="mf">13.33</span><span class="n">M</span>
<span class="o">-------------------------------------------------------------------------------</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 参考文件 example.sh</span>
<span class="c1"># `2&gt;&amp;1` 表示将标准错误（stderr）重定向到标准输出（stdout），这意味着错误信息将与标准输出一起处理。</span>
<span class="c1"># `&amp;` 表示在后台运行命令，这样可以使命令在后台持续运行而不会阻塞当前终端。</span>
<span class="c1"># 保存输出到mnist_fedavg.out文件中</span>
<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">20</span> <span class="o">&gt;</span> <span class="n">mnist_fedavg</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> <span class="o">&amp;</span>

<span class="c1"># 读取mnist_fedavg.out文件</span>
<span class="c1"># 查找并提取出包含最佳准确率的行，然后计算这些最佳准确率的平均值和标准差</span>
<span class="kn">from</span> <span class="nn">statistics</span> <span class="kn">import</span> <span class="n">mean</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">file_name</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span> <span class="o">+</span> <span class="s1">&#39;.out&#39;</span>

<span class="n">acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">is_best</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">is_best</span><span class="p">:</span>
            <span class="n">acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
            <span class="n">is_best</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="s1">&#39;Best accuracy&#39;</span> <span class="ow">in</span> <span class="n">l</span><span class="p">:</span>
            <span class="n">is_best</span> <span class="o">=</span> <span class="kc">True</span>

<span class="nb">print</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="s2">&#34;&#34;&#34;
</span><span class="s2">D:\Python\Python37\python.exe D:/Python/PycharmProjects/PFL-Non-IID-master/PFL-Non-IID-master/system/get_mean_std.py
</span><span class="s2">D:/Python/PycharmProjects/PFL-Non-IID-master/PFL-Non-IID-master/system/mnist_fedavg
</span><span class="s2">[0.9575707154742097]
</span><span class="s2">95.75707154742096 0.0
</span><span class="s2">&#34;&#34;&#34;</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># python generate_mnist.py iid - -</span>

<span class="n">D</span><span class="p">:</span>\<span class="n">Python</span>\<span class="n">Python37</span>\<span class="n">python</span><span class="o">.</span><span class="n">exe</span> <span class="n">D</span><span class="p">:</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="n">PycharmProjects</span><span class="o">/</span><span class="n">PFL</span><span class="o">-</span><span class="n">Non</span><span class="o">-</span><span class="n">IID</span><span class="o">-</span><span class="n">master</span><span class="o">/</span><span class="n">PFL</span><span class="o">-</span><span class="n">Non</span><span class="o">-</span><span class="n">IID</span><span class="o">-</span><span class="n">master</span><span class="o">/</span><span class="n">system</span><span class="o">/</span><span class="n">main</span><span class="o">.</span><span class="n">py</span>
<span class="o">==================================================</span>
<span class="n">Algorithm</span><span class="p">:</span> <span class="n">FedAvg</span>
<span class="n">Local</span> <span class="n">batch</span> <span class="n">size</span><span class="p">:</span> <span class="mi">10</span>
<span class="n">Local</span> <span class="n">steps</span><span class="p">:</span> <span class="mi">1</span> <span class="c1">#每次聚合的时候本地训练次数</span>
<span class="n">Local</span> <span class="n">learing</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">0.005</span>
<span class="n">Local</span> <span class="n">learing</span> <span class="n">rate</span> <span class="n">decay</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">Total</span> <span class="n">number</span> <span class="n">of</span> <span class="n">clients</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">Clients</span> <span class="n">join</span> <span class="ow">in</span> <span class="n">each</span> <span class="nb">round</span><span class="p">:</span> <span class="mf">1.0</span> <span class="c1"># cliet全部参与</span>
<span class="n">Clients</span> <span class="n">randomly</span> <span class="n">join</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">Client</span> <span class="n">drop</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="n">Client</span> <span class="n">select</span> <span class="n">regarding</span> <span class="n">time</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">Running</span> <span class="n">times</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">Dataset</span><span class="p">:</span> <span class="n">mnist</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">classes</span><span class="p">:</span> <span class="mi">10</span>
<span class="n">Backbone</span><span class="p">:</span> <span class="n">cnn</span>
<span class="n">Using</span> <span class="n">device</span><span class="p">:</span> <span class="n">cuda</span>
<span class="n">Using</span> <span class="n">DP</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">Auto</span> <span class="k">break</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">Global</span> <span class="n">rounds</span><span class="p">:</span> <span class="mi">2000</span>
<span class="n">Cuda</span> <span class="n">device</span> <span class="nb">id</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">DLG</span> <span class="n">attack</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">Total</span> <span class="n">number</span> <span class="n">of</span> <span class="n">new</span> <span class="n">clients</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">Fine</span> <span class="n">tuning</span> <span class="n">epoches</span> <span class="n">on</span> <span class="n">new</span> <span class="n">clients</span><span class="p">:</span> <span class="mi">0</span>
<span class="o">==================================================</span>

<span class="o">=============</span> <span class="n">Running</span> <span class="n">time</span><span class="p">:</span> <span class="mi">0</span><span class="n">th</span> <span class="o">=============</span><span class="c1"># 运行次数(整个实验的次数)</span>
<span class="n">Creating</span> <span class="n">server</span> <span class="ow">and</span> <span class="n">clients</span> <span class="o">...</span>
<span class="n">FedAvgCNN</span><span class="p">(</span>
  <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">Join</span> <span class="n">ratio</span> <span class="o">/</span> <span class="n">total</span> <span class="n">clients</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">Finished</span> <span class="n">creating</span> <span class="n">server</span> <span class="ow">and</span> <span class="n">clients</span><span class="o">.</span>

<span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">0</span><span class="o">-------------</span><span class="c1">#轮次</span>

<span class="n">Evaluate</span> <span class="k">global</span> <span class="n">model</span>
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.3146</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0591</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.4260</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0206</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0355</span>
<span class="o">-------------------------</span> <span class="n">time</span> <span class="n">cost</span> <span class="o">-------------------------</span> <span class="mf">5.320608139038086</span>

<span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">1</span><span class="o">-------------</span>

<span class="n">Evaluate</span> <span class="k">global</span> <span class="n">model</span>
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.0873</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.2995</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.7448</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0540</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0632</span>
<span class="o">-------------------------</span> <span class="n">time</span> <span class="n">cost</span> <span class="o">-------------------------</span> <span class="mf">1.006882905960083</span>

<span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">2</span><span class="o">-------------</span>

<span class="n">Evaluate</span> <span class="k">global</span> <span class="n">model</span>
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.3040</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.7155</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9386</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0332</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0046</span>
<span class="o">-------------------------</span> <span class="n">time</span> <span class="n">cost</span> <span class="o">-------------------------</span> <span class="mf">0.971451997756958</span>

<span class="o">......</span>

<span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">20</span><span class="o">-------------</span>

<span class="n">Evaluate</span> <span class="k">global</span> <span class="n">model</span>
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0775</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.9576</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9933</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0035</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0011</span>
<span class="o">-------------------------</span> <span class="n">time</span> <span class="n">cost</span> <span class="o">-------------------------</span> <span class="mf">0.9970800876617432</span>

<span class="n">Best</span> <span class="n">accuracy</span><span class="o">.</span>
<span class="mf">0.9575707154742097</span>

<span class="n">Average</span> <span class="n">time</span> <span class="n">cost</span> <span class="n">per</span> <span class="nb">round</span><span class="o">.</span>
<span class="mf">1.0050838351249696</span>
<span class="n">File</span> <span class="n">path</span><span class="p">:</span> <span class="o">../</span><span class="n">results</span><span class="o">/</span><span class="n">mnist_FedAvg_test_0</span><span class="o">.</span><span class="n">h5</span>

<span class="n">Average</span> <span class="n">time</span> <span class="n">cost</span><span class="p">:</span> <span class="mf">25.04</span><span class="n">s</span><span class="o">.</span>
<span class="n">Length</span><span class="p">:</span>  <span class="mi">21</span>
<span class="n">std</span> <span class="k">for</span> <span class="n">best</span> <span class="n">accurancy</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="n">mean</span> <span class="k">for</span> <span class="n">best</span> <span class="n">accurancy</span><span class="p">:</span> <span class="mf">0.9575707154742097</span>
<span class="n">All</span> <span class="n">done</span><span class="err">!</span>

<span class="n">Storage</span> <span class="n">on</span> <span class="n">cuda</span><span class="p">:</span><span class="mi">0</span>
<span class="o">-------------------------------------------------------------------------------</span>
<span class="n">Total</span> <span class="n">Tensors</span><span class="p">:</span> <span class="mi">4656208</span>  <span class="n">Used</span> <span class="n">Memory</span><span class="p">:</span> <span class="mf">13.33</span><span class="n">M</span>
<span class="n">The</span> <span class="n">allocated</span> <span class="n">memory</span> <span class="n">on</span> <span class="n">cuda</span><span class="p">:</span><span class="mi">0</span><span class="p">:</span> <span class="mf">13.33</span><span class="n">M</span>
<span class="o">-------------------------------------------------------------------------------</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">-</span><span class="n">ls</span> <span class="mi">1</span>
<span class="n">mean</span> <span class="k">for</span> <span class="n">best</span> <span class="n">accurancy</span><span class="p">:</span> <span class="mf">0.9559068219633944</span>
<span class="o">-</span><span class="n">ls</span> <span class="mi">3</span> <span class="c1"># 最优不是最后一轮</span>
<span class="n">mean</span> <span class="k">for</span> <span class="n">best</span> <span class="n">accurancy</span><span class="p">:</span> <span class="mf">0.9700499168053245</span>
<span class="o">-</span><span class="n">ls</span> <span class="mi">5</span> <span class="c1"># 最优不是最后一轮</span>
<span class="n">mean</span> <span class="k">for</span> <span class="n">best</span> <span class="n">accurancy</span><span class="p">:</span> <span class="mf">0.9725457570715474</span>

<span class="n">这种情况可能是由于神经网络出现了过拟合</span><span class="err">。</span><span class="n">过拟合指模型在训练集上表现良好</span><span class="err">，</span><span class="n">但在测试集上性能下降的现象</span><span class="err">。</span><span class="n">解决方法可以包括</span><span class="err">：</span>

<span class="n">早停</span><span class="err">（</span><span class="n">Early</span> <span class="n">Stopping</span><span class="err">）：</span><span class="n">在训练过程中监测验证集上的性能</span><span class="err">，</span><span class="n">当性能开始下降时停止训练</span><span class="err">，</span><span class="n">避免过拟合</span><span class="err">。</span>
<span class="n">使用正则化</span><span class="err">：</span><span class="n">通过</span> <span class="n">L1</span> <span class="n">或</span> <span class="n">L2</span> <span class="n">正则化限制模型的复杂度</span><span class="err">，</span><span class="n">防止过度拟合</span><span class="err">。</span>
<span class="n">数据增强</span><span class="err">（</span><span class="n">Data</span> <span class="n">Augmentation</span><span class="err">）：</span><span class="n">增加训练集的样本多样性</span><span class="err">，</span><span class="n">有助于提高模型的泛化能力</span><span class="err">。</span>
<span class="n">降低模型复杂度</span><span class="err">：</span><span class="n">减少神经网络的层数或每层神经元数量</span><span class="err">，</span><span class="n">以降低模型的复杂度</span><span class="err">。</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># 模型结构图
Input: (batch_size, 1, height, width)  
  
      -&gt; Conv2d(1, 32)  
      -&gt; ReLU  
      -&gt; MaxPool2d  
      -&gt; Conv2d(32, 64)  
      -&gt; ReLU  
      -&gt; MaxPool2d  
      -&gt; Flatten  
      -&gt; Linear(1024, 512)  
      -&gt; ReLU  
      -&gt; Linear(512, 10)  
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Installing collected packages: torch, opacus
Attempting uninstall: torch
Found existing installation: torch 1.10.1+cu113
Uninstalling torch-1.10.1+cu113:
Successfully uninstalled torch-1.10.1+cu113
ERROR: pip&rsquo;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.</p>
<p>重新安装了torch 1.10.1+cu113</p>
</blockquote>
<p>凸优化？</p>
<p>&ldquo;Convex&quot;是一个几何和数学术语，用来描述凸形或凸函数。在数学中，一个集合被称为凸集，如果集合中包含的任意两点之间的线段仍然在该集合内部。凸函数则是一种特殊的函数，其图像上的任意两点之间的线段位于函数图像的上方。</p>
<p>在优化和凸优化中，凸集和凸函数具有重要的性质，例如任意局部最小值都是全局最小值等。因此，凸集和凸函数在数学建模和优化问题中具有重要的作用。</p>
<p>&ldquo;Non-convex&quot;是指非凸的意思。在数学中，非凸通常用来描述不满足凸性质的集合或函数。对于一个集合来说，如果存在集合内的两点，连接这两点的线段不完全位于集合内部，则该集合是非凸的。对于一个函数来说，如果函数图像上的某一部分不满足凸性质，那么该函数就是非凸的。</p>
<p>在优化问题中，非凸函数通常具有多个局部最小值，使得寻找全局最小值变得更加困难。非凸问题的优化通常比凸问题更具挑战性，因为非凸函数的性质更为复杂。</p>
<p>MLR（多元线性回归）是凸函数，因为其损失函数是平方损失，平方损失函数是凸函数。</p>
<p>CNN（卷积神经网络）通常是非凸函数，因为它们的损失函数通常是非凸的，例如交叉熵损失函数。交叉熵损失函数在一般情况下是非凸的，因此导致了 CNN 通常是非凸函数。</p>
<p>需要注意的是，在特定情况下，CNN 的一些部分可能是凸函数，但整个网络通常是非凸的。</p>
<p><a href="https://www.cnblogs.com/wuliytTaotao/archive/2019/12/01/11967620.html">交叉熵损失</a>：对于 Logit模型来说，交叉熵损失函数是凸的，因为这个函数是对数和 exp 函数的一个凸组合，这个函数是凸的。这种凸性允许使用基于梯度的方法进行有效的优化。然而，在神经网络环境中，由于交叉熵损失函数由于多层和激活函数引入的非线性而不能保证是凸的。神经网络中权值和偏差的复杂相互作用导致损失函数通常是非凸的，使得优化更具挑战性。因此，神经网络训练通常涉及使用基于梯度的优化算法，可以处理非凸函数，如随机梯度下降及其变体。</p>
<p>梯度下降的学习率是指在每次迭代中，沿着梯度方向更新参数时所乘以的步长大小。学习率的选择会影响算法的收敛速度和最终的收敛效果。过大的学习率可能导致震荡甚至发散，而过小的学习率则会导致收敛速度过慢。因此，选择合适的学习率对于梯度下降算法的有效性至关重要。</p>
<p>梯度下降可能会遇到以下问题：</p>
<ol>
<li>局部最优解：可能陷入局部最优解而无法找到全局最优解。</li>
<li>学习率选择困难：选择不合适的学习率可能导致收敛缓慢或发散。</li>
<li>高维度问题：在高维空间中，梯度下降的收敛可能变得困难，需要更复杂的优化算法来解决。</li>
<li>鞍点问题：在鞍点处梯度接近零，使得优化过程变得困难。</li>
</ol>
<p>针对这些问题，有许多改进的优化算法，如随机梯度下降、动量法、自适应学习率方法等。</p>
<p>在深度学习中，梯度下降不当可能导致以下问题：</p>
<ol>
<li>收敛缓慢或不稳定：选择不合适的学习率或优化算法可能导致训练过程收敛缓慢或不稳定。</li>
<li>陷入局部最优解：梯度下降可能使模型陷入局部最优解，而无法达到全局最优解。</li>
<li>梯度消失或爆炸：在深层网络中，梯度下降不当可能导致梯度消失或梯度爆炸，使得网络难以训练。</li>
<li>过拟合：梯度下降过程中的参数更新可能导致模型过度拟合训练数据，导致泛化性能下降。</li>
</ol>
<p>为了解决这些问题，深度学习中常用的优化算法包括随机梯度下降（SGD）、动量法、Adam优化器等，它们通常能够更稳定地训练深度神经网络</p>
<p>过拟合是指模型在训练数据上表现很好，但在测试数据上表现不佳的情况。过拟合通常是由以下原因导致的：</p>
<ol>
<li>模型复杂度过高：模型过于复杂，能够很好地拟合训练数据的细节和噪声，但泛化能力较差。</li>
<li>训练数据不足：训练数据量太少，模型无法从有限的数据中学习到数据的真实分布，容易记住训练集的特定样本而无法泛化到新数据。</li>
<li>特征选择不当：选择的特征过多或过少，或者特征工程不合适，都可能导致模型过拟合。</li>
<li>噪声干扰：数据中的噪声干扰使得模型学习到了数据的随机变化而非真实的模式。</li>
</ol>
<p>为了缓解过拟合问题，可以采取的方法包括增加训练数据、正则化、特征选择、交叉验证等。</p>
<p>收敛性分析是指对迭代算法在解决特定问题时的收敛性质进行分析和研究的过程。在数值优化、机器学习等领域中，收敛性分析通常用于评估迭代算法在何种条件下能够收敛到期望的解，或者收敛到问题的某种性质（如局部最优解）。</p>
<p>收敛性分析通常包括以下内容：</p>
<ol>
<li>收敛准则：确定算法收敛的准则，例如迭代序列是否收敛到某个极限、误差是否趋于零等。</li>
<li>收敛速度：分析算法的收敛速度，即迭代序列收敛到目标解的速度有多快。</li>
<li>收敛性质：研究算法收敛到的解的性质，如局部最优解、全局最优解等。</li>
</ol>
<p>通过收敛性分析，可以评估算法在实际问题中的表现，并为选择合适的算法、调整参数提供指导。</p>
<p>在CNN中，梯度下降是通过反向传播算法来计算的。</p>
<ol>
<li>前向传播：首先进行前向传播，通过输入数据，计算损失函数，并沿着网络逐层计算每一层的输出。</li>
<li>反向传播：然后进行反向传播，利用链式法则计算损失函数对每一层权重参数的梯度。</li>
<li>梯度下降更新：最后使用计算得到的梯度，按照梯度下降的更新规则，对每一层的权重参数进行更新。w = w - learning_rate * gradient</li>
</ol>
<p>这样，通过反向传播算法，CNN可以高效地计算梯度并更新网络参数，从而实现模型的训练和优化。</p>
<h2 id="fedala--aaai2023">FedALA -AAAI2023</h2>
<blockquote>
<p><a href="https://readpaper.com/pdf-annotate/note?pdfId=4801716600084365313&amp;noteId=2109780263180554752">paper</a>-张剑清 上交</p>
<p><a href="https://github.com/TsingZ0/FedALA">code</a></p>
<p><a href="https://mp.weixin.qq.com/s/ILkTq38FTXOnQ4tZ8nx6Fw">blog</a>，<a href="https://zhuanlan.zhihu.com/p/625493181">2</a></p>
<p><a href="https://www.latexlive.com/##">公式</a></p>
</blockquote>
<p>本文：添加了一个微调的ALA模块进行元素级别的聚合(ALA只修改局部模型初始化的方法)-&gt;模型能够适应客户端的目标</p>
<p>全局模型<code>较低层包含了很多的信息</code>，因此在对局部模型进行初始化的时候设置一个超参数p，在<code>较高层使用上述的ALA</code>方法进行初始化，而在下面的层直接将全局模型的参数复制过来。</p>
<p>W矩阵式每个客户端自己的一个需要学习的超参数，当在第二轮W收敛之后的学习过程中W几乎保持不变，因此FedALA在之后训练过程中”复用“它。p也是超参数，但是没有对p的学习方法进行说明。</p>
<p>超参数分析：</p>
<p>s:客户端用来ala初始化的数据占比。实验表明s更大能够取得更好的测试准确率，然而s过大会增加计算开销，设置s为80</p>
<p>p:实验表明<code>减少p，ala中需要学习的参数减少了，并且ala的准确率下降并不明显</code>，设为1</p>
<p>结果分析：效果好</p>
<p>计算开销：用时与fedavg类似，但fedala只用了额外的0.34min实现了极大的提升</p>
<p>通信开销：相似</p>
<p>&ndash;</p>
<p>ALA 模块通过以下方式自适应地聚合全局模型和本地模型，以应对不同的数据分布和模型结构：</p>
<ol>
<li><strong>逐元素聚合</strong>：ALA 模块逐元素地聚合全局模型和本地模型，以适应每个客户端的局部目标。这样可以更好地捕捉全局模型中有利于改进局部模型的信息。</li>
<li><strong>局部初始化</strong>：在每次迭代中，ALA 模块在训练之前用自适应地聚合的全局模型和本地模型来初始化局部模型。这有助于提高局部模型的质量，从而提高全局模型的性能。</li>
<li><strong>分层聚合</strong>：ALA 模块允许用户通过设置超参数 p 来控制聚合范围，将 ALA 应用于模型的高层，而在较低层次上保留全局模型的信息。这样可以在降低计算开销的同时，仍然捕捉到有用的一般信息。</li>
<li><strong>适用性</strong>：由于 ALA 模块仅修改 FL 中的局部初始化过程，因此它可以应用于大多数现有的 FL 方法，以提高它们的性能，而无需修改其他学习过程。这使得 ALA 可以广泛应用于不同的数据分布和模型结构。</li>
</ol>
<p>&ndash;</p>
<p>但由于客户机之间数据不可见，数据的统计异质性（数据非独立同分布（non-IID）和数据量不平衡现象）便成了FL 的巨大挑战之一。数据的统计异质性使得传统联邦学习方法（如FedAvg等）很难通过FL过程训练得到适用于每个客户机的单一全局模型。</p>
<p>与寻求高质量全局模型的传统FL不同，pFL方法的目标是借助联邦学习的协同计算能力为每个客户机训练适用于自身的个性化模型。现有的<code>在服务器上聚合模型的pFL</code>研究可以分为以下三类：</p>
<ul>
<li>
<p>（1）学习单个全局模型并对其进行微调的方法，包括Per-FedAvg和FedRep；</p>
<p>per-fedavg(联邦元学习)：客户端根据全局模型初始化本身，然后在本地选择一小批数据计算损失的梯度，然后得到本地的元函数，再选取一小批的数据对元函数进行求导，最后本地模型更新以及上传模型</p>
</li>
<li>
<p>（2）学习额外个性化模型的方法，包括pFedMe和Ditto；</p>
<p>pfedme：额外的个性化模型即 moreau envelopes</p>
</li>
<li>
<p>（3）通过个性化聚合（或本地聚合）学习本地模型的方法，包括FedAMP、FedPHP、FedFomo、APPLE和PartialFed。</p>
<p>fedfomo：在上传本地的模型到服务器后，服务器会记录各个模型上传的参数，在本地客户端从服务器下载模型时，首先服务器会决定将哪些模型发送给哪些客户端(使用到关联矩阵)，接着本地客户端根据下载的其他模型以及自己当前的模型在验证集上的损失来计算其他模型更新的权重，最后对本地模型聚合；使用差分隐私的方法对存在的风险归并</p>
</li>
</ul>
<p>类别（1）和（2）中的pFL方法将全局模型中的<code>所有信息</code>用于本地初始化（指在每次迭代的局部训练之前初始化局部模型）。然而，在全局模型中，<code>只有提高本地模型质量的信息</code>（符合本地训练目标的客户机所需信息）才对客户机有益。全局模型的泛化能力较差是因为其中同时存在对于单一客户机来说需要和不需要的信息。</p>
<p>类别（3）中的pFL方法，通过<code>个性化聚合捕获全局模型中每个客户机所需的信息</code>。但是，类别（3）中的pFL方法依旧存在（a）<code>没有考虑客户机本地训练目标</code>（如FedAMP和FedPHP）、（b）计算代价和通讯代价较高（如FedFomo和APPLE）、（c）隐私泄露（如FedFomo和APPLE）和（d）<code>个性化聚合与本地训练目标不匹配</code>（如PartialFed）等问题。此外，由于这些方法对FL过程做了大量修改，它们使用的个性化聚合方法并不能被直接用于大多数现有FL方法。</p>
<p>为了<code>从全局模型中精确地捕获客户机所需信息</code>，且相比于FedAvg不增加每一轮迭代中的通讯代价，作者提出了一种用于联邦学习的自适应本地聚合方法（FedALA）。如图1所示，<code>FedALA在每次本地训练之前，通过自适应本地聚合（ALA）模块将全局模型与本地模型进行聚合的方式，捕获全局模型中的所需信息</code>。由于FedALA相比于FedAvg仅使用<code>ALA修改了每一轮迭代中的本地模型初始化过程</code>，而没有改动其他FL过程，因此ALA可被直接应用于大多数现有的其他FL方法，以提升它们的个性化表现。</p>
<p><img src="/images/2023/11.png" alt=""></p>
<p>图1：客户端i第t轮，从服务器下载全局模型，通过ALA模块将全局模型与旧的局部模型局部聚合，进行局部初始化，训练局部模型，最后将训练好的局部模型上传到服务器。</p>
<h3 id="自适应本地聚合ala过程">自适应本地聚合（ALA）过程</h3>
<p><img src="/images/2023/12.png" alt=""></p>
<p>图2：ALA的学习过程。LA代表“本地聚合”。这里，我们考虑一个五层模型，setp = 3。颜色越浅，值越大。</p>
<p>相比于传统联邦学习中直接将下载的全局模型覆盖本地模型得到本地初始化模型的方式<img src="/images/2023/13.png" alt="">,FedALA通过<code>为每个参数学习本地聚合权重，进行自适应本地聚合</code>，<img src="/images/2023/14.png" alt="">。</p>
<p>作者通过逐元素权重剪枝方法<img src="/images/2023/15.png" alt="">实现正则化，并将W中的值限制在[0,1]中。</p>
<p>Hadamard乘积指的是对应位置上的元素相乘。在神经网络中，Hadamard乘积通常用于执行元素级别的乘法操作，例如两个具有相同维度的矩阵或向量进行元素级别的乘法运算。&ldquo;element-wise aggregate&rdquo; 是一个术语，指的是对每个元素进行独立的聚合操作。在神经网络中，这通常指的是对两个具有相同维度的张量或矩阵进行逐元素的聚合操作，例如逐元素相加、相乘等。</p>
<p>因为深度神经网络（DNN）的较低层网络相比于较高层倾向于学习<code>相对更通用的信息</code>，而通用信息是各个本地模型所需信息，所以全局模型中较低层网络中的大部分信息与本地模型中较低层网络所需信息一致。为了降低学习本地聚合权重所需的计算代价，作者引入一个超参数p来控制ALA的作用范围，使得全局模型中<code>较低层网络参数直接覆盖本地模型中的较低层网络，而只在较高层启用ALA</code>。</p>
<p><img src="/images/2023/16.png" alt=""></p>
<p>其中，|Θi|表示Θi中的神经网络层数(或神经网络块数)，[;]中的前者与Θi的低层网络形状一致，后者与Θi中剩下的p层高层网络形状一致。</p>
<p>Wip 中的值全部初始化为1，且在每一轮本地初始化过程中基于旧的Wip来更新Wip。为了进一步降低计算代价，采用随机采样s%，本地训练数据的方式，在数据集Dis,t上通过基于梯度的学习方法更新Wip。n是更新wip的学习率。在学习wip的过程中，将除wip之外的其他可训练参数冻结。</p>
<p><img src="/images/2023/17.png" alt=""></p>
<blockquote>
<p>ˆΘt i：t轮clienti的模型参数</p>
<p>Θt−1：t-1轮的全局模型参数</p>
<p>分号通常表示条件，表示在给定条件下的损失函数&ndash;类似于条件分布函数</p>
<p>这种形式的损失函数通常用于需要考虑额外条件或先前参数状态的情况</p>
<p>在给定 Θt−1 的情况下，通过使用 ˆΘt i 和 Ds,t i 来计算损失函数 L 的值。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># w 是待更新的权重，learning_rate 是学习率，</span>
<span class="c1"># gradient 是目标函数关于权重 w 的梯度。</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>  
<span class="c1"># ∇w L 表示损失函数 L 对模型参数 w 的梯度。</span>
<span class="n">gradient</span> <span class="o">=</span> <span class="err">∇</span><span class="n">w</span> <span class="n">L</span> 

<span class="n">在卷积神经网络</span><span class="err">（</span><span class="n">CNN</span><span class="err">）</span><span class="n">中</span><span class="err">，</span><span class="n">梯度下降是针对网络中的权重参数进行的</span><span class="err">。</span><span class="n">这些权重参数包括卷积层的卷积核参数</span><span class="err">、</span><span class="n">池化层的池化核参数以及全连接层的权重参数和偏置项</span><span class="err">。</span>

<span class="c1"># 前向传播</span>
<span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># #梯度缓存清零，以确保每个训练批次的梯度都是从头开始计算的</span>
<span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="c1"># 对损失值1oss进行反向传播，计算模型参数的梯度</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># 梯度下降，更新模型的参数，以使损失函数达到最小值。</span>
<span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p><img src="/images/2024/2.png" alt=""></p>
<p>图3 在MNIST和Cifar10数据集上8号客户机的学习曲线，在每次迭代中训练至少六个 epoch 的权重。</p>
<p>一旦我们训练 W pi 在第二次迭代（初始阶段）中收敛，它在随后的迭代中几乎没有变化。换句话说，可以复用 W pi。我们只为 W pi 训练一个 epoch</p>
<p>通过选择较小的p值，在几乎不影响FedALA表现的情况下，大幅度地降低ALA中训练所需的参数。此外，如图3，一旦在第一次训练Wip 将其训练到收敛，即使在后续迭代中训练Wip，其对本地模型质量也没有很大影响。也就是说，<code>每个客户机可以复用旧的Wip实现对其所需信息的捕获。作者采取在后续迭代中微调Wip的方式</code>，降低计算代价。</p>
<p>ala分析？？？</p>
<p>把在ALA中更新W看成更新Θt i。将更新Wi视为更新Θt i意味着将权重矩阵Wi的更新视为对整体参数Θt中的子参数Θt i的更新。?</p>
<h3 id="实验">实验：</h3>
<p>在实际（practical）数据异质环境下的<code>Tiny-ImageNet数据集上用ResNet-18</code>进行了对超参数s和p的对FedALA影响的研究。</p>
<p>对于s来说，<code>采用越多的随机采样的本地训练数</code>据用于ALA模块学习可以使个性化模型表现更好，但也同时增加了计算代价。在使用ALA的过程中，可以根据每个客户机的计算能力调整s的大小。从表中可以得知，即使使用极小的s（如s=5），FedALA依旧具有杰出的表现。</p>
<p>对于p来说，<code>不同的p值对个性化模型的表现几乎没有影响</code>，在计算代价方面却有着巨大的差别。这一现象也从一个侧面展示了FedRep等方法，<code>将模型分割后保留靠近输出的神经网络层在客户机不上传的做法的有效性</code>。使用ALA时，我们可以采用较小且合适的p值，在保证个性化模型表现能力的情况下，进一步降低计算代价。</p>
<p>作者在病态（pathological）数据异质环境和实际（practical）数据异质环境下，将FedALA与11个SOTA方法进行了对比和详细分析。如表2所示，数据显示FedALA在这些情况下的表现都超越了这11个SOTA方法，其中“TINY”表示在Tiny-ImageNet上使用4-layer CNN。例如，FedALA在TINY情况下比最优基线（baseline）(看表是FedRep)高了3.27%。</p>
<p>也在不同异质性环境和客户机总量情况下评估了FedALA的表现。如表3所示，FedALA在这些情况下依旧保持着优异的表现。</p>
<p><img src="/images/2024/1.png" alt=""></p>
<p>图4 病理异质环境下局部学习轨迹(从迭代140到200)和局部损失面的二维可视化</p>
<p>可视化在MNIST上可视化了ALA模块的加入对原本FL过程中模型训练的影响，如图4所示。不激活ALA时，模型训练轨迹与使用FedAvg一致。一旦ALA被激活，模型便可以通过全局模型中捕获的其训练所需信息径直朝着最优目标优化。</p>
<p>更新方向校正。局部学习轨迹（从迭代 140 到 200）和病理异质环境中 MNIST 局部损失面的 2D 可视化。绿色方圆点和红色圆圈分别表示每次迭代开始和结束时的局部模型。带有箭头的黑色和蓝色轨迹分别代表 FedAvg 和 FedALA。使用 PCA 将局部模型投影到 2D 平面。C1 和 C2 是 PCA 生成的两个主成分。</p>
<p><code>实验设置：</code></p>
<p>MNIST Cifar10/100 Tiny-ImageNet 4 层 CNN</p>
<p>Tiny-ImageNet 上使用 ResNet-18-&gt;tiny*</p>
<p>局部学习率设置为 0.005</p>
<p>批量大小设置为 10，将局部模型训练 epoch 数设置为 1。我们运行了 2000 次迭代的所有任务，以使所有方法都在经验上收敛。在 pFedMe 之后，我们有 20 个客户端，默认情况下设置 ρ = 1,ρ: client joining ratio。</p>
<p>病理异构设置:</p>
<p>每个客户的10/10/100个类别中抽取了MNIST/Cifar10/Cifar100的2/2/10个类别，数据样本不相交</p>
<p>实际的异质环境:</p>
<p>它由Dirichlet分布控制，表示为Dir（β）。β越小，设置就越异构。我们为默认的异构设置设置β=0.1</p>
<p>我们使用与pFedMe相同的评估指标，它报告了传统FL的最佳单个全局模型的测试精度和pFL的最佳局部模型的平均测试精度。为了模拟实际的pFL设置，我们在客户端评估所学习的模型。25%的局部数据形成测试数据集，其余75%的数据用于训练。我们运行所有任务五次，并报告平均值和标准偏差。</p>
<p>FedALA设置为s=80。</p>
<p>通过减小超参数p，我们可以缩小ALA的范围，而精度下降可以忽略不计，如表1所示。当p从6减少到1时，ALA中可训练参数的数量也会减少，特别是从p=2减少到p=1，因为ResNet-18中的最后一个块包含了大部分参数（He等人，2016）。尽管FedALA在这里p＝2时表现最好，但我们为ResNet-18设置p＝1以减少计算开销。这也表明，<code>全局模型的较低层大多包含客户端所需的通用信息</code></p>
<p>类别（1）中的pFL方法。个性化方法表现得更好。Per-FedAvg的准确性在这些方法中是最低的，因为它只找到与所有客户的学习趋势相对应的初始共享模型，这可能无法满足单个客户的需求。在FedAvg-C/FedProx-C中微调全局模型会生成特定于客户端的本地模型，从而提高FedAvg/FedProx的准确性。然而，在像FedALA这样的本地训练中，微调只关注本地数据，而不能意识到通用信息。尽管FedRep在每次迭代时也会<code>对头部进行微调</code>，但它在微调时会冻结下载的表示部分，并将大部分通用信息保留在全局模型中，因此表现出色。然而，在客户端之间不共享头部的情况下，头部的通用信息丢失。</p>
<p>下载的表示部分通常指的是在联邦学习（或分布式学习）中，从全局模型中发送到客户端的模型参数或表示权重。这些表示部分可以包括卷积神经网络（CNN）的卷积层权重、循环神经网络（RNN）的循环权重等。即使在每次迭代时对头部进行微调，但在微调过程中冻结了下载的表示部分，这意味着在客户端进行微调时，下载的表示部分权重不会被修改。这种做法旨在保留大部分通用信息在全局模型中，从而提高模型的整体性能。然而，如果客户端之间不共享头部，这种方法可能会导致头部的通用信息丢失。具体来说，在神经网络中，头部通常指的是网络结构的顶部，包括用于特定任务的层或模块，例如分类器。这些层或模块负责将底层表示转换为最终的任务特定输出，比如对图像进行分类或对文本进行情感分析。头部的通用信息表示则指的是这些顶部层中学到的特征或模式，这些特征对于多个任务都是有用的。在联邦学习中，通过冻结表示部分并保留通用信息，可以确保在客户端的微调过程中，保留了全局模型中对多个任务都有用的通用特征表示。</p>
<p>尽管 pFedMe 和 Dititto 都使用近端项来学习它们额外的个性化模型，但 pFedMe 从局部模型中学习所需的信息，而 Dititto 从全局模型中学习它。因此，Ditto 在本地学习更通用的信息，它表现更好。然而，使用近端项学习个性化模型是提取所需信息的隐式方法。</p>
<p>使用基于规则的方法聚合模型是无目标的，无法捕捉全局模型中所需的信息，因此 FedPHP 的性能比 FedRep 和 Ditto 差。FedAMP、FedFomo 和 APPLE 中的模型级个性化聚合以及PartialFed 中的层级和二进制选择不精确，这可能会在全局模型中引入不希望的信息到局部模型。此外，在每次迭代中为每个客户端下载多个模型对于 FedFomo 和 APPLE 提供了额外的通信成本。</p>
<p>在实际异构设置中，由于每个客户端的数据分布复杂，很难衡量客户端之间的相似性。因此，FedAMP 不能通过注意力引导函数精确地为局部模型分配重要性，以生成具有个性化聚合的聚合模型。在下载全局模型/表示后，Ditto 和 FedRep 可以从中捕获通用信息，而不是测量局部模型之间的相似性。这样，在大多数任务中，它们都取得了优异的性能。可训练权重比近似权重信息量更大，因此 APPLE 的性能优于 FedFomo。</p>
<p>尽管 FedPHP 在 TINY 上表现良好，但标准偏差相对较高。由于 FedALA 可以通过 ALA 适应不断变化的环境，因此在实际设置中仍然优于所有基线。如果我们在不适应的情况下重用初始阶段学习到的聚合权重，则 TINY 的准确度下降到 33.81%。由于 ALA 的细粒度特征，它仍然优于 Per-FedAvg、pFedMe、Ditto、FedAMP 和 FedFomo。</p>
<h3 id="code">code</h3>
<p>以 Dir（0.1）为例，在默认的异构设置中上传 mnist 数据集&ndash;practical,论文中无对应</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-Json" data-lang="Json"><span class="p">{</span><span class="nt">&#34;num_clients&#34;</span><span class="p">:</span><span class="mi">20</span><span class="p">,</span><span class="nt">&#34;num_classes&#34;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="nt">&#34;non_iid&#34;</span><span class="p">:</span><span class="kc">true</span><span class="p">,</span><span class="nt">&#34;balance&#34;</span><span class="p">:</span><span class="kc">false</span><span class="p">,</span><span class="nt">&#34;partition&#34;</span><span class="p">:</span><span class="s2">&#34;dir&#34;</span><span class="p">,</span><span class="nt">&#34;Size of samples for labels in clients&#34;</span><span class="p">:[[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">140</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">890</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">319</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">29</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">1067</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">184</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">27</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">19</span><span class="p">],[</span><span class="mi">6</span><span class="p">,</span><span class="mi">335</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">107</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">143</span><span class="p">],[</span><span class="mi">6</span><span class="p">,</span><span class="mi">1461</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">23</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">155</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">2381</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">71</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">13</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">207</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">1129</span><span class="p">],[</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">40</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">451</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">38</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">39</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">25</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">6086</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">873</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">176</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">46</span><span class="p">],[</span><span class="mi">6</span><span class="p">,</span><span class="mi">42</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">13</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">106</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">21</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">11</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">787</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">441</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3599</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">633</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1997</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">89</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">519</span><span class="p">],[</span><span class="mi">6</span><span class="p">,</span><span class="mi">768</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">920</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1450</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">513</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">134</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">97</span><span class="p">]],[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">159</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3055</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">558</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">8</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">180</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3277</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">148</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">237</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">343</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">453</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">1095</span><span class="p">]],[[</span><span class="mi">5</span><span class="p">,</span><span class="mi">2719</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">3011</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">31</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1785</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">16</span><span class="p">],[</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">756</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">2856</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3628</span><span class="p">]],[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">26</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1463</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1379</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">335</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">60</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">17</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">2373</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">998</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">],[</span><span class="mi">6</span><span class="p">,</span><span class="mi">4260</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">310</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5789</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">1</span><span class="p">]]],</span><span class="nt">&#34;alpha&#34;</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span><span class="nt">&#34;batch_size&#34;</span><span class="p">:</span><span class="mi">10</span><span class="p">}</span>

<span class="err">client</span> <span class="mi">0</span> <span class="err">--</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">4</span> <span class="mi">5</span> <span class="mi">7</span> <span class="mi">8</span> <span class="mi">9</span>

<span class="err">client</span> <span class="mi">1</span> <span class="err">--</span> <span class="mi">0</span> <span class="mi">2</span> <span class="mi">5</span> <span class="mi">6</span> <span class="mi">8</span> <span class="mi">9</span>

<span class="err">client</span> <span class="mi">2</span> <span class="err">--</span> <span class="mi">0</span> <span class="mi">3</span> <span class="mi">6</span> <span class="mi">9</span>
</code></pre></td></tr></table>
</div>
</div><p>ALA的使用</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 将 ALA 模块导入为 Python 模块</span>
<span class="kn">import</span> <span class="nn">ALA</span>

<span class="c1"># 输入所需的参数以初始化模块。</span>
<span class="k">class</span> <span class="nc">Client</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="c1"># other code</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ALA</span> <span class="o">=</span> <span class="n">ALA</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">rand_percent</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># other code</span>
        
<span class="c1"># 将恢复的全局模型和旧的本地模型馈送到本地初始化。 </span>
<span class="bp">self</span><span class="o">.</span><span class="n">ALA</span><span class="o">.</span><span class="n">adaptive_local_aggregation</span><span class="p">()</span>

<span class="c1"># 在启动阶段，可能需要为ALA模块设置一个合适的threshold （我们在本文中默认设置为0.01）来控制其收敛水平</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">mnist</span><span class="o">-</span><span class="mf">0.1</span><span class="o">-</span><span class="n">npz</span> <span class="o">-</span><span class="n">m</span> <span class="n">cnn</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">mnist</span><span class="o">-</span><span class="mf">0.1</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 

<span class="c1"># -t:Running times 1</span>
<span class="c1"># -jr:Ratio of clients per round 1</span>
<span class="c1"># -nc:Total number of clients 20</span>
<span class="c1"># -nb:num_classes 10</span>
<span class="c1"># -data:dataset 数据集文件夹名 mnist-&gt;mnist-0.1-npz</span>
<span class="c1"># -m:model cnn</span>
<span class="c1"># -algo:algorithm FedALA</span>
<span class="c1"># -et: eta ALA weight学习率 1.0</span>
<span class="c1"># -p:layer_idx 2</span>
<span class="c1"># -s:rand_percent 80</span>
<span class="c1"># -did:device_id 0</span>

<span class="c1"># test</span>
<span class="c1"># -gr:global_rounds 1000-&gt;20</span>
<span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">mnist</span><span class="o">-</span><span class="mf">0.1</span><span class="o">-</span><span class="n">npz</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">20</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">model_str</span>
<span class="s1">&#39;cnn&#39;</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span>
<span class="n">FedAvgCNN</span><span class="p">(</span>
  <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 实例化clientALA</span>
<span class="bp">self</span><span class="o">.</span><span class="n">set_clients</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">clientALA</span><span class="p">)</span>

<span class="c1"># ALA初始化</span>
<span class="bp">self</span><span class="o">.</span><span class="n">ALA</span> <span class="o">=</span> <span class="n">ALA</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> 
          <span class="bp">self</span><span class="o">.</span><span class="n">rand_percent</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">cid</span>
<span class="mi">0</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
<span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">)</span>
<span class="mi">1972</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">rand_percent</span><span class="c1"># s</span>
<span class="mi">80</span>
<span class="c1">#  Control the weight range. By default, all the layers are selected. Default: 0</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="c1"># 选中的层数 后两层</span>
<span class="mi">2</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span><span class="c1"># weight 学习率</span>
<span class="mf">1.0</span>
<span class="c1"># 训练权重直到记录的损失的标准差小于给定的 阈值</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span>
<span class="mf">0.1</span>
<span class="c1"># 在计算标准差时要考虑的记录损失的数量。默认值:10</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_pre_loss</span>
<span class="mi">10</span>
<span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># 可学习的局部聚合权值</span>
<span class="bp">self</span><span class="o">.</span><span class="n">start_phase</span> <span class="o">=</span> <span class="kc">True</span> 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 下发模型</span>
<span class="bp">self</span><span class="o">.</span><span class="n">send_models</span><span class="p">()</span>
<span class="c1"># 客户端初始化</span>
<span class="n">client</span><span class="o">.</span><span class="n">local_initialization</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_model</span><span class="p">)</span>

<span class="c1"># ALA</span>
<span class="c1"># self.model = copy.deepcopy(args.model)</span>
<span class="c1"># self.global_model = copy.deepcopy(args.model)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">ALA</span><span class="o">.</span><span class="n">adaptive_local_aggregation</span><span class="p">(</span><span class="n">received_global_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># 随机抽取局部训练数据 s=80</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">rand_ratio</span>
<span class="mf">0.8</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">rand_num</span><span class="c1"># 0.8*1972</span>
<span class="mi">1577</span>
<span class="c1"># randint(0,395) 作为数据的数据切片的下标-(50,1627)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">rand_idx</span>
<span class="mi">50</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">rand_loader</span><span class="p">)</span>
<span class="mi">157</span>
<span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">0</span>
<span class="c1"># 在第一次通信迭代时停用ALA 全局模型参数和客户端模型参数一致</span>
<span class="c1"># 直接return </span>
<span class="o">-------------------------------------</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">1</span>
<span class="o">-------------------------------------</span>
<span class="bp">self</span><span class="o">.</span><span class="n">send_models</span><span class="p">()</span>
<span class="n">client</span><span class="o">.</span><span class="n">local_initialization</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_model</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">ALA</span><span class="o">.</span><span class="n">adaptive_local_aggregation</span><span class="p">(</span><span class="n">received_global_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>

<span class="n">由于每层神经网络参数包括权重和偏置</span><span class="err">，</span><span class="n">所以每一层实际上有两个参数列表</span><span class="err">。</span><span class="n">因此</span><span class="err">，</span><span class="n">即使有四层神经网络</span><span class="err">，</span><span class="n">每层都包括权重和偏置</span><span class="err">，</span><span class="n">所以总共会有</span> <span class="mi">4</span> <span class="n">层</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">=</span> <span class="mi">8</span> <span class="n">个参数列表</span><span class="err">。</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="c1"># 网络参数</span>
<span class="mi">8</span>
<span class="c1"># 保留较低层的所有更新 global model的前3层 赋给client 0 </span>
<span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]</span><span class="c1"># [:-2]-&gt;[0,5]</span>
<span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param_g</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="c1"># 临时副本,不影响原始本地模型的情况下进行一些权重学习的操作</span>
<span class="n">model_t</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">local_model</span><span class="p">)</span>
<span class="n">params_t</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model_t</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="c1"># model高层</span>
<span class="n">params_p</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">:]</span>
<span class="n">params_gp</span> <span class="o">=</span> <span class="n">params_g</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">:]</span>
<span class="n">params_tp</span> <span class="o">=</span> <span class="n">params_t</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">:]</span>

<span class="c1"># 冻结model_t低层,减少在 PyTorch 中进行计算时的计算成本。</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params_t</span><span class="p">[:</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span><span class="p">]:</span>
     <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span><span class="c1"># 参数设为不需要梯度计算</span>
        
<span class="c1"># SGD 优化器，用于获取高层参数的梯度，但不会执行参数更新步骤。</span>
<span class="c1"># 因此，学习率被设为 0，表示不会对参数进行更新。</span>
<span class="c1"># no need to use optimizer.step(),</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params_tp</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="c1"># params_tp要优化的参数列表</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
<span class="kc">None</span>
<span class="c1"># 初始化权重，全为1 params_p的格式，仅高层参数</span>
<span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params_p</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
<span class="p">[</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="c1"># 高层参数</span>
<span class="mi">2</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">512</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>

<span class="c1"># 在临时本地模型中初始化高层参数。计算临时本地模型的高层参数值。</span>
<span class="c1"># 按权重计算高层参数值 ALA的实现</span>
<span class="n">param_t</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span> <span class="o">+</span> <span class="p">(</span><span class="n">param_g</span> <span class="o">-</span> <span class="n">param</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight</span>

<span class="c1"># weight学习</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span> <span class="c1"># 在当前选取的数据集上一直训练，直至满足收敛条件跳出循环</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">rand_loader</span><span class="p">:</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span><span class="c1"># 梯度归零</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model_t</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># 根据当地目标进行修改</span>
<span class="n">loss_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="n">loss_value</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span><span class="c1"># 反向传播，得到梯度param_t.grad</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">loss_value</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">2.1265</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">NllLossBackward0</span><span class="o">&gt;</span><span class="p">)</span>

<span class="c1"># update weight in this batch </span>
<span class="c1"># 参数weight更新，类似于self.optimizer.step()</span>
<span class="c1"># torch.clamp` 函数将更新后的值限制在 [0, 1] 的范围内</span>
<span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
         <span class="n">weight</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">param_t</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="p">(</span><span class="n">param_g</span> <span class="o">-</span> <span class="n">param</span><span class="p">)),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">weight</span><span class="o">.</span><span class="n">data</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9998</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">,</span> <span class="mf">0.9979</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.9997</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9992</span><span class="p">,</span> <span class="mf">0.9998</span><span class="p">,</span> <span class="mf">0.9994</span><span class="p">],</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">0.9998</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.9959</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9997</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9929</span><span class="p">,</span> <span class="mf">0.9985</span><span class="p">,</span> <span class="mf">0.9982</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">]],</span>
       <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>

<span class="c1"># update temp local model in this batch 更新model_t的参数</span>
<span class="n">param_t</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param</span> <span class="o">+</span> <span class="p">(</span><span class="n">param_g</span> <span class="o">-</span> <span class="n">param</span><span class="p">)</span> <span class="o">*</span> <span class="n">weight</span>

<span class="c1"># 所有选中的数据训练完后，得到的loss</span>
<span class="c1"># 模型训练按batch进行，loss值会覆盖，表示最后一个batch的loss值</span>
<span class="c1"># 由于每次计算损失之后会更新模型参数，所以仅记录最后一个batch是有意义的</span>
<span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_value</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">losses</span>
<span class="p">[</span><span class="mf">1.810765266418457</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">cnt</span> <span class="c1"># weight training iteration counter</span>
<span class="mi">1</span>

<span class="c1"># 在随后的迭代中只训练一个epoch</span>
<span class="bp">self</span><span class="o">.</span><span class="n">start_phase</span> <span class="o">=</span> <span class="kc">True</span> 
<span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_phase</span><span class="p">:</span> <span class="c1">#false跳出</span>
     <span class="k">break</span>
        
<span class="c1"># train the weight until convergence</span>
<span class="c1"># 如果损失值列表的长度大于预先指定的损失值数量，</span>
<span class="c1"># 并且最近几个损失值的标准差小于预先设定的阈值，</span>
<span class="c1"># 则打印相关信息并跳出训练循环，以表示权重已经收敛。</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_pre_loss</span> <span class="ow">and</span> 
	<span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">num_pre_loss</span><span class="p">:])</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Client:&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cid</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Std:&#39;</span><span class="p">,</span> 
    	<span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">num_pre_loss</span><span class="p">:]),</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">ALA epochs:&#39;</span><span class="p">,</span> <span class="n">cnt</span><span class="p">)</span>
 	<span class="k">break</span>
    
<span class="c1"># 跳出训练</span>
<span class="bp">self</span><span class="o">.</span><span class="n">start_phase</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># obtain initialized local model 复制高层</span>
<span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">param_t</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="n">Client</span><span class="p">:</span> <span class="mi">0</span>       <span class="n">Std</span><span class="p">:</span> <span class="mf">0.04929309959898841</span>        <span class="n">ALA</span> <span class="n">epochs</span><span class="p">:</span> <span class="mi">11</span>
<span class="n">Client</span><span class="p">:</span> <span class="mi">1</span>       <span class="n">Std</span><span class="p">:</span> <span class="mf">0.09025316159724787</span>        <span class="n">ALA</span> <span class="n">epochs</span><span class="p">:</span> <span class="mi">12</span>
<span class="n">Client</span><span class="p">:</span> <span class="mi">2</span>       <span class="n">Std</span><span class="p">:</span> <span class="mf">0.062314922642027974</span>       <span class="n">ALA</span> <span class="n">epochs</span><span class="p">:</span> <span class="mi">11</span>

<span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">2</span>
<span class="o">-------------------------------------</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_phase</span>
<span class="kc">False</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
<span class="p">[</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.8682</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9655</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9862</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.8617</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9901</span><span class="p">,</span> <span class="mf">0.7574</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.7473</span><span class="p">,</span> <span class="mf">0.9997</span><span class="p">,</span> <span class="mf">0.9452</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.3092</span><span class="p">,</span> <span class="mf">0.7954</span><span class="p">,</span> <span class="mf">0.4487</span><span class="p">],</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">0.7925</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9288</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.9782</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.8773</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.9990</span><span class="p">,</span> <span class="mf">0.6782</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.9319</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.9962</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="mf">0.5651</span><span class="p">,</span> <span class="mf">0.9631</span><span class="p">,</span> <span class="mf">0.7355</span><span class="p">]],</span>
       <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">),</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.8297</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.7981</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0387</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span>
        <span class="mf">0.7162</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)]</span>

<span class="n">继续学习权重</span>

<span class="c1"># 当前 权重训练只训练一个batch</span>
<span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_phase</span><span class="p">:</span>
	<span class="k">break</span><span class="c1"># 跳出训练</span>
    
<span class="n">从第一次聚合之后</span><span class="err">，</span><span class="n">以后轮次只训练1个batch</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 评估 ---全局模型发送到client之后评估</span>
<span class="c1"># 当前轮次先评估再训练，训练后下一轮的评估是这一轮的结果</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="c1"># client训练</span>
<span class="n">client</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1">#梯度缓存清零，以确保每个训练批次的梯度都是从头开始计算的</span>
<span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
<span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 前向传播</span>
<span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># 对损失值1oss进行反向传播，计算模型参数的梯度，loss函数对于参数的梯度</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># 梯度下降，更新模型的参数，以使损失函数达到最小值</span>
<span class="c1"># 根据梯度下降算法中的优化器规则，通过调整参数的数值来最小化损失函数，以达到优化模型的目的</span>
<span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># 接收模型</span>
<span class="bp">self</span><span class="o">.</span><span class="n">receive_models</span><span class="p">()</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">active_train_samples</span>
<span class="mi">52492</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">client</span><span class="o">.</span><span class="n">id</span>
<span class="mi">7</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">client</span><span class="o">.</span><span class="n">train_samples</span>
<span class="mi">951</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_weights</span><span class="c1"># train_samples/active_train_samples</span>
<span class="p">[</span><span class="mf">0.018117046407071555</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_weights</span>
<span class="p">[</span><span class="mf">0.018117046407071555</span><span class="p">,</span> <span class="mf">0.05722776804084432</span><span class="p">,</span> <span class="mf">0.08186009296654728</span><span class="p">,</span> <span class="mf">0.07784043282785948</span><span class="p">,</span> <span class="mf">0.027375600091442506</span><span class="p">,</span> <span class="mf">0.01794559170921283</span><span class="p">,</span> <span class="mf">0.0884134725291473</span><span class="p">,</span> <span class="mf">0.030480835174883793</span><span class="p">,</span> <span class="mf">0.05160786405547512</span><span class="p">,</span> <span class="mf">0.023279737864817497</span><span class="p">,</span> <span class="mf">0.051836470319286745</span><span class="p">,</span> <span class="mf">0.03629124438009602</span><span class="p">,</span> <span class="mf">0.03756762935304427</span><span class="p">,</span> <span class="mf">0.08719423912215195</span><span class="p">,</span> <span class="mf">0.04452106987731464</span><span class="p">,</span> <span class="mf">0.0514364093576164</span><span class="p">,</span> <span class="mf">0.007124895222129086</span><span class="p">,</span> <span class="mf">0.08075516269145774</span><span class="p">,</span> <span class="mf">0.0538939266935914</span><span class="p">,</span> <span class="mf">0.07523051131601007</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">uploaded_ids</span>
<span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">18</span><span class="p">]</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 聚合模型</span>
<span class="bp">self</span><span class="o">.</span><span class="n">aggregate_parameters</span><span class="p">()</span>
<span class="n">server_param</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">client_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">*</span> <span class="n">w</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">0</span><span class="o">-------------</span>
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.3127</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0587</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.4315</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0710</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.1843</span>
<span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">1</span><span class="o">-------------</span>
<span class="n">Averaged</span> <span class="n">Train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.9177</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.8254</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9823</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.1348</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0219</span>
</code></pre></td></tr></table>
</div>
</div><hr>
<p>病态（pathological）数据异质环境pat</p>
<p>每个客户机上的数据只包含特定数量的标签</p>
<p>注意实验参数的设置</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">generate_mnist</span><span class="o">.</span><span class="n">py</span> <span class="n">noniid</span> <span class="o">-</span> <span class="n">pat</span> <span class="o">&gt;</span> <span class="n">mnist_dataset</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="p">{</span><span class="s2">&#34;num_clients&#34;</span><span class="p">:</span><span class="mi">20</span><span class="p">,</span><span class="s2">&#34;num_classes&#34;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="s2">&#34;non_iid&#34;</span><span class="p">:</span><span class="n">true</span><span class="p">,</span><span class="s2">&#34;balance&#34;</span><span class="p">:</span><span class="n">false</span><span class="p">,</span><span class="s2">&#34;partition&#34;</span><span class="p">:</span><span class="s2">&#34;pat&#34;</span><span class="p">,</span><span class="s2">&#34;Size of samples for labels in clients&#34;</span><span class="p">:[[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1233</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1101</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">407</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">911</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1268</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1865</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3995</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">4000</span><span class="p">]],[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1021</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">307</span><span class="p">]],[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1134</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">927</span><span class="p">]],[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">318</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">686</span><span class="p">]],[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4517</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5221</span><span class="p">]],[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1584</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">1457</span><span class="p">]],[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1475</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">1435</span><span class="p">]],[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1372</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">514</span><span class="p">]],[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">2393</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">2907</span><span class="p">]],[[</span><span class="mi">6</span><span class="p">,</span><span class="mi">1085</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">434</span><span class="p">]],[[</span><span class="mi">6</span><span class="p">,</span><span class="mi">639</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">1696</span><span class="p">]],[[</span><span class="mi">6</span><span class="p">,</span><span class="mi">1078</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">850</span><span class="p">]],[[</span><span class="mi">6</span><span class="p">,</span><span class="mi">4074</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">4313</span><span class="p">]],[[</span><span class="mi">8</span><span class="p">,</span><span class="mi">568</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">1412</span><span class="p">]],[[</span><span class="mi">8</span><span class="p">,</span><span class="mi">732</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">1174</span><span class="p">]],[[</span><span class="mi">8</span><span class="p">,</span><span class="mi">750</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">926</span><span class="p">]],[[</span><span class="mi">8</span><span class="p">,</span><span class="mi">4775</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">3446</span><span class="p">]]],</span><span class="s2">&#34;alpha&#34;</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span><span class="s2">&#34;batch_size&#34;</span><span class="p">:</span><span class="mi">10</span><span class="p">}</span>

<span class="n">client</span> <span class="mi">0</span> <span class="p">:</span><span class="mi">0</span> <span class="mi">1</span>
<span class="n">client</span> <span class="mi">1</span> <span class="p">:</span><span class="mi">0</span> <span class="mi">1</span> 
<span class="o">...</span>
<span class="n">clinet</span> <span class="mi">4</span> <span class="p">:</span><span class="mi">2</span> <span class="mi">3</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">mnist</span> <span class="o">-</span><span class="n">m</span> <span class="n">cnn</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">2000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">mnist</span><span class="o">-</span><span class="n">pat</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 

<span class="mf">0.9989147198263552</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">generate_tiny_imagenet</span><span class="o">.</span><span class="n">py</span> <span class="n">noniid</span> <span class="o">-</span> <span class="nb">dir</span> <span class="o">&gt;</span> <span class="n">tiny_dataset</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">200</span> <span class="o">-</span><span class="n">data</span> <span class="n">Tiny</span><span class="o">-</span><span class="n">imagenet</span> <span class="o">-</span><span class="n">m</span> <span class="n">cnn</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">3</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">2000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="nb">dir</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="fedcp--kdd2023">FedCP -KDD2023</h2>
<p>通过条件策略分离个性化联邦学习的特征信息、</p>
<blockquote>
<p><a href="https://readpaper.com/paper/4774517374925668353">paper</a></p>
<p><a href="https://github.com/TsingZ0/FedCP">code</a></p>
<p><a href="https://mp.weixin.qq.com/s/N9h16GPTNg08VKbOYAgNuw">blog</a></p>
</blockquote>
<p>大多数现有的 pFL 方法将全局模型视为存储全局信息的容器，并使用全局模型中的参数丰富个性化模型。然而，它们只关注客户端级别的模型参数，即全局/个性化模型来利用全局/个性化信息。具体来说，基于元学习的方法(如Per-FedAvg[8])只微调全局模型参数以适应本地数据，而正则化方法(如pFedMe[42]、FedAMP[15]和Ditto[24])只在局部训练过程中正则化模型参数。尽管基于个性化头的方法（例如 FedPer [2]、FedRep [6] 和 FedRoD [4]）明确地将主干拆分为全局部分（特征提取器）和个性化部分（头），但它们仍然专注于利用模型参数中的全局和局部信息，而不是信息来源：数据。</p>
<p>由于模型是在数据上训练的，因此模型参数中的全局/个性化信息是从客户端数据中导出的。换句话说，客户端的异构数据包含全局信息和个性化信息。如图1所示，广泛使用的颜色，如蓝色，以及很少使用的颜色（如紫色和粉色）分别包含图像中的全局信息和个性化信息。𝑾ℎ𝑑 : 冻结的全局头，𝑾ℎ𝑑 𝑖/𝑗 : 个性化的头部。</p>
<p><img src="/images/2024/5.png" alt=""></p>
<p>为了分别利用数据中的全局信息和个性化信息，我们提出了一种基于条件计算技术的联邦条件策略（FedCP）方法[11，35]。由于原始输入数据的维数远大于特征提取器提取的特征向量，因此为了提高效率，我们将重点放在特征向量上。由于全局信息和个性化信息在特征中的比例在样本和客户端之间不同，我们提出了一个<code>辅助条件策略网络（CPN）来生成用于特征信息分离的样本特定策略</code>。然后，我们分别通过全局头和个性化头在不同的路径上处理全局特征信息和个性化特征信息，如图1所示。<code>我们将个性化信息存储在个性化头部中，并通过冻结全局头部来保留全局信息</code>，而无需对其进行本地训练。通过端到端学习，CPN自动学习生成样本特定策略。</p>
<p>贡献：</p>
<ul>
<li>我们是第一个考虑FL中样本特定特征信息的个性化。它比大多数现有FL方法中使用客户端级模型参数更细粒度。</li>
<li>我们提出了一种新的FedCP，它生成一个特定于样本的策略，以在每个客户端的特征中分离全局信息和个性化信息。它分别通过每个客户端上的冻结全局头和个性化头来处理这两种特征信息。</li>
<li>此外，即使某些客户端意外退出，FedCP也能保持其卓越的性能</li>
</ul>
<p>个性化联邦</p>
<ul>
<li>在基于元学习的方法中，Per-FedAvg[8]学习初始共享模型作为全局模型，满足每个客户端的学习趋势。</li>
<li>在基于正则化的方法中，pFedMe [42] 使用 Moreau 包络为每个客户端在本地学习一个额外的个性化模型。除了只为所有客户端学习一个全局模型外，FedAMP[15]还通过注意诱导函数为一个客户端生成一个服务器模型，以找到相似的客户端。在Ditto[24]中，每个客户端使用近端项在本地学习其个性化模型，以从全局模型参数中获取全局信息。、</li>
<li>在基于个性化头部的方法中，FedPer[2] 和 FedRep [6] 学习了一个全局特征提取器和一个特定于客户端的头。前者使用特征提取器在本地训练头部，而后者在每次迭代训练特征提取器之前局部微调头部直到收敛。为了弥合传统的 FL 和 pFL，FedRoD [4] 使用全局特征提取器和两个头显式学习两个预测任务。它使用平衡的 softmax (BSM) 损失 [39] 进行全局预测任务，并通过个性化头部处理个性化任务。</li>
<li>在其他 pFL 方法中，FedFomo [56] 使用来自其他客户端的个性化模型计算每个客户端聚合的特定于客户端的权重。FedPHP[27]使用移动平均局部聚合全局模型和旧的个性化模型，以保持历史个性化信息。它还通过广泛使用的最大平均差异 (MMD) 损失 [10, 37] 传递全局特征提取器中的信息。</li>
<li>上述 pFL 方法只专注于利用模型参数的全局和个性化信息，但不深入挖掘数据。</li>
</ul>
<p><img src="/images/2024/6.png" alt=""></p>
<p>图2（a）CPN模块(红色圆角矩形):样本特征信息𝒉𝑖和客户机信息𝒗𝑖，能输出对应的策略向量（𝒓𝑖和𝒔𝑖）</p>
<p>菱形则表示特征信息分离操作，使用条件策略将信息𝒉𝑖通过红色菱形分离为 𝒓𝑖 ⊙ 𝒉𝑖 和𝒔𝑖 ⊙ 𝒉𝑖 。用策略向量即可提取得到全局特征信息𝒓𝑖 ⊙ 𝒉𝑖 和个性化特征信息𝒔𝑖 ⊙ 𝒉𝑖</p>
<p>然后交由全局头部𝑾ℎ𝑑 和个性化头部𝑾ℎ𝑑𝑖 分别处理。最后将输出合并（即加和），得到最终输出值</p>
<p>除了特征向量和向量𝒗𝑖 , 标准矩形和圆形矩形分别表示层和模块。</p>
<p>带虚线边框的圆角矩形 𝑾^ℎ𝑑𝑖 在等式（6）中</p>
<p>𝑾𝑓𝑒 （灰色边界）不是个性化模型的一部分，数据只在训练过程中向前流动。在训练过程中，数据在所有线中流动，但在推理过程中，数据只在实线中流动。</p>
<p><img src="/images/2024/7.png" alt=""></p>
<p>(b)分别显示了特征提取器、头部和CPN的上传和下载流.我们在实践中上传或下载它们作为服务器和每个客户端之间的联合</p>
<p>feature extractors是指用来提取输入数据特征的部分，通常是指卷积神经网络（CNN）中的卷积层和池化层，用来提取输入数据的特征表示。而 “the heads” 通常指的是在深度学习模型中负责执行最终任务（如分类、回归等）的部分，通常是指全连接层（也称为密集层）或输出层。 “the heads” 会接收从&quot;feature extractors&quot;提取的特征，并根据特定的任务进行最终的预测或输出。</p>
<p>条件计算</p>
<p>条件计算是一种根据任务相关的条件输入将动态特性引入模型的技术[11，30，35]。形式上，给定条件输入𝐶 （例如，图像/文本、模型参数矢量或其他辅助信息）和辅助模块𝐴𝑀 (·;𝜃 ), 一个信号𝑆 可以由𝑆 = 𝐴𝑀 (𝐶; 𝜃 ) 生成，并且用于干扰诸如动态路由和特征自适应之类的模型。</p>
<hr>
<p>该论文提出了一种用于个性化联邦学习的全局和个性化特征信息分离方法FedCP，<code>首次在数据层面实现了全局和个性化信息的分离</code>，为分别处理这两类信息提供了可能。</p>
<p><img src="/images/2024/3.png" alt=""></p>
<p>第一行为样本图片，第二行为全局特征信息，第三行为个性化特征信息。</p>
<p>在实际场景下，由于各个客户机上数据的异质性，每一轮上传到服务器上的客户机模型参数之间具有较大差异，聚合得到的全局模型无法在单个客户机上具有良好的表现。于是研究者们提出个性化联邦学习方法，将学习全局模型的目标，转变为通过全局模型辅助个性化本地模型训练。</p>
<p>在数据异质的情况下进行协同训练，既要考虑个性化（用于应对异质性）又要考虑全局性（用于协同训练）。如何把握全局和个性化这两者之间的关系，是设计个性化联邦学习方法的关键。</p>
<p>大多数现有的个性化联邦学习方法，仅仅从<code>模型参数</code>层面对全局信息和个性化信息进行分离和分别利用（见图4,Per-FedAvg,Ditto,FedRep），却忽略了模型参数中的信息是从数据中学到的这个事实。</p>
<p><img src="/images/2020/4.png" alt=""></p>
<p>虽然每个客户机上的数据是异质的，但异质数据也是在同一个世界中产生的，或多或少都具有一部分全局信息和另一部分个性化信息.</p>
<p>不同客户机上的异质数据示意图，其中蓝色代表全局信息，紫色和粉色代表个性化信息</p>
<p><strong>从数据中分离和利用全局和个性化信息</strong></p>
<p>由于输入空间的数据维度较高，我们便考虑对转换后的特征向量进行处理。如图6所示，在特征空间中，我们通过设计一个辅助的条件策略网络（CPN）来实现特征信息的分离；随后我们利用全局和个性化头部来分别处理分离出来的两类信息。</p>
<p>圆角矩形上的斜线代表“冻结”（即不参与训练、不作更新）菱形则表示特征信息分离操作。半透明的模块表示只在训练过程中参与。实现特征分离的关键在于CPN模块。</p>
<p>&ndash;</p>
<p>个性化特征分离主要通过以下方法和步骤实现：</p>
<ol>
<li>使用联邦条件策略（Federated Conditional Policy，FedCP）方法：FedCP生成一个样本特定的策略，将每个客户端的特征分为全局特征信息和个性化特征信息。这两种特征信息分别由全局头和个性化头处理。</li>
<li>分离特征信息：在FedCP中，通过将策略{α, β}与特征向量x相乘，可以分别获得全局特征信息（x⊙α）和个性化特征信息（x⊙β）。由于特征之间存在连接，所以输出{α, β}，而不是布尔值，即α∈(0, 1)且β∈(0, 1)。</li>
<li>生成样本特定策略：FedCP使用辅助的条件策略网络（Conditional Policy Network, CPN）生成样本特定的策略，以实现特征信息的分离。通过端到端学习，CPN将自动学习生成样本特定策略。</li>
<li>保存个性化信息：FedCP通过在每个客户端冻结全局头不进行本地训练，以保留全局信息。同时，它通过个性化头存储个性化信息。</li>
<li>实验验证：FedCP在各种数据集上的实验表现优于其他个性化联邦学习方法，证明了其有效性。</li>
</ol>
<p>总之，通过FedCP方法和CPN模块，个性化特征分离通过为每个样本生成特定策略，实现了全局特征信息和个性化特征信息的有效分离。</p>
<hr>
<p>论文</p>
<p><a href="https://mp.weixin.qq.com/s/-gBQbo5rUD_h9Mv3hHRzeA">https://mp.weixin.qq.com/s/-gBQbo5rUD_h9Mv3hHRzeA</a> iccv</p>
<p><a href="https://mp.weixin.qq.com/s/1XWGZZa5WIsgsuL7BW1Wyg">https://mp.weixin.qq.com/s/1XWGZZa5WIsgsuL7BW1Wyg</a></p>
<p><a href="https://mp.weixin.qq.com/s/N9h16GPTNg08VKbOYAgNuw">https://mp.weixin.qq.com/s/N9h16GPTNg08VKbOYAgNuw</a> kdd</p>
<p>医学个性化联邦</p>
<p>Model-Heterogeneous Semi-Supervised Federated Learning for Medical Image Segmentation</p>
<p>HPFL: hyper-network guided personalized federated learning for multi-center</p>
<p>Specificity-Aware Federated Graph Learning for Brain Disorder Analysis with Functional MRI</p>
<p>A Federated Deep Learning Method for Chronic Disease Diagnosis</p>
<p>FedCE：基于客户端贡献估计的公平联邦医学图像分割</p>
<p>FedRH: Federated Learning Based Remote Healthcare</p>
<p>Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis</p>
<p>GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging</p>
<p>AP2FL: Auditable Privacy-Preserving Federated Learning Framework for Electronics in Healthcare</p>
<p>Personalized federated learning for the detection of COVID-19</p>
<p>Adaptive channel-modulated personalized federated learning for magnetic resonance image reconstruction</p>
<p>Federated hospital: a multilevel federated learning architecture for dealing with heterogeneous data distribution in the context of smart hospitals services</p>
<p>Feddp: Dual personalization in federated medical image segmentation</p>
<p>FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</p>
<p>FedDK: Improving Cyclic Knowledge Distillation for Personalized Healthcare Federated Learning</p>
<p>Medical Federated Model with Mixture of Personalized and Sharing Components</p>
<p>Performance Analysis of Personalized Federated Learning Algorithms for Image Classification</p>
<p>&ndash;</p>
<p><strong>FedFTN:</strong> Personalized federated learning <strong>with deep feature transformation network for multi-institutional low-count PET denoising.</strong></p>
<p>Personalized Federated Learning <strong>for</strong> Medical <strong>Segmentation using Hypernetworks.</strong> <code>iclr</code></p>
<p><strong>GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging.</strong></p>
<p>Personalized <strong>Retrogress-Resilient</strong> Federated Learning <strong>Toward Imbalanced</strong> Medical <strong>Data.</strong></p>
<p><strong>Personalized Retrogress-Resilient Framework for Real-World Medical Federated Learning.</strong></p>
<p>Personalized <strong>Retrogress-Resilient Framework for Real-World</strong> Medical Federated Learning**.**</p>
<p><a href="https://dblp.uni-trier.de/search?q=personali%20federated%20learning">个性化联邦</a></p>
<hr>
<p>Data-Free Federated Learning <a href="https://zhuanlan.zhihu.com/p/669527738">blog</a></p>
<p>以下是一些近期在医疗数据分析领域的联邦学习相关论文推荐：</p>
<ol>
<li>FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy
该论文提出了一种名为FedCP的方法，通过条件策略分离特征信息，实现个性化联邦学习。FedCP在多个数据集和场景下的实验表现优于现有的11种先进方法。</li>
<li>Practical Challenges in Differentially-Private Federated Survival Analysis of Medical Data
这篇论文探讨了在医疗数据分析中进行联邦生存分析的实际挑战，并提出了一种名为DPFed-post的方法，通过在联邦学习过程中加入后处理阶段，提高模型的收敛速度和性能。</li>
<li>Open problems in medical federated learning
这篇论文讨论了医疗联邦学习中的一些开放性问题，包括隐私保护、数据异质性、模型压缩和鲁棒性等方面。</li>
</ol>
<hr>
<p>医学联邦</p>
<p>FedA3 I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation Against Heterogeneous Annotation Noise</p>
<p><a href="https://blog.csdn.net/qq_51393239/article/details/135148553">blog</a> 无代码 客户端标注噪声 每个客户端的噪声估计是通过高斯混合模型完成的，然后以层状方式纳入模型聚合中，以增加高质量客户端的权重</p>
<p>FedSODA 无代码 用于组织病理学细胞核和组织分割</p>
<p><a href="https://conferences.miccai.org/2022/papers/">https://conferences.miccai.org/2022/papers/</a></p>
<p><a href="https://conferences.miccai.org/2023/papers/">https://conferences.miccai.org/2023/papers/</a></p>
<p>FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-aware Aggregation</p>
<p>无代码 基于中心内部和跨中心的局部原型特征的对比学习框架来增强本地模型更新过程中的特征表达一种简单但十分有效的进程感知模型融合算法，可以有效缓解系统异质导致的落后</p>
<p>Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning跨域</p>
<p>Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging 我们开发了一种新的不确定性感知加权模块(UAW)，可以根据每个客户端的不确定性评分分布动态调整模型聚合的权重。无代码</p>
<p>FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation通过计算局部模型之间的亲和度，探索局部模型之间的内在关联，从而提高聚合权值。模型聚合 无代码</p>
<p>FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification <a href="https://github.com/wnn2000/FedIIC">code</a>在特征学习中，设计了两个层次的对比学习，以便在FL中对不平衡数据提取更好的类特定特征。在分类器学习中，根据实时难度和类先验动态设置每类边际，帮助模型平等地学习类</p>
<p>FedSoup: Improving Generalization and <code>Personalization</code> in Federated Learning via Selective Model Interpolation <a href="https://github.com/ubc-tea/FedSoup">code</a> 我们提出了一种新的联邦模型汤方法(即模型参数的选择性插值)来优化局部和全局性能之间的权衡。具体来说，在联邦训练阶段，每个客户机通过监视局部模型和全局模型之间的内插模型的性能来维护自己的全局模型池。这使我们能够缓解过拟合并寻求平坦最小值，这可以显着提高模型的泛化性能</p>
<p>FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling <a href="https://github.com/faresmalik/FeSViBS">code</a>联邦分离学习 ，并引入了一个块采样模块，该模块利用了服务器上VisionTransformer (ViT)提取的中间特征</p>
<p>Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis 无代码</p>
<p>GRACE: A Generalized and <code>Personalized</code> Federated Learning Method for Medical Imaging泛化和个性化联邦学习 <a href="https://github.com/MediaBrain-SJTU/GPFL-GRACE">code</a>GRACE在客户端的元学习框架下结合了特征对齐正则化，以纠正过度拟合的个性化梯度。同时，GRACE采用一致性增强的重加权聚合在服务器端校准上传的梯度，以实现更好的泛化。</p>
<p>One-shot Federated Learning on Medical Data using Knowledge Distillation with Image Synthesis and Client Model Adaptation <a href="https://github.com/myeongkyunkang/FedISCA">code</a>客户端模型自适应知识提取的医学数据一次联邦学习</p>
<p>Rethinking Semi-Supervised Federated Learning: How to co-train fully-labeled and fully-unlabeled client imaging data 无代码 半监督联邦学习</p>
<p>Scale Federated Learning for Label Set Mismatch in Medical Image Classification <a href="https://github.com/dzp2095/FedLSM">code</a> 解决标签集不匹配 不同不确定程度的数据采用不同的训练策略，有效利用未标记或部分标记的数据，并在分类层采用分类自适应聚合，避免客户端缺少标签时的不准确聚合</p>
<hr>
<h2 id="fedsoup">fedsoup</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">100</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">|</span> <span class="n">tee</span> <span class="o">../</span><span class="n">tmp</span><span class="o">/</span><span class="n">tiny_camelyon17</span><span class="o">/</span><span class="n">fedsoup_debug_console</span><span class="o">.</span><span class="n">output</span> 

<span class="c1"># -eg: Rounds gap for evaluation</span>
<span class="c1"># -nc:client</span>
<span class="c1"># -hoid：for out-of-federation evaluation Hold-out out-of-federated evaluation set. 1e8 means no hold-out set</span>
<span class="c1"># -lr：</span>
<span class="c1"># -wa_alpha：FedSoup soup wa alpha Weight averaging ratio of personalized global model pool for FedSoup</span>

<span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">20</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">1</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">100</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="mi">4</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 
<span class="mf">0.8973214285714286</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_mg5_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">--</span><span class="n">pruning</span> <span class="o">--</span><span class="n">sparsity_ratio</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">pruning_warmup_round</span> <span class="mi">500</span> <span class="o">--</span><span class="n">masking_grad</span> <span class="o">--</span><span class="n">dynamic_mask</span> <span class="o">|</span> <span class="n">tee</span> <span class="o">../</span><span class="n">tmp</span><span class="o">/</span><span class="n">tiny_camelyon17</span><span class="o">/</span><span class="n">fedsoup_mg5_debug_console</span><span class="o">.</span><span class="n">output</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">2</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_mg5_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">--</span><span class="n">pruning</span> <span class="o">--</span><span class="n">sparsity_ratio</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">pruning_warmup_round</span> <span class="mi">500</span> <span class="o">--</span><span class="n">masking_grad</span> <span class="o">--</span><span class="n">dynamic_mask</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 
<span class="mf">0.9071428571428571</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_mg5_debug_2</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">--</span><span class="n">pruning</span> <span class="o">--</span><span class="n">sparsity_ratio</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">pruning_warmup_round</span> <span class="mi">500</span> <span class="o">--</span><span class="n">masking_grad</span> <span class="o">--</span><span class="n">dynamic_mask</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 
<span class="mf">0.9042857142857142</span>


<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">200</span> <span class="o">-</span><span class="n">data</span> <span class="n">Tiny</span><span class="o">-</span><span class="n">imagenet</span> <span class="o">-</span><span class="n">m</span> <span class="n">cnn</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">3</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">2000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="nb">dir</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">2</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">3</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 
<span class="mf">0.8985714285714286</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.8736</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9395</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">2</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">3</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="mi">16001</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 
<span class="mf">0.8864285714285715</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.8607</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9283</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">go</span> <span class="n">tiny</span><span class="o">-</span><span class="n">cam</span><span class="o">-</span><span class="n">fedala</span><span class="o">-</span><span class="n">lbs10</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">10</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">2</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="mi">10001</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.8307</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9124</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">go</span> <span class="n">tiny</span><span class="o">-</span><span class="n">cam</span><span class="o">-</span><span class="n">fedala</span><span class="o">-</span><span class="n">lbs16</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span>  <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">2</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">1</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="mi">16</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.8786</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.9423</span>
</code></pre></td></tr></table>
</div>
</div><p>表1 ：FedSoup 85.71(0.37) 92.47(0.31) 72.87(1.35) 81.45(1.40)</p>
<p>对应result-tiny-came-4-npz.out hold0</p>
<p>Local Performance:
Local Client-Equally Test Accurancy: 0.8777
Local Client-Equally Test AUC: 0.9438
Global Performance:
Glocal Client-Equally Test Accurancy: 0.7325
Glocal Client-Equally Test AUC: 0.8042</p>
<p>表2 tta微调</p>
<p>FedSoup 72.87(1.35) 81.45(1.40) 85.36(0.86) 88.86(1.07)</p>
<p>没有</p>
<p>表3  (DG) 的其他结果&ndash;OOF</p>
<p>71.97 79.63</p>
<p>OOF Client Test Accurancy: 0.5234
OOF Client Test AUC: 0.6048</p>
<p>表4 no hoid on</p>
<p>FedSoup 89.20(0.53) 95.08(0.61) 86.44(1.12) 93.46(0.74)</p>
<p>对应result-tiny-came-npz.out和result-tiny-came-2-npz.out</p>
<p>Local Performance:
Local Client-Equally Test Accurancy: 0.8964
Local Client-Equally Test AUC: 0.9482
Global Performance:
Glocal Client-Equally Test Accurancy: 0.8628
Glocal Client-Equally Test AUC: 0.9311</p>
<p>Performance Summarizing&hellip;
Local Performance:
Local Client-Equally Test Accurancy: 0.9050
Local Client-Equally Test AUC: 0.9613
Global Performance:
Glocal Client-Equally Test Accurancy: 0.8681
Glocal Client-Equally Test AUC: 0.9387</p>
<hr>
<p><code>研究实验超参数和结果</code></p>
<p>第一项任务涉及使用Camelyon17数据集[1]对来自五个不同来源的病理学图像进行分类，每个来源都被视为客户端。病理学实验共包括4600张图像张，每张图像的分辨率为96×96。我们从原始的 Camelyon17 数据集中取一个随机子集来匹配 FL[18] 中的小数据设置。</p>
<p>Tiny Camelyon 17（TUPAC-2）数据集是一种用于乳腺组织分类的医学影像数据集。该数据集是从肿瘤组织切片图像中提取的，用于研究和开发计算机辅助诊断（CAD）系统和深度学习模型。这个数据集主要用于乳腺癌研究领域，可用于训练和测试医学图像分析算法，以辅助医生诊断乳腺癌。<a href="https://camelyon17.grand-challenge.org/Data/">data</a></p>
<p>第二项任务涉及来自四个不同机构的视网膜眼底图像[9，26，21]，每个机构都被视为客户。视网膜眼底实验共包括1264张图像，每张图像的分辨率为128×128。这两个数据集的目标是从正常图像中识别异常图像。</p>
<p>Retinal Fundus-RFMiD</p>
<p>对于每个客户端，我们将75%的数据作为训练集。为了评估我们模型的泛化能力和个性化，我们构建了局部和全局测试集。</p>
<p>在[28]之后，在我们的实验环境中，我们首先通过每个来源/研究所随机采样相同数量的图像来创建一个保持的<code>全局测试集</code>，因此其分布与任何一个客户端都不同。</p>
<p>每个FL客户端的本地测试数据集是来自与其训练集相同来源的剩余样本。每个客户端的本地测试集数量与保留的全局测试集数量大致相同。对于病理学数据集，由于每个受试者可以有多个样本，我们已经确保来自同一受试者的数据只出现在训练或测试集中。</p>
<p>为了与交叉验证设置保持一致，用于后续的域外评估，我们进行了五折留一客户端数据交叉验证，每个折叠中使用不同的随机种子进行三次重复。每次使用不同的随机种子重复三次。</p>
<p>这句话意思是&quot;我们进行了一次五折留一客户数据交叉验证&rdquo;。这种交叉验证方法将数据集分成五个部分，在每次验证中，<code>将其中一个客户的数据作为验证集</code>，其余客户的数据作为训练集，以此来评估模型的性能。</p>
<p><code>附录中提供了没有保留客户数据的重复实验的结果</code>。对于PFL方法，我们通过平均每个个性化模型的结果来报告性能。</p>
<p>模型和训练超参数。我们采用ResNet-18体系结构作为主干模型。我们的方法在75%的训练阶段启动局部全局插值，与SWA的默认超参数设置一致。我们使用Adam优化器，学习率为1e−3，动量系数为0.9和0.99，并将批量大小设置为16。我们将本地训练epoch设置为1，并执行总共1000轮通信。</p>
<hr>
<p><strong><code>compute_hessian_eigenthings</code></strong> 是一个包含计算Hessian矩阵特征值和特征向量的函数的库。主要作用是在优化算法中用于评估损失函数的二阶导数信息。通过计算Hessian矩阵的特征值和特征向量，可以更好地理解优化空间的形状，并为优化算法提供更准确的方向信息，从而加快模型收敛速度。<a href="https://maimai.cn/article/detail?fid=1321848261&amp;efid=iffeICNH1Tup8FaAmcUpyg">blog</a> <a href="https://github.com/noahgolmant/pytorch-hessian-eigenthings">code</a></p>
<p>pip install &ndash;upgrade git+https://github.com/noahgolmant/pytorch-hessian-eigenthings.git@master</p>
<p>pip install &ndash;upgrade git+https://github.com/noahgolmant/pytorch-hessian-eigenthings.git@dce2e54a19963b0dfa41b93f531fb7742d46ea04</p>
<p><strong><code>gpytorch</code></strong> 是一个基于 PyTorch 的高斯过程库，用于概率编程和高斯过程模型的构建。它提供了在 PyTorch 框架下构建、训练和推断高斯过程模型所需的工具和功能。</p>
<p>AttributeError: module &lsquo;argparse&rsquo; has no attribute &lsquo;BooleanOptionalAction&rsquo;&ndash;python3.9以上</p>
<hr>
<p>windows下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Pillow_SIMD==9.0.0.post1
netifaces==0.11.0
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">conda create -n fedsoup python=3.9 --y

conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre></td></tr></table>
</div>
</div><hr>
<h3 id="paper">paper</h3>
<p><code>abstract</code></p>
<p>跨孤岛fl，当面对分布变化时，当前的FL算法面临着局部和全局性能之间的权衡。具体来说，个性化FL方法有过度拟合局部数据的倾向，导致局部模型出现陡谷，抑制了其推广到分布外数据的能力。我们提出了一种新的联邦模型汤方法(即模型参数的选择性插值)来优化局部和全局性能之间的权衡。具体来说，在联邦训练阶段，每个客户机通过监视局部模型和全局模型之间的内插模型的性能来维护自己的全局模型池。这使我们能够缓解过拟合并寻求平坦最小值，这可以显着提高模型的泛化性能。视网膜和病理图像分类</p>
<p><code>introduction</code></p>
<p>最近的研究[28]发现了当前FL算法的一个重要问题，即当遇到分布变化时，局部和全局性能之间的权衡。个性化FL (PFL)技术通常通过对每个客户机的分布内(ID)数据施加更多权重来解决数据异构问题。例如，FedRep[5]在本地更新期间学习整个网络，并使部分网络免于全局同步。然而，它们有过拟合本地数据的风险[23]，特别是当客户端本地数据有限时，并且对out- distribution (OOD)数据的泛化性较差。另一项工作是通过规范局部模型的更新来研究异构性问题。例如，FedProx[15]约束本地更新更接近全局模型。评估FL泛化性的一种有效方法是研究其在联合全局分布上的性能[28]，这是指在⋃{Di}上测试FL模型，其中Di表示客户端i的分布4。遗憾的是，现有的研究还没有找到个性化(局部)和共识(全局)模型之间的平衡点</p>
<p>Dj is viewed as the OOD data for client i.</p>
<p>为此，我们的目标是解决 FL 中的以下两个问题：可能导致局部和全局权衡的原因是什么。以及如何实现更好的局部和全局权衡。我们发现 FL 中的过度个性化会导致对本地数据的过度拟合，并将模型捕获到损失景观的尖锐山谷中（对参数扰动高度敏感，参见第 2.2 节中的详细定义），从而限制了其泛化性。避免损失景观中尖锐山谷的有效策略是强制模型获得平坦的最小值。在集中式领域，<code>权重插值</code>weight interpolation已被探索为寻找平坦最小值的一种手段，因为它的解决方案更接近高性能模型的质心，这对应于更平坦的最小值 [11,3,6,24]。然而，对这些插值方法的研究在FL中被忽略了。</p>
<p>在此基础上，我们建议在联合训练过程中跟踪局部模型和全局模型，并执行模型插值来寻求最优平衡。我们的见解是从模型汤方法[27]中提取的，这表明具有相同初始参数的多个训练模型的平均权重可以增强模型的泛化能力。然而，原始模型汤方法需要训练具有不同超参数的大量模型，这在 FL 期间的通信方面可能非常耗时且成本高昂。鉴于 FL 中的通信成本高且无法从头开始训练，我们利用<code>单个训练会话中不同时间点的全局模型作</code>为使模型汤方法 [27] 适应 FL 的成分。</p>
<p>在本文中，我们提出了一种新的联邦模型汤方法（FedSoup），以从局部和全局模型中生成集成模型，从而实现更好的局部-全局权衡。我们将<code>“soup”称为不同联邦模型</code>的组合。我们提出的FedSoup包括两个关键模块。第一种是<code>时间模型选择</code>，旨在选择合适的模型组合成一个模型。第二个模块是<code>联邦模型修补</code>[10]，它指的是一种微调技术，旨在增强个性化，而不影响已经令人满意的全局性能</p>
<p>“temporal model selection” 意味着在时间序列数据或其他时间相关数据集上进行模型选择。这可能涉及在不同的时间点上选择不同的模型或参数配置，以适应数据随时间变化的特性。这种方法有助于构建更具有适应性和泛化能力的模型，以更好地应对时间序列数据的特点。</p>
<p>对于第一个模块，时间模型选择，我们<code>使用了基于局部验证性能的贪婪模型选择策略</code>。避免将可能位于不同误差景观盆地的模型合并到本地的损失景观中（如图1所示）。因此，每个客户都拥有他们的个性化全局模型汤，由基于其本地验证集选择的<code>历史全局模型的子集</code>组成。</p>
<p>&ldquo;景观&quot;在这里指的是指代模型参数空间中的拓扑结构或形状。在机器学习领域，误差景观通常用来描述损失函数在参数空间中的形状，以及模型在该空间中移动时损失值的变化情况。</p>
<p><img src="/images/2024/8.png" alt=""></p>
<p>图1：PFL方法通常最小化局部损失，但存在较高的全局损失。而我们的联邦模型汤方法通过寻求平坦极小值来平衡局部和全局损失。图中的黑点表示省略号，表示其间的多轮模型上传和模型训练。与以前的pFL方法相比，我们的方法引入了全局模型选择模块和全局模型与局部模型进行插值（称为模型修补）。</p>
<p>“seeking flat minima” 意味着在机器学习中寻找“平坦的最小值”。在训练神经网络时，有时候希望模型收敛到一个平坦的局部最小值，而不是一个非常陡峭的局部最小值。这是因为平坦的最小值可能对噪声更具鲁棒性，有助于提高模型的泛化能力。</p>
<p>“local model interpolation with the global model” 意味着在机器学习中，将局部模型与全局模型进行插值。这可能涉及将局部训练的模型与全局模型进行融合或插值，以获得更具泛化能力的模型。这种方法有助于平衡全局和局部模型的性能，以获得更好的整体性能。</p>
<p>第二个模块，联邦模型修补，它通过将局部模型和全局模型汤插入到新的局部模型中，在局部客户端训练中引入模型修补，弥合局部域和全局域之间的差距。它促进了ID测试模型的个性化，并为OOD泛化保持了良好的全局性能。</p>
<p>（i）提出了一种新的FL方法，称为联邦模型流（FedSoup），通过提高平滑度和寻求平坦极小值来提高泛化和个性化。（ii）为FL设计了一种新的时间模型选择机制，该机制维护了具有时间历史全局模型的客户特定模型汤，以满足个性化要求，同时不产生额外的训练成本。（iii）在联合客户端训练中引入了一种创新的局部和全局模型之间的联合模型修补方法，以缓解局部有限数据的过拟合。</p>
<p><code>method</code></p>
<p>个性化的目的是最小化本地客户端训练集 Di 上的经验loss，<img src="/images/2024/9.png" alt=""></p>
<p>泛化（全局性能）的目标是通过所有训练客户的训练集 D 上的经验loss最小化 (ERM) 来最小化多both population损失 ED (θ) 和 ET (θ)&ndash;(定义了一组看不见的目标域 T)</p>
<p>我们评估了 Di 的局部测试样本的局部性能，并从联合全局分布 D := {Di}N i=1 评估测试样本的全局性能</p>
<p>&ldquo;minimize both population loss&rdquo; 意味着在机器学习中，不仅要最小化经验损失（在训练数据上的损失），还要尽量减小整体（或总体）损失。这表示模型不仅要在训练数据上表现良好，还要在整体总体上具有良好的泛化能力。</p>
<p><code>泛化和平坦最小值</code></p>
<p>在实践中，深度神经网络中的ERM，即arg-minθõED（θ），可以产生多个解决方案，这些解决方案提供可比较的训练损失，但可推广性水平截然不同[3]。然而，如果没有适当的正则化，模型容易对训练数据进行过度拟合，并且训练模型将陷入损失面的陡峭山谷，这是不太可推广的[4]。ERM失败的一个常见原因是数据分布存在变化，这可能会导致损失景观的变化。如图 1 所示，优化的最小值越尖锐，它对损失景观的变化就越敏感。这导致泛化误差增加。在跨设备 FL 中，每个客户端可能会过度拟合其本地训练数据，导致全局性能较差。这是由于分布偏移问题，它在局部模型[23]中造成了相互冲突的目标。因此，当局部模型收敛到一个尖锐的最小值时，模型的个性化程度（局部性能）越高，泛化能力较差（全局性能）的可能性就越大。</p>
<p><code>fedsoup</code></p>
<p>Temporal Model Selection</p>
<p>随机加权平均 (SWA)  是一种更简洁和有效的方法，通过平均权重隐式偏爱平坦最小值。SWA 算法的动机是观察到 SGD 通常在权重空间中找到高性能模型，但很少达到最优集的中心点。通过对迭代上的参数值进行平均，SWA 算法将解移动到更接近该点空间的质心。</p>
<p>引入了一种称为模型汤[27]的选择性加权平均方法来增强微调模型的泛化能力。我们通过利用在FL训练的一次传递中不同时间点训练的全局模型，使这个想法适应新的方法。我们提出了一种模型选择策略，其中每个客户端利用其本地验证集的性能作为监控指标</p>
<p>Federated Model Patching</p>
<p>根据之前关于损失景观的分析，由于不同FL客户端的域差异，不同FL客户端之间存在损失景观偏移。因此，简单地集成全局模型会损害模型的个性化。我们在 FL 的客户端局部训练期间引入了模型修补 [10]（即局部和全局模型插值），旨在提高模型个性化并保持良好的全局性能。具体来说，模型修补方法迫使本地客户端不会严重扭曲全局模型，并在局部和全局之间寻求低损失插值模型，鼓励局部和全局模型位于同一个盆地，没有较大的线性连通性障碍。[19]。我们称这个模块为联邦模型修补。</p>
<p>我们提出的 FedSoup 算法只需要一个仔细调整的超参数，即插值开始 epoch。为了减轻当起始 epoch 太晚并且当起始 epoch 太早时防止潜在性能下降的风险，我们将默认插值起始 epoch 设置为总训练时期的 75%，与 SWA 的默认设置对齐。此外，值得一提的是，我们提出的 FedSoup 框架中修改后的模型汤和模型修补模块是相互依赖的。模型修补是一种基于我们修改后的模型汤算法的技术，提供了丰富的模型来探索更平坦的最小值并提高性能。</p>
<p>Experiments</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="p">{</span>
  <span class="s2">&#34;num_clients&#34;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
  <span class="s2">&#34;num_classes&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="s2">&#34;non_iid&#34;</span><span class="p">:</span> <span class="n">null</span><span class="p">,</span>
  <span class="s2">&#34;balance&#34;</span><span class="p">:</span> <span class="n">null</span><span class="p">,</span>
  <span class="s2">&#34;partition&#34;</span><span class="p">:</span> <span class="n">null</span><span class="p">,</span>
  <span class="s2">&#34;Size of samples for labels in clients&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">[</span>
      <span class="p">[</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">460</span>
      <span class="p">],</span>
  <span class="s2">&#34;alpha&#34;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
  <span class="s2">&#34;batch_size&#34;</span><span class="p">:</span> <span class="mi">10</span>
<span class="p">}</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># hoid 0: 指定保留客户的索引。在交叉验证中，可能需要保留特定的客户数据以进行验证，这就需要指定要保留的客户在数据集中的索引。</span>
<span class="c1"># nc :client数目</span>
<span class="c1"># -wa_alpha:FedSoup个性化全局模型池的权重平均比率</span>
<span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">20</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">1</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>

<span class="n">Algorithm</span><span class="p">:</span> <span class="n">FedSoup</span>
<span class="n">Local</span> <span class="n">batch</span> <span class="n">size</span><span class="p">:</span> <span class="mi">16</span>
<span class="n">Local</span> <span class="n">steps</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">Local</span> <span class="n">learing</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">0.001</span>
<span class="n">Total</span> <span class="n">number</span> <span class="n">of</span> <span class="n">clients</span><span class="p">:</span> <span class="mi">4</span>
<span class="n">Clients</span> <span class="n">join</span> <span class="ow">in</span> <span class="n">each</span> <span class="nb">round</span><span class="p">:</span> <span class="mf">1.0</span>
<span class="n">Client</span> <span class="n">drop</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="n">Time</span> <span class="n">select</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">Time</span> <span class="n">threthold</span><span class="p">:</span> <span class="mi">10000</span>
<span class="n">Global</span> <span class="n">rounds</span><span class="p">:</span> <span class="mi">20</span>
<span class="n">Running</span> <span class="n">times</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">Dataset</span><span class="p">:</span> <span class="n">tiny_camelyon17</span>
<span class="n">Local</span> <span class="n">model</span><span class="p">:</span> <span class="n">resnet</span>
<span class="n">Using</span> <span class="n">device</span><span class="p">:</span> <span class="n">cpu</span>
<span class="n">Hold</span><span class="o">-</span><span class="n">out</span> <span class="n">Client</span> <span class="n">ID</span><span class="p">:</span> <span class="mi">0</span>
    
<span class="o">=============</span> <span class="n">Running</span> <span class="n">time</span><span class="p">:</span> <span class="mi">0</span><span class="n">th</span> <span class="o">=============</span>
    
<span class="n">ResNet</span><span class="p">(</span>
  <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">maxpool</span><span class="p">):</span> <span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">(</span><span class="n">layer1</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">layer2</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">downsample</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">layer3</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">downsample</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">layer4</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">downsample</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
        <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">BasicBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn1</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">relu</span><span class="p">):</span> <span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="p">(</span><span class="n">conv2</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">(</span><span class="n">bn2</span><span class="p">):</span> <span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">avgpool</span><span class="p">):</span> <span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="p">(</span><span class="n">fc</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">server</span> <span class="o">=</span> <span class="n">FedSoup</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

<span class="c1"># 设置客户端</span>
<span class="bp">self</span><span class="o">.</span><span class="n">set_clients</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">clientSoup</span><span class="p">)</span>
<span class="c1"># client0 没有设置</span>
<span class="nb">set</span> <span class="n">client</span><span class="p">:</span>  <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span>
<span class="n">Join</span> <span class="n">ratio</span> <span class="o">/</span> <span class="n">total</span> <span class="n">clients</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mi">5</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">server</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">selected_clients</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientsoup</span><span class="o">.</span><span class="n">clientSoup</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x0000019197EB87C0</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientsoup</span><span class="o">.</span><span class="n">clientSoup</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x00000191ABB543A0</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientsoup</span><span class="o">.</span><span class="n">clientSoup</span> <span class="nb">object</span>
<span class="n">at</span> <span class="mh">0x000001919BE7AF40</span><span class="o">&gt;</span><span class="p">,</span> <span class="o">&lt;</span><span class="n">flcore</span><span class="o">.</span><span class="n">clients</span><span class="o">.</span><span class="n">clientsoup</span><span class="o">.</span><span class="n">clientSoup</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x0000019194DBA9D0</span><span class="o">&gt;</span><span class="p">]</span>

</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># server-&gt;client</span>
<span class="bp">self</span><span class="o">.</span><span class="n">send_models</span><span class="p">()</span>
<span class="c1"># 接收全局模型</span>
<span class="n">client</span><span class="o">.</span><span class="n">set_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_model</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">0</span><span class="o">-------------</span>
<span class="n">Evaluate</span> <span class="k">global</span> <span class="n">model</span>
<span class="c1"># out-of-distribution (OOD) 数据的评估为True</span>
<span class="c1"># 模型在额外的、不同分布的数据上进行评估，以测试其泛化能力。</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 获取测试指标</span>
<span class="n">stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">()</span> <span class="c1"># ood_eval=False</span>
<span class="c1"># 测试每个client的数据 clientbase.py(113)test_metrics()</span>
<span class="n">c</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">()</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">])</span>
<span class="c1"># 遍历完所有client0 测试集， </span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span><span class="c1"># 18个batchsize</span>
<span class="mi">18</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="c1"># batchsize:16</span>
<span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># 按第一个维度拼接之后，所有数据拼接，去掉了batchsize</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="c1"># client的测试数据数</span>
<span class="mi">280</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">2</span><span class="p">,)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc</span>
<span class="mi">140</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_num</span>
<span class="mi">280</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">auc</span>
<span class="mf">0.5641454081632653</span>
<span class="c1"># 对所有client而言</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">ids</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">280</span><span class="p">,</span> <span class="mi">280</span><span class="p">,</span> <span class="mi">280</span><span class="p">,</span> <span class="mi">280</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_correct</span>
<span class="p">[</span><span class="mf">140.0</span><span class="p">,</span> <span class="mf">140.0</span><span class="p">,</span> <span class="mf">140.0</span><span class="p">,</span> <span class="mf">140.0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_auc</span><span class="c1"># 即使`y_true`和`y_prob`相同，但AUC的计算结果可能不同</span>
<span class="p">[</span><span class="mf">157.9607142857143</span><span class="p">,</span> <span class="mf">150.17857142857142</span><span class="p">,</span> <span class="mf">136.8892857142857</span><span class="p">,</span> <span class="mf">140.6357142857143</span><span class="p">]</span>

<span class="c1"># # acc：每个client样本正确数分别除所有client样本数 相加</span>
<span class="c1"># 相当于，每个客户端的正确率*每个客户端数据的比重 相加  </span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">test_acc</span>
<span class="mf">0.5</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">test_auc</span>
<span class="mf">0.5229145408163266</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc_list</span>
<span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_auc_list</span>
<span class="p">[</span><span class="mf">0.5641454081632653</span><span class="p">,</span> <span class="mf">0.5363520408163265</span><span class="p">,</span> <span class="mf">0.48889030612244894</span><span class="p">,</span> <span class="mf">0.5022704081632654</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">accs</span>
<span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">aucs</span>
<span class="p">[</span><span class="mf">0.5641454081632653</span><span class="p">,</span> <span class="mf">0.5363520408163265</span><span class="p">,</span> <span class="mf">0.48889030612244894</span><span class="p">,</span> <span class="mf">0.5022704081632654</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">local_test_acc</span>
<span class="mf">0.5</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">local_test_auc</span>
<span class="mf">0.5229145408163266</span>



<span class="n">OOD</span> <span class="nb">eval</span> <span class="n">details</span><span class="p">:</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">ood_eval</span>
<span class="kc">True</span>
<span class="n">stats_all</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">clients</span><span class="p">:</span><span class="c1"># 4 self.num_clients </span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">c</span><span class="o">.</span><span class="n">id</span>
<span class="mi">1</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">dataset_ids</span> <span class="c1"># 其余客户端的id</span>
<span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">c</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="n">ood_eval</span><span class="p">,</span> <span class="n">dataset_ids</span><span class="o">=</span><span class="n">dataset_ids</span><span class="p">)</span>

<span class="n">testloaderfull</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_test_data</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="n">ood_eval</span><span class="p">,</span><span class="n">dataset_ids</span><span class="o">=</span><span class="n">dataset_ids</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">batch_size</span>
<span class="mi">16</span>
<span class="k">for</span> <span class="n">dataset_id</span> <span class="ow">in</span> <span class="n">dataset_ids</span><span class="p">:</span><span class="c1"># [0, 2, 3, 4]</span>
	<span class="k">if</span> <span class="n">dataset_id</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">hold_out_id</span><span class="p">:</span><span class="c1"># 0</span>
		<span class="n">train_data</span> <span class="o">=</span> <span class="n">read_client_data</span><span class="p">(</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># client 0 的训练集</span>
        <span class="n">test_data_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
        <span class="c1"># client 1 的测试集</span>
     <span class="n">test_data</span> <span class="o">=</span> <span class="n">read_client_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="n">test_data_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="mi">640</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data_list</span><span class="p">)</span>
<span class="mi">5</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data_list</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># client 0 训练集</span>
<span class="mi">640</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data_list</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># client 1 测试集</span>
<span class="mi">280</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data_list</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="mi">280</span>
<span class="c1"># client 0的所有数据集＋其余client 2, 3, 4数据集</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">concat_test_data</span><span class="p">)</span><span class="c1"># 拼接后的测试集</span>
<span class="mi">1760</span><span class="c1"># 640   +   280   ×   4 </span>

<span class="c1"># client0 的所有数据集</span>
<span class="n">oof_testloaderfull</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_test_data</span><span class="p">(</span>
                    <span class="n">ood_eval</span><span class="o">=</span><span class="n">ood_eval</span><span class="p">,</span> <span class="n">dataset_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hold_out_id</span><span class="p">]</span>
                <span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data_list</span><span class="p">)</span>
<span class="mi">2</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">concat_test_data</span><span class="p">)</span>
<span class="mi">920</span>

<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">testloaderfull</span><span class="p">:</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">oof_testloaderfull</span><span class="p">:</span>
<span class="k">return</span> <span class="n">test_acc</span><span class="p">,</span> <span class="n">test_num</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">oof_test_acc</span><span class="p">,</span> <span class="n">oof_test_num</span><span class="p">,</span> <span class="n">oof_auc</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc</span>
<span class="mi">880</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_num</span>
<span class="mi">1760</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">auc</span>
<span class="mf">0.5438849431818182</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_test_acc</span>
<span class="mi">460</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_test_num</span>
<span class="mi">920</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_auc</span>
<span class="mf">0.5537807183364839</span>

<span class="c1"># client1上的模型测试client0(hold)数据集</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_model_ids</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_tot_correct</span>
<span class="p">[</span><span class="mf">460.0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_tot_auc</span>
<span class="p">[</span><span class="mf">509.4782608695652</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_num_samples</span>
<span class="p">[</span><span class="mi">920</span><span class="p">]</span>

<span class="c1"># client1上测试 client0数据集 和234的测试集</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">model_ids</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_correct</span>
<span class="p">[</span><span class="mf">880.0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_auc</span>
<span class="p">[</span><span class="mf">957.2375000000001</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">1760</span><span class="p">]</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">model_ids</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">num_samples</span>
<span class="p">[</span><span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_correct</span>
<span class="p">[</span><span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">tot_auc</span>
<span class="p">[</span><span class="mf">957.2375000000001</span><span class="p">,</span> <span class="mf">959.5624999999999</span><span class="p">,</span> <span class="mf">977.6306818181818</span><span class="p">,</span> <span class="mf">965.6460227272728</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_model_ids</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_num_samples</span>
<span class="p">[</span><span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_tot_correct</span>
<span class="p">[</span><span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">oof_tot_auc</span>
<span class="p">[</span><span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">]</span>


<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">stats_all</span>
<span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">],</span> <span class="p">[</span><span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">957.2375000000001</span><span class="p">,</span> <span class="mf">959.5624999999999</span><span class="p">,</span> <span class="mf">977.6306818181818</span><span class="p">,</span> <span class="mf">965.6460227272728</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">stats</span> <span class="c1"># 切片前四个为 client0数据集 和234的测试集</span>
<span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">],</span> <span class="p">[</span><span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">957.2375000000001</span><span class="p">,</span> <span class="mf">959.5624999999999</span><span class="p">,</span> <span class="mf">977.6306818181818</span><span class="p">,</span> <span class="mf">965.6460227272728</span><span class="p">])</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc</span>
<span class="mf">0.5</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_auc</span>
<span class="mf">0.5483063500774793</span>

<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_acc_list</span>
<span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">test_auc_list</span>
<span class="p">[</span><span class="mf">0.5438849431818182</span><span class="p">,</span> <span class="mf">0.5452059659090909</span><span class="p">,</span> <span class="mf">0.5554719783057851</span><span class="p">,</span> <span class="mf">0.5486625129132232</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">accs</span>
<span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">aucs</span>
<span class="p">[</span><span class="mf">0.5438849431818182</span><span class="p">,</span> <span class="mf">0.5452059659090909</span><span class="p">,</span> <span class="mf">0.5554719783057851</span><span class="p">,</span> <span class="mf">0.5486625129132232</span><span class="p">]</span>



<span class="n">OOD</span> <span class="n">Performance</span> <span class="p">(</span><span class="n">client</span> <span class="n">model</span> <span class="n">i</span> <span class="n">on</span> <span class="nb">all</span> <span class="n">other</span> <span class="n">dataset</span> <span class="n">j</span><span class="p">):</span>
<span class="c1"># 根据客户端数量加权的测试准确率</span>
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Num</span><span class="o">-</span><span class="n">Weighted</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span> 
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Num</span><span class="o">-</span><span class="n">Weighted</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5483</span>
<span class="c1"># 客户端平均测试准确率</span>
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5483</span>
<span class="n">OOD</span> <span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0000</span>
<span class="n">OOD</span> <span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0045</span>

<span class="n">Performance</span> <span class="n">Summarizing</span><span class="o">...</span>
<span class="n">Local</span> <span class="n">Performance</span><span class="p">:</span><span class="c1"># client 在自己测试集上的性能的平均值</span>
<span class="n">Local</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Local</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5229</span>
<span class="c1"># accs:每个client在其余client数据集上的准确率</span>
<span class="c1"># append:加上每个client在自身数据集上的平均值</span>
<span class="n">accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">local_test_acc</span><span class="p">)</span>
<span class="n">aucs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">local_test_auc</span><span class="p">)</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">accs</span>
<span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">aucs</span>
<span class="p">[</span><span class="mf">0.5438849431818182</span><span class="p">,</span> <span class="mf">0.5452059659090909</span><span class="p">,</span> <span class="mf">0.5554719783057851</span><span class="p">,</span> <span class="mf">0.5486625129132232</span><span class="p">,</span> <span class="mf">0.5229145408163266</span><span class="p">]</span>
<span class="c1"># 取平均？得到全局？</span>
<span class="c1"># 可以理解为 local_test_acc为每个client在自身数据集的准确率的平均值</span>
<span class="c1"># 先取平均 np.mean(accs):每个客户端在其余client数据集上准确率的平均值</span>
<span class="c1"># 再取平均为全局模型的准确率</span>
<span class="n">Global</span> <span class="n">Performance</span><span class="p">:</span>
<span class="n">Glocal</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Glocal</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5432</span>

<span class="c1"># client在holdcliet上的准确率(client 0) 为参与到联邦学习中的client</span>
<span class="n">stats</span> <span class="o">=</span> <span class="n">stats_all</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="n">stats</span>
<span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">],</span> <span class="p">[</span><span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">,</span> <span class="mf">509.4782608695652</span><span class="p">])</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">-------------</span><span class="n">Round</span> <span class="n">number</span><span class="p">:</span> <span class="mi">0</span><span class="o">-------------</span>

<span class="n">Evaluate</span> <span class="k">global</span> <span class="n">model</span><span class="c1"># 每个客户端在自身测试集的准确率</span>
<span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">280</span><span class="p">,</span> <span class="mi">280</span><span class="p">,</span> <span class="mi">280</span><span class="p">,</span> <span class="mi">280</span><span class="p">],</span> <span class="p">[</span><span class="mf">140.0</span><span class="p">,</span> <span class="mf">140.0</span><span class="p">,</span> <span class="mf">140.0</span><span class="p">,</span> <span class="mf">140.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">157.9607142857143</span><span class="p">,</span> <span class="mf">150.17857142857142</span><span class="p">,</span> <span class="mf">136.8892857142857</span><span class="p">,</span> <span class="mf">140.6357142857143</span><span class="p">])</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">Client</span> <span class="mi">1</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">1</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5641</span>
<span class="n">Client</span> <span class="mi">2</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">2</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5364</span>
<span class="n">Client</span> <span class="mi">3</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">3</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.4889</span>
<span class="n">Client</span> <span class="mi">4</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">4</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5023</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span><span class="c1"># 平均准确率</span>
<span class="n">Averaged</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5229</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0000</span>
<span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0294</span>

<span class="n">OOD</span> <span class="nb">eval</span> <span class="n">details</span><span class="p">:</span> <span class="c1"># ood每个客户端在其他所有客户端数据的准确率</span>
<span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">,</span> <span class="mi">1760</span><span class="p">],</span> <span class="p">[</span><span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">,</span> <span class="mf">880.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">957.2369318181819</span><span class="p">,</span> <span class="mf">959.5619318181818</span><span class="p">,</span> <span class="mf">977.6301136363636</span><span class="p">,</span> <span class="mf">965.6454545454545</span><span class="p">])</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">Client</span> <span class="mi">1</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">1</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5439</span>
<span class="n">Client</span> <span class="mi">2</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">2</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5452</span>
<span class="n">Client</span> <span class="mi">3</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">3</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5555</span>
<span class="n">Client</span> <span class="mi">4</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Client</span> <span class="mi">4</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5487</span>
<span class="n">OOD</span> <span class="n">Performance</span> <span class="p">(</span><span class="n">client</span> <span class="n">model</span> <span class="n">i</span> <span class="n">on</span> <span class="nb">all</span> <span class="n">other</span> <span class="n">dataset</span> <span class="n">j</span><span class="p">):</span>
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Num</span><span class="o">-</span><span class="n">Weighted</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span><span class="c1"># 加权平均</span>
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Num</span><span class="o">-</span><span class="n">Weighted</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5483</span>
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span><span class="c1"># 平均</span>
<span class="n">OOD</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5483</span>
<span class="n">OOD</span> <span class="n">Std</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.0000</span>
<span class="n">OOD</span> <span class="n">Std</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.0045</span>

<span class="n">Performance</span> <span class="n">Summarizing</span><span class="o">...</span>
<span class="n">Local</span> <span class="n">Performance</span><span class="p">:</span><span class="c1"># client在本地数据上的准确率</span>
<span class="n">Local</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Local</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5229</span>
<span class="n">Global</span> <span class="n">Performance</span><span class="p">:</span><span class="c1"># 全局模型准确率</span>
<span class="n">Glocal</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">Glocal</span> <span class="n">Client</span><span class="o">-</span><span class="n">Equally</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5432</span>
<span class="c1"># OOF未参与联邦学习的client 0的准确率</span>
<span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">,</span> <span class="mi">920</span><span class="p">],</span> <span class="p">[</span><span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">,</span> <span class="mf">460.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">509.4771739130435</span><span class="p">,</span> <span class="mf">509.4771739130435</span><span class="p">,</span> <span class="mf">509.4771739130435</span><span class="p">,</span> <span class="mf">509.4771739130435</span><span class="p">])</span>
<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="o">================</span>
<span class="n">OOF</span> <span class="n">Performance</span><span class="p">:</span>
<span class="n">OOF</span> <span class="n">Client</span> <span class="n">Test</span> <span class="n">Accurancy</span><span class="p">:</span> <span class="mf">0.5000</span>
<span class="n">OOF</span> <span class="n">Client</span> <span class="n">Test</span> <span class="n">AUC</span><span class="p">:</span> <span class="mf">0.5538</span>
</code></pre></td></tr></table>
</div>
</div><p>out-of-distribution (OOD)：在不同客户端数据上进行测试，未知分布。</p>
<p>out-of-federated performance(oof)：指的是在联邦学习环境之外的性能表现。这个术语通常用于描述模型在非联邦学习设置下的性能，例如在集中式学习或单个数据源的情况下的性能表现。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">client</span><span class="o">.</span><span class="n">train</span><span class="p">()</span><span class="c1"># clientsoup.py(33)train()</span>

<span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">client</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1">#轮次达到0.75时执行权重平均化算法</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="bp">self</span><span class="o">.</span><span class="n">wa_alpha</span>
<span class="mf">0.75</span>


<span class="nb">round</span> <span class="mi">15</span>
<span class="n">Begin</span> <span class="n">Weight</span> <span class="n">Averaging</span><span class="o">......</span>
<span class="n">Original</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.9392857142857143</span>
<span class="n">Updated</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.8357142857142857</span>
<span class="n">Remain</span> <span class="n">the</span> <span class="n">same</span> <span class="n">Personalized</span> <span class="n">Global</span> <span class="n">Model</span><span class="o">.</span>
<span class="n">Begin</span> <span class="n">Weight</span> <span class="n">Averaging</span><span class="o">......</span>
<span class="n">Original</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.8607142857142858</span>
<span class="n">Updated</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.7785714285714286</span>
<span class="n">Remain</span> <span class="n">the</span> <span class="n">same</span> <span class="n">Personalized</span> <span class="n">Global</span> <span class="n">Model</span><span class="o">.</span>
<span class="n">Begin</span> <span class="n">Weight</span> <span class="n">Averaging</span><span class="o">......</span>
<span class="n">Original</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.7642857142857142</span>
<span class="n">Updated</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.7178571428571429</span>
<span class="n">Remain</span> <span class="n">the</span> <span class="n">same</span> <span class="n">Personalized</span> <span class="n">Global</span> <span class="n">Model</span><span class="o">.</span>
<span class="n">Begin</span> <span class="n">Weight</span> <span class="n">Averaging</span><span class="o">......</span>
<span class="n">Original</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.8857142857142857</span>
<span class="n">Updated</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.8714285714285714</span>
<span class="n">Remain</span> <span class="n">the</span> <span class="n">same</span> <span class="n">Personalized</span> <span class="n">Global</span> <span class="n">Model</span><span class="o">.</span>

<span class="n">Original</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.6214285714285714</span>
<span class="n">Updated</span> <span class="n">Weight</span> <span class="n">Averaging</span> <span class="n">Accuracy</span><span class="p">:</span>  <span class="mf">0.7857142857142857</span>
<span class="n">Update</span> <span class="n">Personalized</span> <span class="n">Global</span> <span class="n">Model</span><span class="o">......</span>
<span class="n">Client</span> <span class="n">ID</span><span class="p">:</span>  <span class="mi">4</span>
<span class="n">Personalized</span> <span class="n">Global</span> <span class="n">Model</span> <span class="n">Num</span><span class="p">:</span>  <span class="mi">1</span>


</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">Evaluating</span> <span class="n">Post</span><span class="o">-</span><span class="n">Fine</span><span class="o">-</span><span class="n">Tuning</span> <span class="ow">and</span> <span class="n">OOD</span> <span class="n">Performance</span><span class="o">...</span>

<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">Fine</span><span class="o">-</span><span class="n">Tuning</span> <span class="k">with</span> <span class="n">the</span> <span class="n">Last</span> <span class="n">Trained</span> <span class="n">Model</span><span class="o">......</span>
</code></pre></td></tr></table>
</div>
</div><p>fedsoup</p>
<ol>
<li><strong><code>选择性模型混合（Selective Model Interpolation）</code></strong>：FedSoup引入了一种新的模型混合策略，它允许在全局模型和个性化模型之间进行选择性的插值。这种方法不仅考虑了全局模型的泛化能力，还考虑了个性化模型对本地数据的适应性。</li>
<li><strong>个性化和泛化的平衡</strong>：FedSoup通过<code>调整全局模型和个性化模型的插值比例</code>，<code>动态地平衡了模型的泛化和个性化</code>。这种平衡使得模型既能捕捉到跨用户的共同特征，又能适应每个用户的特定数据分布。</li>
<li><strong><code>自适应插值权重</code></strong>：FedSoup算法中的选择性模型混合不是静态的，而是基于每个用户的数据和模型性能自适应地调整插值权重。这种自适应机制使得算法能够根据不同用户的数据多样性和模型性能需求灵活调整模型结构。</li>
<li><strong>降低通信开销</strong>：在联邦学习中，模型参数的同步是一个通信密集型的过程。FedSoup通过减少需要传输的参数数量，从而降低了通信开销。这是通过只传输那些在全局模型和个性化模型插值中起到关键作用的参数来实现的。</li>
<li><strong>实验验证</strong>：论文中对FedSoup算法进行了广泛的实验验证，证明了它在多个数据集和不同的联邦学习设置下，相较于现有方法，能够显著提高模型的泛化和个性化性能。</li>
</ol>
<p>总之，FedSoup算法通过选择性模型混合和自适应插值权重，提供了一种有效的机制来同时增强联邦学习中的泛化和个性化性能，并且降低了通信开销。</p>
<p>&ndash;</p>
<p>FedSoup算法在提升模型泛化能力方面的创新体现在以下几个方面：</p>
<ol>
<li><strong>全局与个性化模型的融合</strong>：FedSoup算法通过结合全局模型和本地个性化模型来提升泛化能力。<code>全局模型能够捕捉到所有用户数据的共同特征，从而提高模型对于未见数据的泛化能力。个性化模型则针对每个用户的本地数据进行优化，提高模型对特定用户数据的适应性</code>。</li>
<li><strong>自适应插值权重</strong>：算法根据每个用户的数据特性和模型性能动态调整全局模型和个性化模型的插值权重。这种自适应机制使得算法能够在保证模型个性化的同时，确保模型在全局范围内的泛化性能。</li>
<li><strong>模型更新的选择性同步</strong>：在联邦学习的过程中，<code>并不是每次都需要将整个模型更新同步给所有用户。</code>FedSoup算法通过选择性地同步模型更新，减少了通信开销，并可能只同步那些对提升泛化能力最为关键的部分，这样可以更加高效地利用带宽和计算资源，从而间接提升模型的泛化能力。</li>
<li><strong>避免过度拟合</strong>：由于联邦学习的数据分散在多个用户手中，局部模型可能会过度拟合到本地数据，从而降低泛化能力。FedSoup通过<code>全局模型的引导，帮助局部模型避免这种过度拟合</code>现象，增强模型对新数据的泛化能力。</li>
<li><strong>实验验证</strong>：论文中提供了详细的实验结果，展示了FedSoup算法在不同的数据集和联邦学习场景下，相较于传统的联邦学习方法，在提升模型泛化能力方面的优势。</li>
</ol>
<p>综上所述，FedSoup算法通过全局和个性化模型的融合、自适应插值权重、选择性模型同步、避免过度拟合以及实验验证等创新手段，有效地提升了模型的泛化能力。</p>
<blockquote>
<p>插值权重，在插值方法的上下文中，是指每个已知数据点对未知位置或值的影响权重。这些权重通常是根据已知数据点之间的距离或其他相关性来计算的。简而言之，插值权重用于确定在插值过程中，各个已知数据点应如何贡献于未知点的估计值。通过适当地调整这些权重，可以更准确地逼近未知点的真实值。</p>
</blockquote>
<p>&ndash;</p>
<p>在论文《FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation》中，选择性模型混合（Selective Model Interpolation）是通过以下步骤实现的：&mdash;类似于fedala</p>
<ol>
<li><strong>模型训练</strong>：在联邦学习的框架中，每个客户端都会根据自己的本地数据训练一个个性化模型。同时，所有客户端还会训练一个全局模型，该模型旨在捕捉所有客户端数据的共同特征。</li>
<li><strong>模型融合策略</strong>：在模型训练完成后，<code>FedSoup采用一种融合策略，该策略基于每个客户端的个性化模型和全局模型之间的相似性来选择性地进行模型混合</code>。具体来说，FedSoup计算每个个性化模型与其对应客户端的全局模型的差异度量（例如，KL散度或余弦相似性）。</li>
<li><strong>插值系数计算</strong>：接着，FedSoup为每个客户端计算一个<code>插值系数</code>，该系数表示个性化模型在最终模型中的权重。插值系数的计算基于差异度量和一个预设的温度参数（temperature parameter），后者用于控制插值的平滑程度。</li>
<li><strong>模型更新</strong>：最后，每个客户端根据计算出的插值系数更新自己的模型，即将全局模型与个性化模型按照插值系数融合成一个新的模型。这个新模型既包含了全局模型的泛化能力，又保留了个性化模型对本地数据的适应性。</li>
</ol>
<p>通过这种选择性模型混合，FedSoup旨在实现更好的模型泛化和个性化。模型泛化指的是模型对新数据的适用性，而个性化则是模型对特定用户数据的适应性。通过这种方式，FedSoup旨在平衡这两个目标，使得模型既能够泛化到新用户，也能够很好地服务于每个参与联邦学习的客户端。</p>
<p>在《FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation》论文中，通过全局模型引导局部模型的方式来避免过度拟合主要体现在模型融合策略上。具体而言，FedSoup算法采用了一种选择性模型混合（selective model interpolation）的策略，该策略结合了全局模型和局部模型的优点，以提高模型在联邦学习环境中的泛化能力和个性化能力。</p>
<p>在这个过程中，每个客户端都会根据自己的本地数据训练一个个性化模型，同时也会从服务器那里接收到一个全局模型。然后，客户端利用全局模型来引导本地模型的训练。这种引导通常是通过在训练过程中将全局模型的参数作为一种先验知识加入到本地模型的训练中，或者在训练结束后，通过模型混合的方式来实现的。</p>
<p><code>在模型混合阶段，客户端会计算本地模型和全局模型之间的差异，并根据这个差异以及其他因素（比如模型性能、数据分布等）来决定如何混合这两个模型。如果本地模型在本地数据上过度拟合，那么它可能会与全局模型有较大的差异。此时，通过选择性地混合全局模型和本地模型，可以帮助本地模型“解拟合”，即减少模型对本地数据的过拟合现象，从而提高模型对新数据的泛化能力</code>。</p>
<p>通过这种方式，FedSoup算法能够在保持模型个性化的同时，<code>确保模型不会仅仅适应本地数据，而是能够泛化到更广泛的数据分布上</code>。这样的策略对于联邦学习中的模型训练尤为重要，因为联邦学习涉及多个客户端，每个客户端的数据分布可能都不尽相同，因此需要模型既要有良好的泛化能力，又要能适应不同客户端的个性化需求。</p>
<p>&ndash;</p>
<p>在论文《FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation》中，模型更新的选择性同步是通过以下步骤实现的：</p>
<ol>
<li>每个参与联邦学习的客户端在本地训练自己的个性化模型，同时也训练一个全局模型。</li>
<li>在每次联邦学习轮次结束时，客户端计算自己的个性化模型与全局模型之间的差异度量，并据此确定是否需要同步更新。</li>
<li>客户端根据差异度量和一个预设的阈值来决定<code>是否将模型更新发送给服务器</code>。只有当差异超过这个阈值时，客户端才会同步其模型更新。</li>
<li>服务器收到来自不同客户端的更新后，会根据一定的策略（比如加权平均）来聚合这些更新，生成新的全局模型。</li>
<li>服务器将新的全局模型发送回所有客户端，客户端再根据自己的插值系数将新的全局模型与本地个性化模型融合，形成新的本地模型。</li>
</ol>
<p>通过这种方式，只有当客户端的模型更新相对于全局模型有显著差异时，更新才会被同步，这样可以减少不必要的通信开销，并且使得全局模型更快地收敛到一个对所有客户端都有用的状态。同时，选择性同步也有助于保护客户端的隐私，因为它减少了需要传输的信息量。</p>
<p>论文《FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation》中，判断是否需要上传模型更新的机制是基于模型间的差异性。具体来说，客户端会计算本地模型更新与全局模型更新之间的差异度量，例如使用欧氏距离或者其他相似度指标。如果差异超过某个设定的阈值，表明本地模型与全局模型存在较大差异，这时候才需要上传模型更新。</p>
<p>这样做的目的主要有两个：</p>
<ol>
<li>减少通信成本：通过只上传那些真正有差异、能够为全局模型带来新信息的模型更新，可以减少网络带宽的使用和通信开销，特别是在大规模分布式系统中，这一点尤为重要。</li>
<li>提高模型效率：只上传重要的更新有助于加快全局模型收敛速度，因为它避免了冗余的、相似的或者不太有用的更新，从而使得全局模型能够更快地整合所有客户端的有用信息，提高整体模型的性能。</li>
</ol>
<hr>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">last_global_model</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">per_global_model_num</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">per_global_model_num</span> <span class="o">*</span> <span class="n">global_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="o">+</span> <span class="n">last_global_model</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><ol>
<li><code>self.per_global_model_num * global_param.data.clone()</code>：这部分计算<code>global_param</code>的当前值与其对应权重的乘积。权重是<code>self.per_global_model_num</code>。</li>
<li><code>last_global_model.data.clone()</code>：这部分直接使用<code>last_global_model</code>的当前值，没有乘以任何权重。</li>
<li>将上述两部分相加，得到<code>global_param</code>和<code>last_global_model</code>的加权和。</li>
<li>最后，这个加权和乘以一个归一化因子<code>1.0 / (self.per_global_model_num + 1.0)</code>，以确保更新后的<code>last_global_model</code>值在合理的范围内。</li>
<li><code>global_param</code>的权重随着<code>self.per_global_model_num</code>的增加而增加。<code>last_global_model</code>的实际影响会随着<code>self.per_global_model_num</code>的增加而相对减小。</li>
</ol>
<hr>
<p><code>FedSoupALA</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelast_global_model</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">per_global_model_num</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">per_global_model_num</span> <span class="o">*</span> <span class="n">global_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="o">+</span> <span class="n">last_global_model</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="p">)</span><span class="n">lyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">20</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">1</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>

<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">mnist</span><span class="o">-</span><span class="mf">0.1</span><span class="o">-</span><span class="n">npz</span> <span class="o">-</span><span class="n">m</span> <span class="n">cnn</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">mnist</span><span class="o">-</span><span class="mf">0.1</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>


<span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoupALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">20</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">1</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">pala</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span>


<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoupALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">100</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>  <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">pala</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="n">npz</span><span class="o">-</span><span class="n">ala</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoupALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">100</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>  <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">pala</span> <span class="mi">6</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="n">npz</span><span class="o">-</span><span class="n">ala</span><span class="o">-</span><span class="n">neweval</span><span class="o">-</span><span class="n">p6</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoupALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">100</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">--</span><span class="n">pruning</span> <span class="o">--</span><span class="n">sparsity_ratio</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">pruning_warmup_round</span> <span class="mi">500</span> <span class="o">--</span><span class="n">masking_grad</span> <span class="o">--</span><span class="n">dynamic_mask</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="n">npz</span><span class="o">-</span><span class="n">ala</span><span class="o">-</span><span class="n">prun</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># ResNet</span>
<span class="p">(</span><span class="n">Pdb</span><span class="p">)</span> <span class="n">p</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="mi">62</span>

</code></pre></td></tr></table>
</div>
</div><p><code>cifar -- client-20 gr 50</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">client</span><span class="o">-</span><span class="mi">20</span> <span class="n">gr</span> <span class="mi">50</span>
<span class="c1"># 修改self.evaluate(ood_eval=True)</span>

<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="nb">round</span> <span class="o">=</span> <span class="n">i</span><span class="p">)</span>
<span class="k">if</span> <span class="n">ood_eval</span> <span class="ow">and</span> <span class="nb">round</span> <span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rounds</span>

<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="nb">round</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rounds</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="nb">round</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gloabl_rounds</span><span class="p">)</span>
 <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span><span class="n">即可</span>
    
   
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">local_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">():</span>
    		<span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_gap</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">-------------Round number: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">-------------&#34;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Evaluate global model&#34;</span><span class="p">)</span>
                <span class="c1">#pdb.set_trace()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
           	<span class="n">trian</span> <span class="n">之后</span>     
          	<span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_gap</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Evaluate local model&#34;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">acc</span><span class="o">=</span><span class="n">local_acc</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Evaluate ID, OOD and OOF Performance&#34;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">ood_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="c1"># break</span>
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">fedsoup代码中有命令hold中</span>
<span class="n">python</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">10</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedAvg</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span>  <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">go</span> <span class="n">resnet</span>  

<span class="n">fedavg</span>
<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedAvg</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">go</span> <span class="n">resnet</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">fedavg</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>


<span class="o">---</span>

<span class="n">fedprox</span>
<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedProx</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">mu</span> <span class="mf">0.001</span> <span class="o">-</span><span class="n">go</span> <span class="n">resnet</span> <span class="o">&gt;</span> <span class="n">result_fedprox</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="o">---</span>

<span class="n">moon</span>
<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">MOON</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span>  <span class="o">-</span><span class="n">go</span> <span class="n">resnet</span> <span class="o">&gt;</span> <span class="n">result_moon</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">fedbn</span>
<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedBN</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedbn_debug</span> <span class="o">&gt;</span> <span class="n">result_fedbn</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">FedFomo</span>
<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedFomo</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedfomo</span> <span class="o">&gt;</span> <span class="n">result_fedfomo</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">FedRep</span>
<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span>  <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedRep</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedrep</span> <span class="o">&gt;</span> <span class="n">result_fedrep</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">FedBABU</span>
<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedBABU</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedbabu</span> <span class="o">&gt;</span> <span class="n">result_fedbabu</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">fedala</span>
<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">2</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">p</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">did</span> <span class="mi">3</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="mi">16001</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span>  <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">pala</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedala</span> <span class="o">&gt;</span> <span class="n">result_fedala</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>


<span class="n">fedsoup</span>
<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">2</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">50</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_mg5_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">5</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">--</span><span class="n">pruning</span> <span class="o">--</span><span class="n">sparsity_ratio</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">pruning_warmup_round</span> <span class="mi">500</span> <span class="o">--</span><span class="n">masking_grad</span> <span class="o">--</span><span class="n">dynamic_mask</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="n">npz</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 

<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoup</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">go</span> <span class="n">resnet</span>  <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">fedsoup</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span>

<span class="n">fedsoupala</span>
<span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">data</span> <span class="n">tiny_camelyon17</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoupALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">100</span> <span class="o">-</span><span class="n">go</span> <span class="n">fedsoup_debug</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">4</span> <span class="o">-</span><span class="n">hoid</span> <span class="mi">0</span> <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>  <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">pala</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">tiny</span><span class="o">-</span><span class="n">came</span><span class="o">-</span><span class="n">npz</span><span class="o">-</span><span class="n">ala</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span> 

<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoupALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">1000</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">go</span> <span class="n">resnet</span>  <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>  <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">pala</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">fedsoupala</span><span class="o">-</span><span class="mf">1000.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span><span class="p">;</span> <span class="n">shutdown</span>

<span class="n">nohup</span> <span class="n">python</span> <span class="o">-</span><span class="n">u</span> <span class="n">main</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">lbs</span> <span class="mi">16</span> <span class="o">-</span><span class="n">nc</span> <span class="mi">20</span> <span class="o">-</span><span class="n">jr</span> <span class="mi">1</span> <span class="o">-</span><span class="n">nb</span> <span class="mi">10</span> <span class="o">-</span><span class="n">data</span> <span class="n">Cifar10</span> <span class="o">-</span><span class="n">m</span> <span class="n">resnet</span> <span class="o">-</span><span class="n">algo</span> <span class="n">FedSoupALA</span> <span class="o">-</span><span class="n">gr</span> <span class="mi">50</span> <span class="o">-</span><span class="n">eg</span> <span class="mi">10</span> <span class="o">-</span><span class="n">did</span> <span class="mi">0</span> <span class="o">-</span><span class="n">go</span> <span class="n">resnet</span>  <span class="o">-</span><span class="n">lr</span> <span class="mf">1e-3</span> <span class="o">-</span><span class="n">wa_alpha</span> <span class="mf">0.75</span>  <span class="o">-</span><span class="n">et</span> <span class="mi">1</span> <span class="o">-</span><span class="n">pala</span> <span class="mi">2</span> <span class="o">-</span><span class="n">s</span> <span class="mi">80</span> <span class="o">&gt;</span> <span class="n">result</span><span class="o">-</span><span class="n">fedsoupala</span><span class="o">.</span><span class="n">out</span> <span class="mi">2</span><span class="o">&gt;&amp;</span><span class="mi">1</span><span class="p">;</span> <span class="n">shutdown</span>
</code></pre></td></tr></table>
</div>
</div><p>加上-eg 10</p>
<p><code>camelyon17</code>&ndash;hold client 0</p>
<table>
<thead>
<tr>
<th>method</th>
<th>local_acc</th>
<th>loacl_auc</th>
<th>global_acc</th>
<th>global_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>FedAvg</td>
<td>82.41</td>
<td>90.44</td>
<td>70.18</td>
<td>78.74</td>
</tr>
<tr>
<td>FedProx</td>
<td>86.34</td>
<td>92.78</td>
<td>67.42</td>
<td>77.18</td>
</tr>
<tr>
<td>MOON</td>
<td>85.71</td>
<td>91.98</td>
<td>70.61</td>
<td>79.27</td>
</tr>
<tr>
<td>FedBN</td>
<td>82.32</td>
<td>90.07</td>
<td>65.16</td>
<td>71.61</td>
</tr>
<tr>
<td>FedFomo</td>
<td>80.99</td>
<td>86.51</td>
<td>61.00</td>
<td>61.69</td>
</tr>
<tr>
<td>FedRep</td>
<td>82.50</td>
<td>89.77</td>
<td>66.87</td>
<td>72.34</td>
</tr>
<tr>
<td>FedBABU</td>
<td>85.18</td>
<td>92.39</td>
<td>69.56</td>
<td>77.26</td>
</tr>
<tr>
<td>FedSoup</td>
<td><strong>85.71</strong></td>
<td><strong>92.47</strong></td>
<td><strong>72.87</strong></td>
<td><strong>81.45</strong></td>
</tr>
<tr>
<td>Fedsoup-4</td>
<td>87.68、87.90</td>
<td>94.23、94.34</td>
<td>73.31、73.18</td>
<td>80.37、80.86</td>
</tr>
<tr>
<td>Fedsoup-4-tune</td>
<td>87.77</td>
<td>94.38</td>
<td>73.25</td>
<td>80.42</td>
</tr>
<tr>
<td>FedSoupALA本地</td>
<td>67.77、76.85</td>
<td>75.69、85.63</td>
<td>58.63、63.95</td>
<td>67.44、71.08</td>
</tr>
<tr>
<td>FedSoupALA-tune</td>
<td>90.54</td>
<td>95.01</td>
<td>71.82</td>
<td>78.43</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>method</th>
<th>local_acc</th>
<th>loacl_auc</th>
<th>global_acc</th>
<th>global_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>fedasoupala p2</td>
<td>88.93/89.20/89.02</td>
<td>94.48/94.90/94.88</td>
<td>73.01/73.21/72.92</td>
<td>80.21/80.38/80.20</td>
</tr>
<tr>
<td>同上_1</td>
<td>80.09/88.04/87.95</td>
<td>88.91/94.68/94.55</td>
<td>71.04/73.28/72.61</td>
<td>78.06/81.08/80.16</td>
</tr>
<tr>
<td>gr20</td>
<td>83.04/84.02/89.20</td>
<td>90.31/92.56/95.45</td>
<td>66.23/65.28/71.29</td>
<td>75.01/75.88/81.18</td>
</tr>
<tr>
<td>p4</td>
<td>86.52/86.16/86.96</td>
<td>93.60/93.73/93.94</td>
<td>72.05/72.32/72.52</td>
<td>78.35/78.64/78.94</td>
</tr>
<tr>
<td><code>p6</code></td>
<td><strong>88.93/88.75/89.02</strong></td>
<td><strong>94.62/94.55/94.64</strong></td>
<td><strong>74.57/74.30/74.51</strong></td>
<td><strong>81.92/81.54/81.89</strong></td>
</tr>
<tr>
<td>p8</td>
<td>87.50/87.54/87.86</td>
<td>94.20/94.09/94.19</td>
<td>72.95/72.88/73.11</td>
<td>79.04/78.66/78.95</td>
</tr>
<tr>
<td>p10</td>
<td>88.66/87.86/88.48</td>
<td>94.04/93.34/93.68</td>
<td>73.16/72.39/73.01</td>
<td>79.84/78.61/79.74</td>
</tr>
<tr>
<td>p2_prun</td>
<td>86.07/86.52/86.16</td>
<td>93.15/93.23/93.01</td>
<td>72.41/72.31/73.32</td>
<td>78.75/78.75/78.74</td>
</tr>
</tbody>
</table>
<p>新的评价方式</p>
<p>nohup python -u main.py -data tiny_camelyon17 -m resnet -algo FedSoupALA -gr 1000 -did 0 -eg 100 -go fedsoup_debug -nc 4 -hoid 0 -lr 1e-3 -wa_alpha 0.75  -et 1 -pala 6 -s 80 &gt; result-tiny-came-npz-ala-neweval-p6.out 2&gt;&amp;1; shutdown</p>
<p>先单独跑FedALA</p>
<p>nohup python -u main.py -lbs 16 -nc 4 -hoid 0 -lr 1e-3-data tiny_camelyon17  -m resnet -algo FedALA -gr 1000 -did 0 -eg 100  -et 1 -pala 6 -s 80 -go fedala &gt; tiny_fedala.out 2&gt;&amp;1</p>
<p>nohup python -u main.py -data tiny_camelyon17 -m resnet -algo FedALA -gr 1000 -did 0 -eg 100 -go fedala -nc 4 -hoid 0 -lr 1e-3 -wa_alpha 0.75  -et 1 -pala 6 -s 80 &gt; tiny-came-fedala-neweval-p6.out 2&gt;&amp;1</p>
<p>fedsoup</p>
<p>nohup python -u main.py -data tiny_camelyon17 -m resnet -algo FedSoup -gr 1000 -did 0 -eg 100 -go fedsoup_debug -nc 4 -hoid 0 -lr 1e-3 -wa_alpha 0.75  -et 1 -pala 6 -s 80 &gt;tiny-came-npz-soup-neweval-p6.out 2&gt;&amp;1;</p>
<table>
<thead>
<tr>
<th>method</th>
<th>local_acc</th>
<th>loacl_auc</th>
<th>global_acc</th>
<th>global_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>fedasoupala p6</td>
<td>86.96//87.32</td>
<td>99.27///</td>
<td>71.32//71.29</td>
<td>95.78//97.58</td>
</tr>
<tr>
<td>fedala p6</td>
<td>84.91//84.38</td>
<td>99.17//99.11</td>
<td>71.24//71.32</td>
<td>97.37//97.47</td>
</tr>
<tr>
<td>fedsoup</td>
<td>84.46//85.18</td>
<td>98.88//99.08</td>
<td>73.03/73.07/</td>
<td>97.85//97.88</td>
</tr>
</tbody>
</table>
<hr>
<p>cifar 10 -nohold</p>
<p>50轮次</p>
<table>
<thead>
<tr>
<th>method</th>
<th>local_acc</th>
<th>loacl_auc</th>
<th>global_acc</th>
<th>global_auc</th>
</tr>
</thead>
<tbody>
<tr>
<td>FedAvg</td>
<td>28.70/90.14/91.10</td>
<td>71.77/99.03/99.19</td>
<td>38.22/20.79/21.04</td>
<td>81.09/72.88/73.11</td>
</tr>
<tr>
<td>fedavg_1</td>
<td>89.63///90.90</td>
<td>98.91///99.14</td>
<td>29.74/20.73//20.93</td>
<td>72.42/72.63//73.04</td>
</tr>
<tr>
<td>FedProx</td>
<td>27.18/89.78/90.74</td>
<td>69.68/98.96/99.13</td>
<td>37.07/20.88/20.99</td>
<td>80.58/72.66/72.88</td>
</tr>
<tr>
<td>FedProx_1</td>
<td>89.98///90.85</td>
<td>99.01///99.17</td>
<td>30.52/20.58//20.94</td>
<td>72.93/72.66//72.86</td>
</tr>
<tr>
<td><code>MOON</code></td>
<td>91.07/91.07/92.10</td>
<td>70.10/70.10/77.00</td>
<td>20.99/20.99/23.30</td>
<td>52.97/52.97/54.69</td>
</tr>
<tr>
<td>FedBN</td>
<td>90.82/89.23/91.92</td>
<td>88.15/86.94/94.39</td>
<td>21.33/20.60/22.61</td>
<td>55.27/54.27/57.01</td>
</tr>
<tr>
<td>FedFomo</td>
<td>88.93/89.41/89.74</td>
<td>98.83/98.87/98.97</td>
<td>1/19.45/19.90</td>
<td>1/53.94/54.22</td>
</tr>
<tr>
<td>FedRep</td>
<td>86.59/88.51/91.09</td>
<td>98.61/98.44/99.14</td>
<td>1/19.82/22.28</td>
<td>1/54.27/56.17</td>
</tr>
<tr>
<td>FedBABU</td>
<td>58.46/90.92//92.46</td>
<td>88.67/98.89//99.03</td>
<td>//22.03/23.51</td>
<td>//62.16/64.04</td>
</tr>
<tr>
<td>FedALA</td>
<td>89.56///90.58</td>
<td>93.35///94.93</td>
<td>/20.83//20.89</td>
<td>//64.11//64.47</td>
</tr>
<tr>
<td>FEDALA_1</td>
<td>89.04/90.77</td>
<td>92.58/94.80/</td>
<td>20.65/20.79/</td>
<td>/64.04/64.78</td>
</tr>
<tr>
<td>FedSoup</td>
<td>26.31/89.65/91.35</td>
<td>57.78/66.91/64.49</td>
<td>18.58/22.18/25.86</td>
<td>53.02/53.18/53.38</td>
</tr>
<tr>
<td>Fedsoupala</td>
<td>88.37/91.37/92.44</td>
<td>85.29/87.80/82.10</td>
<td>21.22/21.44/23.85</td>
<td>58.35/57.33/58.35</td>
</tr>
<tr>
<td>Fedsoupala_1</td>
<td>90.57/</td>
<td>86.59/92.48</td>
<td>21.66/24.09</td>
<td>56.94/57.82</td>
</tr>
</tbody>
</table>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">kong</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2023-11-28
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/federated-learning/">Federated Learning</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/java01/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Java概述</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/algo/">
            <span class="next-text nav-default">Algo</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=_JOXlp_emZaBjZ3IwcrMuImJ1puXlQ" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/kongfany" class="iconfont icon-github" title="github"></a>
      <a href="https://weibo.com/u/5947688533?is_all=1" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://www.zhihu.com/people/yu-ni-zhong-nian-bu-yu" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://space.bilibili.com/232669848" class="iconfont icon-bilibili" title="bilibili"></a>
  <a href="https://kongfany.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2024<span class="heart"><i class="iconfont icon-heart"></i></span><span>kong</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        tags: 'ams',
        }
    };
  </script>
  <script type="text/javascript" async src="/lib/mathjax/es5/tex-mml-chtml.js"></script>








</body>
</html>
